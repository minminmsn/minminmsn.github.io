<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"minminmsn.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="MinMinMsn">
<meta property="og:url" content="https://minminmsn.github.io/page/22/index.html">
<meta property="og:site_name" content="MinMinMsn">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Jerry Min">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://minminmsn.github.io/page/22/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>MinMinMsn</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">MinMinMsn</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://minminmsn.github.io/2018/12/09/2018/12/2018-12-09-ambari%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2hadoop/index/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jerry Min">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="MinMinMsn">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/12/09/2018/12/2018-12-09-ambari%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2hadoop/index/" class="post-title-link" itemprop="url">Ambari安装部署Hadoop</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-12-09 08:00:00" itemprop="dateCreated datePublished" datetime="2018-12-09T08:00:00+08:00">2018-12-09</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2023-05-26 15:06:30" itemprop="dateModified" datetime="2023-05-26T15:06:30+08:00">2023-05-26</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <blockquote>
<p>Apache Ambari是一种基于Web的工具，支持Apache Hadoop集群的供应、管理和监控。Ambari已支持大多数Hadoop组件，包括HDFS、MapReduce、Hive、Pig、 Hbase、Zookeper、Sqoop和Hcatalog等。Apache Ambari 支持HDFS、MapReduce、Hive、Pig、Hbase、Zookeper、Sqoop和Hcatalog等的集中管理。也是5个顶级hadoop管理工具之一。Ambari能够安装安全的（基于Kerberos）Hadoop集群，以此实现了对Hadoop 安全的支持，提供了基于角色的用户认证、授权和审计功能，并为用户管理集成了LDAP和Active Directory。</p>
</blockquote>
<h4 id="之所以选择Ambari部署hadoop而不是CDH，是因为CDH最新版本只支持Hadoop2-6-X，Ambari最新版本支持Hadoop2-7-3。"><a href="#之所以选择Ambari部署hadoop而不是CDH，是因为CDH最新版本只支持Hadoop2-6-X，Ambari最新版本支持Hadoop2-7-3。" class="headerlink" title="之所以选择Ambari部署hadoop而不是CDH，是因为CDH最新版本只支持Hadoop2.6.X，Ambari最新版本支持Hadoop2.7.3。"></a>之所以选择Ambari部署hadoop而不是CDH，是因为CDH最新版本只支持Hadoop2.6.X，Ambari最新版本支持Hadoop2.7.3。</h4><p>一、安装部署参考官网<a target="_blank" rel="noopener" href="http://ambari.apache.org/">http://ambari.apache.org/</a> 及简书<a target="_blank" rel="noopener" href="https://www.jianshu.com/p/73f9670f71cf">https://www.jianshu.com/p/73f9670f71cf</a> ，主要分以下几步：</p>
<p>1、节点互信</p>
<p>2、关闭防火墙、selinux</p>
<p>3、安装ambari-server</p>
<p>4、设置ambari-server</p>
<p>5、图形界面部署hadoop各组件</p>
<blockquote>
<p><img src="https://upload-images.jianshu.io/upload_images/7535971-7b83fbd7afe7850d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></p>
</blockquote>
<p>二、如下是新增节点步骤： 1、注意密钥为master1节点 prod-hadoop-master-01 &#x2F;root&#x2F;.ssh&#x2F;d_rsa文件</p>
<blockquote>
<p><img src="https://upload-images.jianshu.io/upload_images/7535971-cef5a91c3604aea1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></p>
</blockquote>
<p>2、注册节点</p>
<blockquote>
<p><img src="https://upload-images.jianshu.io/upload_images/7535971-3cee2d1ffe7f6710.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></p>
</blockquote>
<p>3、安装服务也可添加后再安装</p>
<blockquote>
<p><img src="https://upload-images.jianshu.io/upload_images/7535971-941b88b94fba5532.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></p>
</blockquote>
<p>4、配置默认即可</p>
<blockquote>
<p><img src="https://upload-images.jianshu.io/upload_images/7535971-b360e94f5a2aeaf1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></p>
</blockquote>
<p>5、确认下没有变更就开始部署</p>
<blockquote>
<p><img src="https://upload-images.jianshu.io/upload_images/7535971-f8c719d5effabdc3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></p>
</blockquote>
<p>6、安装进度完成即可，也可以登陆首页等待后续安装完成</p>
<blockquote>
<p><img src="https://upload-images.jianshu.io/upload_images/7535971-943c9e8d36bb05e3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></p>
</blockquote>
<p>三、补充Ambari没有集成组件安装： 1、解决ambari-service、ambari-agent默认安装数据目录在&#x2F;下</p>
<p>ambari-agent stop mv &#x2F;var&#x2F;lib&#x2F;ambari-agent &#x2F;data&#x2F;disk1&#x2F; ln -s &#x2F;data&#x2F;disk1&#x2F;ambari-agent &#x2F;var&#x2F;lib&#x2F;ambari-agent</p>
<p>mv &#x2F;usr&#x2F;hdp &#x2F;data&#x2F;disk1&#x2F; ln -s &#x2F;data&#x2F;disk1&#x2F;hdp&#x2F; &#x2F;usr&#x2F;hdp</p>
<p>ambari-agent start</p>
<p>2、ambari与presto整合 参考 <a target="_blank" rel="noopener" href="https://www.jianshu.com/p/0b5f52a959d5">https://www.jianshu.com/p/0b5f52a959d5</a> <a target="_blank" rel="noopener" href="https://github.com/prestodb/ambari-presto-service/releases">https://github.com/prestodb/ambari-presto-service/releases</a> <a target="_blank" rel="noopener" href="https://github.com/prestodb/ambari-presto-service/releases/download/v1.2/ambari-presto-1.2.tar.gz">https://github.com/prestodb/ambari-presto-service/releases/download/v1.2/ambari-presto-1.2.tar.gz</a></p>
<p>[root@prod-hadoop-master-01 ~]# tar zxvf ambari-presto-1.2.tar.gz -C &#x2F;var&#x2F;lib&#x2F;ambari-server&#x2F;resources&#x2F;stacks&#x2F;HDP&#x2F;2.6&#x2F;services&#x2F; ambari-presto-1.2&#x2F; ambari-presto-1.2&#x2F;configuration&#x2F; ambari-presto-1.2&#x2F;configuration&#x2F;connectors.properties.xml ambari-presto-1.2&#x2F;configuration&#x2F;jvm.config.xml ambari-presto-1.2&#x2F;configuration&#x2F;config.properties.xml ambari-presto-1.2&#x2F;configuration&#x2F;node.properties.xml ambari-presto-1.2&#x2F;HISTORY.rst ambari-presto-1.2&#x2F;themes&#x2F; ambari-presto-1.2&#x2F;themes&#x2F;theme.json ambari-presto-1.2&#x2F;Makefile ambari-presto-1.2&#x2F;setup.py ambari-presto-1.2&#x2F;MANIFEST.in ambari-presto-1.2&#x2F;PKG-INFO ambari-presto-1.2&#x2F;package&#x2F; ambari-presto-1.2&#x2F;package&#x2F;scripts&#x2F; ambari-presto-1.2&#x2F;package&#x2F;scripts&#x2F;presto_cli.py ambari-presto-1.2&#x2F;package&#x2F;scripts&#x2F;presto_worker.py ambari-presto-1.2&#x2F;package&#x2F;scripts&#x2F;presto_coordinator.py ambari-presto-1.2&#x2F;package&#x2F;scripts&#x2F;<strong>init</strong>.py ambari-presto-1.2&#x2F;package&#x2F;scripts&#x2F;params.py ambari-presto-1.2&#x2F;package&#x2F;scripts&#x2F;download.ini ambari-presto-1.2&#x2F;package&#x2F;scripts&#x2F;common.py ambari-presto-1.2&#x2F;package&#x2F;scripts&#x2F;presto_client.py ambari-presto-1.2&#x2F;setup.cfg ambari-presto-1.2&#x2F;ambari_presto.egg-info&#x2F; ambari-presto-1.2&#x2F;ambari_presto.egg-info&#x2F;dependency_links.txt ambari-presto-1.2&#x2F;ambari_presto.egg-info&#x2F;not-zip-safe ambari-presto-1.2&#x2F;ambari_presto.egg-info&#x2F;PKG-INFO ambari-presto-1.2&#x2F;ambari_presto.egg-info&#x2F;top_level.txt ambari-presto-1.2&#x2F;ambari_presto.egg-info&#x2F;SOURCES.txt ambari-presto-1.2&#x2F;LICENSE ambari-presto-1.2&#x2F;README.md ambari-presto-1.2&#x2F;metainfo.xml ambari-presto-1.2&#x2F;requirements.txt [root@prod-hadoop-master-01 ~]# cd &#x2F;var&#x2F;lib&#x2F;ambari-server&#x2F;resources&#x2F;stacks&#x2F;HDP&#x2F;2.6&#x2F;services&#x2F; [root@prod-hadoop-master-01 services]# ls ACCUMULO ATLAS FALCON HBASE HIVE KERBEROS MAHOUT PIG RANGER_KMS SPARK SQOOP stack_advisor.pyc STORM TEZ ZEPPELIN ambari-presto-1.2 DRUID FLUME HDFS KAFKA KNOX OOZIE RANGER SLIDER SPARK2 stack_advisor.py stack_advisor.pyo SUPERSET YARN ZOOKEEPER [root@prod-hadoop-master-01 services]# mv ambari-presto-1.2&#x2F; PRESTO [root@prod-hadoop-master-01 services]# chmod -R +x PRESTO&#x2F;* [root@prod-hadoop-master-01 services]# ambari-server restart 平台上添加presto服务器，一个控制节点，两个worker节点</p>
<p>3、安装kylin组件 <a target="_blank" rel="noopener" href="https://blog.csdn.net/vivismilecs/article/details/72763665">https://blog.csdn.net/vivismilecs/article/details/72763665</a> 下载安装 tar -zxvf apache-kylin-2.3.1-hbase1x-bin.tar.gz -C &#x2F;hadoop&#x2F; cd &#x2F;hadoop&#x2F; chown -R <a target="_blank" rel="noopener" href="http://hdfshadoop/">hdfs:hadoop</a> kylin&#x2F; vim &#x2F;etc&#x2F;profile source &#x2F;etc&#x2F;profile echo $KYLIN_HOME &#x2F;hadoop&#x2F;kylin 切换用户检查环境是否正确安装 su hdfs hive（进入hive，quit;退出） hbase shell（进入hbase shell，ctrl+c结束）</p>
<p>[hdfs@prod-hadoop-data-01 kylin]$ bin&#x2F;check-env.sh  Retrieving hadoop conf dir… KYLIN_HOME is set to &#x2F;hadoop&#x2F;kylin hdfs is not in the sudoers file. This incident will be reported. Failed to create <a href="hdfs://wiki.365jiating.com/kylin/spark-history">hdfs:&#x2F;&#x2F;&#x2F;kylin&#x2F;spark-history</a>. Please make sure the user has right to access <a href="hdfs://wiki.365jiating.com/kylin/spark-history">hdfs:&#x2F;&#x2F;&#x2F;kylin&#x2F;spark-history</a></p>
<p>排错 [hdfs@prod-hadoop-data-01 kylin]$ exit [root@prod-hadoop-data-01 hadoop]# vim &#x2F;etc&#x2F;sudoers.d&#x2F;waagent</p>
<p>检测 [hdfs@prod-hadoop-data-01 kylin]$ bin&#x2F;check-env.sh  Retrieving hadoop conf dir… KYLIN_HOME is set to &#x2F;hadoop&#x2F;kylin</p>
<p>启动 [hdfs@prod-hadoop-data-01 kylin]$ bin&#x2F;kylin.sh start Retrieving hadoop conf dir… KYLIN_HOME is set to &#x2F;hadoop&#x2F;kylin Retrieving hive dependency… Retrieving hbase dependency… Retrieving hadoop conf dir… Retrieving kafka dependency… Retrieving Spark dependency… Start to check whether we need to migrate acl tables Retrieving hadoop conf dir… KYLIN_HOME is set to &#x2F;hadoop&#x2F;kylin Retrieving hive dependency… Retrieving hbase dependency… Retrieving hadoop conf dir… Retrieving kafka dependency… Retrieving Spark dependency… SLF4J: Class path contains multiple SLF4J bindings. SLF4J: Found binding in [<a target="_blank" rel="noopener" href="http://jarfile/">jar:file:&#x2F;hadoop&#x2F;apache-kylin-2.3.1-bin&#x2F;tool&#x2F;kylin-tool-2.3.1.jar!&#x2F;org&#x2F;slf4j&#x2F;impl&#x2F;StaticLoggerBinder.class</a>] SLF4J: Found binding in [<a target="_blank" rel="noopener" href="http://jarfile/">jar:file:&#x2F;data&#x2F;disk1&#x2F;hdp&#x2F;2.6.5.0-292&#x2F;hadoop&#x2F;lib&#x2F;slf4j-log4j12-1.7.10.jar!&#x2F;org&#x2F;slf4j&#x2F;impl&#x2F;StaticLoggerBinder.class</a>] SLF4J: Found binding in [<a target="_blank" rel="noopener" href="http://jarfile/">jar:file:&#x2F;hadoop&#x2F;apache-kylin-2.3.1-bin&#x2F;spark&#x2F;jars&#x2F;slf4j-log4j12-1.7.16.jar!&#x2F;org&#x2F;slf4j&#x2F;impl&#x2F;StaticLoggerBinder.class</a>] SLF4J: See <a target="_blank" rel="noopener" href="http://www.slf4j.org/codes.html#multiple_bindings">http://www.slf4j.org/codes.html#multiple_bindings</a> for an explanation. SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory] 2018-05-24 14:23:21,974 INFO [main] common.KylinConfig:319 : Loading kylin-defaults.properties from <a target="_blank" rel="noopener" href="http://file/hadoop/apache-kylin-2.3.1-bin/tool/kylin-tool-2.3.1.jar!/kylin-defaults.properties">file:&#x2F;hadoop&#x2F;apache-kylin-2.3.1-bin&#x2F;tool&#x2F;kylin-tool-2.3.1.jar!&#x2F;kylin-defaults.properties</a> 2018-05-24 14:23:22,016 DEBUG [main] common.KylinConfig:278 : KYLIN_CONF property was not set, will seek KYLIN_HOME env variable 2018-05-24 14:23:22,019 INFO [main] common.KylinConfig:99 : Initialized a new KylinConfig from getInstanceFromEnv : 494317290 2018-05-24 14:23:22,120 INFO [main] persistence.ResourceStore:86 : Using metadata url kylin_metadata@hbase for resource store 2018-05-24 14:23:24,034 DEBUG [main] hbase.HBaseConnection:181 : Using the working dir FS for HBase: <a href="hdfs://prod-hadoop-master-01.hadoop:8020">hdfs:&#x2F;&#x2F;prod-hadoop-master-01.hadoop:8020</a> 2018-05-24 14:23:24,034 INFO [main] hbase.HBaseConnection:258 : connection is null or closed, creating a new one 2018-05-24 14:23:24,168 INFO [main] zookeeper.RecoverableZooKeeper:120 : Process identifier&#x3D;hconnection-0x7561db12 connecting to ZooKeeper ensemble&#x3D;prod-hadoop-master-01.<a href="http://hadoop:2181%2Cprod-hadoop-master-02.hadoop:2181%2Cprod-hadoop-data-01.hadoop:2181/">hadoop:2181,prod-hadoop-master-02.hadoop:2181,prod-hadoop-data-01.hadoop:2181</a> 2018-05-24 14:23:24,176 INFO [main] zookeeper.ZooKeeper:100 : Client <a target="_blank" rel="noopener" href="http://environmentzookeeper.version=3.4.6-292--1/">environment:zookeeper.version&#x3D;3.4.6-292–1</a>, built on 05&#x2F;11&#x2F;2018 07:09 GMT 2018-05-24 14:23:24,176 INFO [main] zookeeper.ZooKeeper:100 : Client <a target="_blank" rel="noopener" href="http://environmenthost.name=prod-hadoop-data-01.hadoop/">environment:host.name&#x3D;prod-hadoop-data-01.hadoop</a> 2018-05-24 14:23:24,176 INFO [main] zookeeper.ZooKeeper:100 : Client <a target="_blank" rel="noopener" href="http://environmentjava.version=1.8.0_91/">environment:java.version&#x3D;1.8.0_91</a> 2018-05-24 14:23:24,177 INFO [main] zookeeper.ZooKeeper:100 : Client <a target="_blank" rel="noopener" href="http://environmentjava.vendor=oracle/">environment:java.vendor&#x3D;Oracle</a> Corporation 2018-05-24 14:23:24,177 INFO [main] zookeeper.ZooKeeper:100 : Client <a target="_blank" rel="noopener" href="http://environmentjava.home=/">environment:java.home&#x3D;&#x2F;usr&#x2F;local&#x2F;java</a> 2018-05-24 14:23:24,182 INFO [main] zookeeper.ZooKeeper:100 : Client <a target="_blank" rel="noopener" href="http://environmentjava.class.path=/">environment:java.class.path&#x3D;&#x2F;hadoop&#x2F;kylin&#x2F;tool&#x2F;kylin-tool-2.3.1.jar:1.8.1.jar:&#x2F;hadoop&#x2F;kylin&#x2F;spark&#x2F;jars&#x2F;hadoop-mapreduce-client-jobclient-2.7.3.jar:&#x2F;hadoop&#x2F;kylin&#x2F;spark&#x2F;jars&#x2F;chill-java-0.8.0.jar:jar:&#x2F;hadoop&#x2F;kylin&#x2F;spark&#x2F;jars&#x2F;xercesImpl-2.9.1.jar:&#x2F;hadoop&#x2F;kylin&#x2F;spark&#x2F;jars&#x2F;netty-3.8.0.Final.jar:&#x2F;usr&#x2F;hdp&#x2F;current&#x2F;ext&#x2F;hbase&#x2F;*</a> 2018-05-24 14:23:24,191 INFO [main] zookeeper.ZooKeeper:100 : Client <a target="_blank" rel="noopener" href="http://environmentjava.library.path=/">environment:java.library.path&#x3D;:&#x2F;usr&#x2F;hdp&#x2F;2.6.5.0-292&#x2F;hadoop&#x2F;lib&#x2F;native&#x2F;Linux-amd64-64:&#x2F;usr&#x2F;hdp&#x2F;2.6.5.0-292&#x2F;hadoop&#x2F;lib&#x2F;native&#x2F;Linux-amd64-64:&#x2F;data&#x2F;disk1&#x2F;hdp&#x2F;2.6.5.0-292&#x2F;hadoop&#x2F;lib&#x2F;native</a> 2018-05-24 14:23:24,191 INFO [main] zookeeper.ZooKeeper:100 : Client <a target="_blank" rel="noopener" href="http://environmentjava.io.tmpdir=/">environment:java.io.tmpdir&#x3D;&#x2F;tmp</a> 2018-05-24 14:23:24,191 INFO [main] zookeeper.ZooKeeper:100 : Client <a target="_blank" rel="noopener" href="http://environmentjava.compiler=/">environment:java.compiler&#x3D;</a> 2018-05-24 14:23:24,193 INFO [main] zookeeper.ZooKeeper:100 : Client <a target="_blank" rel="noopener" href="http://environmentos.name=linux/">environment:os.name&#x3D;Linux</a> 2018-05-24 14:23:24,193 INFO [main] zookeeper.ZooKeeper:100 : Client <a target="_blank" rel="noopener" href="http://environmentos.arch=amd64/">environment:os.arch&#x3D;amd64</a> 2018-05-24 14:23:24,193 INFO [main] zookeeper.ZooKeeper:100 : Client <a target="_blank" rel="noopener" href="http://environmentos.version=2.6.32-696.18.7.el6.x86_64/">environment:os.version&#x3D;2.6.32-696.18.7.el6.x86_64</a> 2018-05-24 14:23:24,193 INFO [main] zookeeper.ZooKeeper:100 : Client <a target="_blank" rel="noopener" href="http://environmentuser.name=hdfs/">environment:user.name&#x3D;hdfs</a> 2018-05-24 14:23:24,194 INFO [main] zookeeper.ZooKeeper:100 : Client <a target="_blank" rel="noopener" href="http://environmentuser.home=/">environment:user.home&#x3D;&#x2F;home&#x2F;hdfs</a> 2018-05-24 14:23:24,194 INFO [main] zookeeper.ZooKeeper:100 : Client <a target="_blank" rel="noopener" href="http://environmentuser.dir=/">environment:user.dir&#x3D;&#x2F;hadoop&#x2F;apache-kylin-2.3.1-bin</a> 2018-05-24 14:23:24,195 INFO [main] zookeeper.ZooKeeper:438 : Initiating client connection, connectString&#x3D;prod-hadoop-master-01.<a href="http://hadoop:2181%2Cprod-hadoop-master-02.hadoop:2181%2Cprod-hadoop-data-01.hadoop:2181/">hadoop:2181,prod-hadoop-master-02.hadoop:2181,prod-hadoop-data-01.hadoop:2181</a> sessionTimeout&#x3D;90000 watcher&#x3D;org.apache.hadoop.hbase.zookeeper.PendingWatcher@66b72664 2018-05-24 14:23:24,237 INFO [main-SendThread(prod-hadoop-data-01.<a target="_blank" rel="noopener" href="http://hadoop:2181/">hadoop:2181</a>)] zookeeper.ClientCnxn:1019 : Opening socket connection to server prod-hadoop-data-01.hadoop&#x2F;172.20.3.6:2181. Will not attempt to authenticate using SASL (unknown error) 2018-05-24 14:23:24,246 INFO [main-SendThread(prod-hadoop-data-01.<a target="_blank" rel="noopener" href="http://hadoop:2181/">hadoop:2181</a>)] zookeeper.ClientCnxn:864 : Socket connection established, initiating session, client: &#x2F;172.20.3.6:50746, server: prod-hadoop-data-01.hadoop&#x2F;172.20.3.6:2181 2018-05-24 14:23:24,256 INFO [main-SendThread(prod-hadoop-data-01.<a target="_blank" rel="noopener" href="http://hadoop:2181/">hadoop:2181</a>)] zookeeper.ClientCnxn:1279 : Session establishment complete on server prod-hadoop-data-01.hadoop&#x2F;172.20.3.6:2181, sessionid &#x3D; 0x163882326e1003b, negotiated timeout &#x3D; 60000 2018-05-24 14:23:24,892 DEBUG [main] hbase.HBaseConnection:181 : Using the working dir FS for HBase: <a href="hdfs://prod-hadoop-master-01.hadoop:8020">hdfs:&#x2F;&#x2F;prod-hadoop-master-01.hadoop:8020</a> 2018-05-24 14:23:24,944 INFO [main] imps.CuratorFrameworkImpl:224 : Starting 2018-05-24 14:23:24,947 INFO [main] zookeeper.ZooKeeper:438 : Initiating client connection, connectString&#x3D;prod-hadoop-master-01.<a href="http://hadoop:2181%2Cprod-hadoop-master-02.hadoop:2181%2Cprod-hadoop-data-01.hadoop:2181/">hadoop:2181,prod-hadoop-master-02.hadoop:2181,prod-hadoop-data-01.hadoop:2181</a> sessionTimeout&#x3D;120000 watcher&#x3D;org.apache.curator.ConnectionState@67207d8a 2018-05-24 14:23:24,950 INFO [main-SendThread(prod-hadoop-master-02.<a target="_blank" rel="noopener" href="http://hadoop:2181/">hadoop:2181</a>)] zookeeper.ClientCnxn:1019 : Opening socket connection to server prod-hadoop-master-02.hadoop&#x2F;172.20.3.5:2181. Will not attempt to authenticate using SASL (unknown error) 2018-05-24 14:23:24,951 INFO [main-SendThread(prod-hadoop-master-02.<a target="_blank" rel="noopener" href="http://hadoop:2181/">hadoop:2181</a>)] zookeeper.ClientCnxn:864 : Socket connection established, initiating session, client: &#x2F;172.20.3.6:60080, server: prod-hadoop-master-02.hadoop&#x2F;172.20.3.5:2181 2018-05-24 14:23:24,952 DEBUG [main] util.ZookeeperDistributedLock:143 : 6616@prod-hadoop-data-01 trying to lock &#x2F;kylin&#x2F;kylin_metadata&#x2F;create_htable&#x2F;kylin_metadata&#x2F;lock 2018-05-24 14:23:24,957 INFO [main-SendThread(prod-hadoop-master-02.<a target="_blank" rel="noopener" href="http://hadoop:2181/">hadoop:2181</a>)] zookeeper.ClientCnxn:1279 : Session establishment complete on server prod-hadoop-master-02.hadoop&#x2F;172.20.3.5:2181, sessionid &#x3D; 0x3638801b4480045, negotiated timeout &#x3D; 60000 2018-05-24 14:23:24,962 INFO [main-EventThread] state.ConnectionStateManager:228 : State change: CONNECTED 2018-05-24 14:23:25,031 INFO [main] util.ZookeeperDistributedLock:155 : 6616@prod-hadoop-data-01 acquired lock at &#x2F;kylin&#x2F;kylin_metadata&#x2F;create_htable&#x2F;kylin_metadata&#x2F;lock 2018-05-24 14:23:25,036 DEBUG [main] hbase.HBaseConnection:337 : Creating HTable ‘kylin_metadata’ 2018-05-24 14:23:27,822 INFO [main] client.HBaseAdmin:789 : Created kylin_metadata 2018-05-24 14:23:27,823 DEBUG [main] hbase.HBaseConnection:350 : HTable ‘kylin_metadata’ created 2018-05-24 14:23:27,824 DEBUG [main] util.ZookeeperDistributedLock:223 : 6616@prod-hadoop-data-01 trying to unlock &#x2F;kylin&#x2F;kylin_metadata&#x2F;create_htable&#x2F;kylin_metadata&#x2F;lock 2018-05-24 14:23:27,833 INFO [main] util.ZookeeperDistributedLock:234 : 6616@prod-hadoop-data-01 released lock at &#x2F;kylin&#x2F;kylin_metadata&#x2F;create_htable&#x2F;kylin_metadata&#x2F;lock 2018-05-24 14:23:28,105 DEBUG [main] hbase.HBaseConnection:181 : Using the working dir FS for HBase: <a href="hdfs://prod-hadoop-master-01.hadoop:8020">hdfs:&#x2F;&#x2F;prod-hadoop-master-01.hadoop:8020</a> 2018-05-24 14:23:28,105 INFO [main] hbase.HBaseConnection:258 : connection is null or closed, creating a new one 2018-05-24 14:23:28,106 INFO [main] zookeeper.RecoverableZooKeeper:120 : Process identifier&#x3D;hconnection-0xf339eae connecting to ZooKeeper ensemble&#x3D;prod-hadoop-master-01.<a href="http://hadoop:2181%2Cprod-hadoop-master-02.hadoop:2181%2Cprod-hadoop-data-01.hadoop:2181/">hadoop:2181,prod-hadoop-master-02.hadoop:2181,prod-hadoop-data-01.hadoop:2181</a> 2018-05-24 14:23:28,106 INFO [main] zookeeper.ZooKeeper:438 : Initiating client connection, connectString&#x3D;prod-hadoop-master-01.<a href="http://hadoop:2181%2Cprod-hadoop-master-02.hadoop:2181%2Cprod-hadoop-data-01.hadoop:2181/">hadoop:2181,prod-hadoop-master-02.hadoop:2181,prod-hadoop-data-01.hadoop:2181</a> sessionTimeout&#x3D;90000 watcher&#x3D;org.apache.hadoop.hbase.zookeeper.PendingWatcher@2822c6ff 2018-05-24 14:23:28,109 INFO [main-SendThread(prod-hadoop-data-01.<a target="_blank" rel="noopener" href="http://hadoop:2181/">hadoop:2181</a>)] zookeeper.ClientCnxn:1019 : Opening socket connection to server prod-hadoop-data-01.hadoop&#x2F;172.20.3.6:2181. Will not attempt to authenticate using SASL (unknown error) 2018-05-24 14:23:28,109 INFO [main-SendThread(prod-hadoop-data-01.<a target="_blank" rel="noopener" href="http://hadoop:2181/">hadoop:2181</a>)] zookeeper.ClientCnxn:864 : Socket connection established, initiating session, client: &#x2F;172.20.3.6:50760, server: prod-hadoop-data-01.hadoop&#x2F;172.20.3.6:2181 2018-05-24 14:23:28,115 INFO [main-SendThread(prod-hadoop-data-01.<a target="_blank" rel="noopener" href="http://hadoop:2181/">hadoop:2181</a>)] zookeeper.ClientCnxn:1279 : Session establishment complete on server prod-hadoop-data-01.hadoop&#x2F;172.20.3.6:2181, sessionid &#x3D; 0x163882326e1003c, negotiated timeout &#x3D; 60000 2018-05-24 14:23:28,138 INFO [close-hbase-conn] hbase.HBaseConnection:137 : Closing HBase connections… 2018-05-24 14:23:28,144 INFO [close-hbase-conn] client.ConnectionManager$HConnectionImplementation:1703 : Closing zookeeper sessionid&#x3D;0x163882326e1003c 2018-05-24 14:23:28,152 INFO [close-hbase-conn] zookeeper.ZooKeeper:684 : Session: 0x163882326e1003c closed 2018-05-24 14:23:28,152 INFO [main-EventThread] zookeeper.ClientCnxn:524 : EventThread shut down 2018-05-24 14:23:28,154 INFO [Thread-8] zookeeper.ZooKeeper:684 : Session: 0x3638801b4480045 closed 2018-05-24 14:23:28,154 INFO [main-EventThread] zookeeper.ClientCnxn:524 : EventThread shut down 2018-05-24 14:23:28,162 INFO [close-hbase-conn] client.ConnectionManager$HConnectionImplementation:2167 : Closing master protocol: MasterService 2018-05-24 14:23:28,163 INFO [close-hbase-conn] client.ConnectionManager$HConnectionImplementation:1703 : Closing zookeeper sessionid&#x3D;0x163882326e1003b 2018-05-24 14:23:28,168 INFO [main-EventThread] zookeeper.ClientCnxn:524 : EventThread shut down 2018-05-24 14:23:28,169 INFO [close-hbase-conn] zookeeper.ZooKeeper:684 : Session: 0x163882326e1003b closed</p>
<p>A new Kylin instance is started by hdfs. To stop it, run ‘kylin.sh stop’ Check the log at &#x2F;hadoop&#x2F;kylin&#x2F;logs&#x2F;kylin.log Web UI is at http:&#x2F;&#x2F;:7070&#x2F;kylin</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://minminmsn.github.io/2018/12/09/2018/12/2018-12-09-dns%E4%B8%BB%E4%BB%8E%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%90%AD%E5%BB%BA/index/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jerry Min">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="MinMinMsn">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/12/09/2018/12/2018-12-09-dns%E4%B8%BB%E4%BB%8E%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%90%AD%E5%BB%BA/index/" class="post-title-link" itemprop="url">DNS主从服务器搭建</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-12-09 08:00:00" itemprop="dateCreated datePublished" datetime="2018-12-09T08:00:00+08:00">2018-12-09</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2023-05-26 15:06:30" itemprop="dateModified" datetime="2023-05-26T15:06:30+08:00">2023-05-26</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/middleware/" itemprop="url" rel="index"><span itemprop="name">middleware</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h3><p><a target="_blank" rel="noopener" href="http://blog.51cto.com/yichenyang/1911098">http://blog.51cto.com/yichenyang/1911098</a> <a target="_blank" rel="noopener" href="http://blog.51cto.com/wubinary/1379595">http://blog.51cto.com/wubinary/1379595</a></p>
<h3 id="一、安装"><a href="#一、安装" class="headerlink" title="一、安装"></a>一、安装</h3><blockquote>
<p>[root@prod-dns-01 etc]# yum -y install bind [root@prod-dns-01 etc]# rpm -qa |grep bind bind-9.9.4-61.el7.x86_64 rpcbind-0.2.0-42.el7.x86_64 bind-libs-9.9.4-61.el7.x86_64 bind-utils-9.9.4-61.el7.x86_64 bind-license-9.9.4-61.el7.noarch bind-libs-lite-9.9.4-61.el7.x86_64 注意hosts文件 [root@prod-dns-01 etc]# cat &#x2F;etc&#x2F;hosts 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 172.20.3.36 prod-dns-01 prod-dns-01.test.com prod-dns-01.test.net prod-dns-01.test.bo 172.20.3.37 prod-dns-02 prod-dns-02.test.com prod-dns-02.test.net prod-dns-02.test.bo</p>
</blockquote>
<h3 id="二、修改主配置"><a href="#二、修改主配置" class="headerlink" title="二、修改主配置"></a>二、修改主配置</h3><p>1、named.conf</p>
<blockquote>
<p>[root@prod-dns-01 etc]# cat named.conf options { directory “&#x2F;var&#x2F;named”; dump-file “&#x2F;var&#x2F;named&#x2F;data&#x2F;cache_dump.db”; statistics-file “&#x2F;var&#x2F;named&#x2F;data&#x2F;named_stats.txt”; memstatistics-file “&#x2F;var&#x2F;named&#x2F;data&#x2F;named_mem_stats.txt”; allow-query { any; }; recursion yes; bindkeys-file “&#x2F;etc&#x2F;named.iscdlv.key”; managed-keys-directory “&#x2F;var&#x2F;named&#x2F;dynamic”; pid-file “&#x2F;run&#x2F;named&#x2F;named.pid”; session-keyfile “&#x2F;run&#x2F;named&#x2F;session.key”; }; logging { channel default_debug { file “data&#x2F;named.run”; severity dynamic; }; }; zone “.” IN { type hint; file “named.ca”; }; include “&#x2F;etc&#x2F;named.rfc1912.zones”; include “&#x2F;etc&#x2F;named.root.key”;</p>
</blockquote>
<p>2、named.rfc1912.zones</p>
<blockquote>
<p>[root@prod-dns-01 etc]# cat &#x2F;etc&#x2F;named.rfc1912.zones zone “test.net” IN { type master; file “test.net.zone”; allow-transfer {127.0.0.1;172.20.3.36;172.20.3.37;}; }; zone “test.com” IN { type master; file “test.com.zone”; allow-transfer {127.0.0.1;172.20.3.36;172.20.3.37;}; }; zone “test.bo” IN { type master; file “test.bo.zone”; allow-transfer {127.0.0.1;172.20.3.36;172.20.3.37;}; };</p>
</blockquote>
<p>3、注意权限</p>
<blockquote>
<p>[root@prod-dns-01 named]# ls -lh total 28K drwxrwx— 2 named named 23 Jun 8 13:45 data drwxrwx— 2 named named 31 Jun 8 13:56 dynamic -rw-r—– 1 root named 2.3K May 22 2017 named.ca -rw-r—– 1 root named 152 Dec 15 2009 named.empty -rw-r—– 1 root named 152 Jun 21 2007 named.localhost -rw-r—– 1 root named 168 Dec 15 2009 named.loopback -rw-r—– 1 root named 848 Jun 8 14:04 test.bo.zone -rw-r—– 1 root named 850 Jun 8 14:04 test.com.zone -rw-r—– 1 root named 850 Jun 8 14:04 test.net.zone</p>
</blockquote>
<p>4、检测配置 $TTL为定义的宏，表示下面资源记录ttl的值都为300秒 @符号可代表区域文件&#x2F;etc&#x2F;named.conf里面定义的区域名称，即：”test.net.”。</p>
<p>每个区域的资源记录第一条必须是SOA，SOA后面接DNS服务器的域名和电子邮箱地址，此处电子邮箱地址里的@因为有特殊用途，所以此处要用点号代替。SOA后面小括号里的各值所代表的意义如下所示： IN SOA prod-dns-01.test.net admin.test.net ( 1806081510 ;标识序列号，十进制数字，不能超过10位，通常使用日期，年月日时分，代表18年6月8号15点10分修改记录 10M;新时间，即每隔多久到主服务器检查一次，此处为10分钟 5M ;重试时间，应该小于刷新时间，此处为5分钟 1D ;过期时间，此处为1天 2D ;主服务器挂后，从服务器至多工作的时间，此处为2天)</p>
<blockquote>
<p>[root@prod-dns-01 named]# named-checkzone “test.net.zone” &#x2F;var&#x2F;named&#x2F;test.net.zone zone <a target="_blank" rel="noopener" href="http://test.net.zone/IN">test.net.zone&#x2F;IN</a>: loaded serial 1806081010 OK</p>
</blockquote>
<p>5、添加新A记录</p>
<blockquote>
<p>[root@prod-dns-01 named]# vim test.com.zone [root@prod-dns-01 named]# vim test.bo.zone [root@prod-dns-01 named]# vim test.net.zone [root@prod-dns-01 etc]# cat &#x2F;var&#x2F;named&#x2F;test.net.zone $TTL 300 ; @ IN SOA prod-dns-01.test.net admin.test.net ( 1806081550 ; Serial 10M ; Refresh 5M ; Retry 1D ; Expire 2D ; TTL ) ; IN NS dns1 IN NS dns2 dns1 IN A 172.20.3.36 dns2 IN A 172.20.3.37 ; ; prod-hadoop-master-01 IN A 172.20.3.4 prod-hadoop-master-02 IN A 172.20.3.5 prod-hadoop-data-01 IN A 172.20.3.6 prod-hadoop-data-02 IN A 172.20.3.7 prod-hadoop-data-03 IN A 172.20.3.8 prod-hadoop-data-04 IN A 172.20.3.9 prod-hadoop-data-05 IN A 172.20.3.10 prod-hadoop-data-06 IN A 172.20.3.11 prod-hadoop-data-07 IN A 172.20.3.12 prod-hadoop-data-08 IN A 172.20.3.13</p>
</blockquote>
<p>6、配置生效</p>
<blockquote>
<p>[root@prod-dns-01 named]# rndc reload server reload successful</p>
</blockquote>
<h3 id="三、测试"><a href="#三、测试" class="headerlink" title="三、测试"></a>三、测试</h3><blockquote>
<p>[root@prod-hadoop-master-01 ~]# dig -t A prod-hadoop-data-01.test.com @172.20.3.36 ; &lt;&lt;&gt;&gt; DiG 9.8.2rc1-RedHat-9.8.2-0.62.rc1.el6_9.4 &lt;&lt;&gt;&gt; -t A prod-hadoop-data-01.test.com @172.20.3.36 ;; global options: +cmd ;; Got answer: ;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 39022 ;; flags: qr aa rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 2, ADDITIONAL: 2 ;; QUESTION SECTION: ;prod-hadoop-data-01.test.com. IN A ;; ANSWER SECTION: prod-hadoop-data-01.test.com. 300 IN A 172.20.3.6 ;; AUTHORITY SECTION: test.com. 300 IN NS dns2.test.com. test.com. 300 IN NS dns1.test.com. ;; ADDITIONAL SECTION: dns1.test.com. 300 IN A 172.20.3.36 dns2.test.com. 300 IN A 172.20.3.37 ;; Query time: 1 msec ;; SERVER: 172.20.3.36#53(172.20.3.36) ;; WHEN: Fri Jun 8 14:05:36 2018 ;; MSG SIZE rcvd: 133</p>
</blockquote>
<h3 id="四、主从同步"><a href="#四、主从同步" class="headerlink" title="四、主从同步"></a>四、主从同步</h3><p>1、dns 从主配置</p>
<blockquote>
<p>[root@prod-dns-02 etc]# cat &#x2F;etc&#x2F;named.conf [root@prod-dns-02 etc]# cat &#x2F;etc&#x2F;named.rfc1912.zones zone “test.net” IN { type slave; masters { 172.20.3.36; }; file “slaves&#x2F;test.net.zone”; allow-transfer { none; }; }; zone “test.com” IN { type slave; masters { 172.20.3.36; }; file “slaves&#x2F;test.com.zone”; allow-transfer { none; }; }; zone “test.bo” IN { type slave; masters { 172.20.3.36; }; file “slaves&#x2F;test.bo.zone”; allow-transfer { none; }; };</p>
</blockquote>
<p>2、启动dns从配置会同步主的zone文件</p>
<blockquote>
<p>[root@prod-dns-02 etc]# ls -ls &#x2F;var&#x2F;named&#x2F;slaves&#x2F; [root@prod-dns-02 etc]# systemctl start named.service [root@prod-dns-02 etc]# systemctl status named.service [root@prod-dns-02 etc]# ls &#x2F;var&#x2F;named&#x2F;slaves&#x2F; test.bo.zone test.com.zone test.net.zone</p>
</blockquote>
<p>3、测试dns从的解析</p>
<blockquote>
<p>[root@prod-hadoop-master-01 ~]# dig -t A prod-hadoop-data-01.test.com @172.20.3.37 ; &lt;&lt;&gt;&gt; DiG 9.8.2rc1-RedHat-9.8.2-0.62.rc1.el6_9.4 &lt;&lt;&gt;&gt; -t A prod-hadoop-data-01.test.com @172.20.3.37 ;; global options: +cmd ;; Got answer: ;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 6112 ;; flags: qr aa rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 2, ADDITIONAL: 2 ;; QUESTION SECTION: ;prod-hadoop-data-01.test.com. IN A ;; ANSWER SECTION: prod-hadoop-data-01.test.com. 300 IN A 172.20.3.6 ;; AUTHORITY SECTION: test.com. 300 IN NS dns1.test.com. test.com. 300 IN NS dns2.test.com. ;; ADDITIONAL SECTION: dns1.test.com. 300 IN A 172.20.3.36 dns2.test.com. 300 IN A 172.20.3.37 ;; Query time: 3 msec ;; SERVER: 172.20.3.37#53(172.20.3.37) ;; WHEN: Fri Jun 8 14:35:03 2018 ;; MSG SIZE rcvd: 133</p>
</blockquote>
<h3 id="五、服务开机启动"><a href="#五、服务开机启动" class="headerlink" title="五、服务开机启动"></a>五、服务开机启动</h3><blockquote>
<p>[root@prod-dns-01 etc]# systemctl enable named Created symlink from &#x2F;etc&#x2F;systemd&#x2F;system&#x2F;multi-user.target.wants&#x2F;named.service to &#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;named.service. [root@prod-dns-02 etc]# systemctl enable named Created symlink from &#x2F;etc&#x2F;systemd&#x2F;system&#x2F;multi-user.target.wants&#x2F;named.service to &#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;named.service.</p>
</blockquote>
<h3 id="六、压力测试"><a href="#六、压力测试" class="headerlink" title="六、压力测试"></a>六、压力测试</h3><blockquote>
<p>[root@prod-dns-01 ~]# wget <a target="_blank" rel="noopener" href="https://www.isc.org/downloads/file/bind-9-9-12/?version=tar-gz">https://www.isc.org/downloads/file/bind-9-9-12/?version=tar-gz</a> [root@prod-dns-01 ~]# tar zxvf index.html\?version\&#x3D;tar-gz [root@prod-dns-01 queryperf]# cd bind-9.9.12&#x2F;contrib&#x2F;queryperf&#x2F; [root@prod-dns-01 queryperf]# .&#x2F;configure [root@prod-dns-01 queryperf]# make 使用300万书记，qps每秒达到1万以上 [root@prod-dns-01 queryperf]# .&#x2F;queryperf -d testname.txt -s 172.20.3.37 DNS Query Performance Testing Tool Version: $Id: queryperf.c,v 1.12 2007&#x2F;09&#x2F;05 07:36:04 marka Exp $ [Status] Processing input data [Status] Sending queries (beginning with 172.20.3.37) [Status] Testing complete Statistics: Parse input file: once Ended due to: reaching end of file Queries sent: 3034641 queries Queries completed: 3034641 queries Queries lost: 0 queries Queries delayed(?): 0 queries RTT max: 0.028393 sec RTT min: 0.000110 sec RTT average: 0.001711 sec RTT std deviation: 0.001989 sec RTT out of range: 0 queries Percentage completed: 100.00% Percentage lost: 0.00% Started at: Fri Jun 8 15:28:33 2018 Finished at: Fri Jun 8 15:33:10 2018 Ran for: 276.930575 seconds Queries per second: 10958.129127 qps</p>
</blockquote>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://minminmsn.github.io/2018/12/09/2018/12/2018-12-09-%E6%8E%92%E6%9F%A5logstash2-4%E5%8D%87%E7%BA%A7%E5%88%B05-0%E7%89%88%E6%9C%AC%E5%90%8Ekafka%E4%B8%8D%E5%85%BC%E5%AE%B9%E9%97%AE%E9%A2%98/index/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jerry Min">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="MinMinMsn">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/12/09/2018/12/2018-12-09-%E6%8E%92%E6%9F%A5logstash2-4%E5%8D%87%E7%BA%A7%E5%88%B05-0%E7%89%88%E6%9C%AC%E5%90%8Ekafka%E4%B8%8D%E5%85%BC%E5%AE%B9%E9%97%AE%E9%A2%98/index/" class="post-title-link" itemprop="url">排查logstash2.4升级到5.0版本后kafka不兼容问题</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-12-09 08:00:00" itemprop="dateCreated datePublished" datetime="2018-12-09T08:00:00+08:00">2018-12-09</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2023-05-26 15:06:30" itemprop="dateModified" datetime="2023-05-26T15:06:30+08:00">2023-05-26</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <blockquote>
<h3 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a><strong>参考文档</strong></h3><p>&#x2F;usr&#x2F;share&#x2F;logstash&#x2F;vendor&#x2F;bundle&#x2F;jruby&#x2F;1.9&#x2F;gems&#x2F;logstash-input-kafka-5.0.5&#x2F;CHANGELOG.md &#x2F;usr&#x2F;share&#x2F;logstash&#x2F;vendor&#x2F;bundle&#x2F;jruby&#x2F;1.9&#x2F;gems&#x2F;logstash-input-kafka-5.0.5&#x2F;DEVELOPER.md &#x2F;usr&#x2F;share&#x2F;logstash&#x2F;vendor&#x2F;bundle&#x2F;jruby&#x2F;1.9&#x2F;gems&#x2F;logstash-input-kafka-5.0.5&#x2F;README.md.md &#x2F;usr&#x2F;share&#x2F;logstash&#x2F;vendor&#x2F;bundle&#x2F;jruby&#x2F;1.9&#x2F;gems&#x2F;logstash-input-kafka-5.0.5&#x2F;lib&#x2F;logstash&#x2F;inputs&#x2F;kafka.rb &#x2F;usr&#x2F;share&#x2F;logstash&#x2F;vendor&#x2F;bundle&#x2F;jruby&#x2F;1.9&#x2F;gems&#x2F;logstash-output-kafka-5.0.4&#x2F;lib&#x2F;logstash&#x2F;outputs&#x2F;kafka.rb</p>
</blockquote>
<h3 id="挑战"><a href="#挑战" class="headerlink" title="挑战"></a><strong>挑战</strong></h3><p>之前对ELKB环境从2.4版本升级到最新的5.0稳定版本，主要升级步骤可以参考<a target="_blank" rel="noopener" href="http://jerrymin.blog.51cto.com/3002256/1870205%EF%BC%8C%E5%90%8E%E6%9D%A5%E5%8F%91%E7%8E%B0kafka%E9%9B%86%E7%BE%A4%E8%BF%90%E8%A1%8C%E6%8A%A5%E9%94%99%EF%BC%8C%E7%8E%B0%E5%9C%A8%E6%8A%8A%E6%8E%92%E6%9F%A5%E8%BF%87%E7%A8%8B%E8%AE%B0%E5%BD%95%E5%A6%82%E4%B8%8B%EF%BC%8C%E4%BB%85%E4%BE%9B%E5%8F%82%E8%80%83">http://jerrymin.blog.51cto.com/3002256/1870205，后来发现kafka集群运行报错，现在把排查过程记录如下，仅供参考</a></p>
<h4 id="之前环境"><a href="#之前环境" class="headerlink" title="之前环境"></a><strong>之前环境</strong></h4><p>logstash2.4 logstash-input-kafka-2.0.9 logstash-output-kafka-2.0.5 kafka_2.10-0.8.2.2.tgz</p>
<h4 id="升级后环境"><a href="#升级后环境" class="headerlink" title="升级后环境"></a><strong>升级后环境</strong></h4><p>logstash5.0 logstash-input-kafka-2.0.9 logstash-output-kafka-2.0.5</p>
<h4 id="报错信息"><a href="#报错信息" class="headerlink" title="报错信息"></a><strong>报错信息</strong></h4><p>[2016-11-16T14:35:44,739][ERROR][logstash.inputs.kafka ] Unknown setting’zk_connect’forkafka [2016-11-16T14:35:44,741][ERROR][logstash.inputs.kafka ] Unknown setting’topic_id’forkafka [2016-11-16T14:35:44,741][ERROR][logstash.inputs.kafka ] Unknown setting’reset_beginning’forkafka</p>
<h4 id="实施步骤"><a href="#实施步骤" class="headerlink" title="实施步骤"></a><strong>实施步骤</strong></h4><p>1，根据错误查看程序哪里报错 grep”Unknown setting”&#x2F;usr&#x2F;share&#x2F;logstash&#x2F;-R &#x2F;usr&#x2F;share&#x2F;logstash&#x2F;logstash-core&#x2F;lib&#x2F;logstash&#x2F;config&#x2F;mixin.rb: self.logger.error(“Unknown setting ‘#{name}’ for #{@plugin_name}”)</p>
<p>2，查看程序相关代码，发现需要查看plugins的config定义文件等</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">def validate_check_invalid_parameter_names(params)</span><br><span class="line">invalid_params = params.keys</span><br><span class="line"># Filter out parameters that match regexp keys.</span><br><span class="line"># These are defined in plugins like this:</span><br><span class="line">#   config /foo.*/ =&gt; ...</span><br><span class="line">@config.each_keydo|config_key|</span><br><span class="line">ifconfig_key.is_a?(Regexp)</span><br><span class="line">invalid_params.reject! &#123; |k| k =~ config_key &#125;</span><br><span class="line">elsif config_key.is_a?(String)</span><br><span class="line">invalid_params.reject! &#123; |k| k == config_key &#125;</span><br><span class="line">end</span><br><span class="line">end</span><br><span class="line">ifinvalid_params.size &gt; 0</span><br><span class="line">invalid_params.eachdo|name|</span><br><span class="line">self.logger.error(&quot;Unknown setting &#x27;#&#123;name&#125;&#x27; for #&#123;@plugin_name&#125;&quot;)</span><br><span class="line">end</span><br><span class="line">returnfalse</span><br><span class="line">end# if invalid_params.size &gt; 0</span><br><span class="line">returntrue</span><br><span class="line">end# def validate_check_invalid_parameter_names</span><br></pre></td></tr></table></figure>

<p>3，进入插件总目录查看具体信息 cd &#x2F;usr&#x2F;share&#x2F;logstash&#x2F;vendor&#x2F;bundle&#x2F;jruby&#x2F;1.9&#x2F;gems&#x2F;logstash-input-kafka-5.0.5 发现重点查看如下文件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">grepconfig ./* -R |awk&#x27;&#123;print $1&#125;&#x27;|uniq</span><br><span class="line">./CHANGELOG.md:</span><br><span class="line">./DEVELOPER.md:See</span><br><span class="line">./lib/logstash/inputs/kafka.rb:#</span><br><span class="line">./lib/logstash/inputs/kafka.rb:</span><br><span class="line">./README.md:-</span><br><span class="line">Binary</span><br></pre></td></tr></table></figure>

<p>1）首先看CHANGELOG.md，就有发现logstash-input-3.0.0.beta1开始就不在向后兼容，且剔除了jruby-kafka，注意这里有个坑2）会讲到，4.0.0版本说开始支持kafka 0.9，5.0.0又说开始支持0.10切不向后兼容，这破坏性更新也是够了。看来问题找到了我的kafka版本是kafka_2.10-0.8.2.2.tgz，kafka版本不兼容导致的。</p>
<p>CHANGELOG.md部分文档如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">## 5.0.4</span><br><span class="line">- Update to Kafka version 0.10.0.1forbug fixes</span><br><span class="line">## 5.0.0</span><br><span class="line">- SupportforKafka 0.10whichis not backward compatible with 0.9 broker.</span><br><span class="line">## 4.0.0</span><br><span class="line">- Republish all the gems under jruby.</span><br><span class="line">- Update the plugin to the version 2.0 of the plugin api, this change is requiredforLogstash 5.0 compatibility. See https://github.com/elastic/logstash/issues/5141</span><br><span class="line">- SupportforKafka 0.9forLS 5.x</span><br><span class="line">## 3.0.0.beta1</span><br><span class="line">- Refactor to use new Java based consumer, bypassing jruby-kafka</span><br><span class="line">- Breaking: Change configuration to match Kafka&#x27;s configuration. This version is not backward compatible</span><br></pre></td></tr></table></figure>

<p>2）之前我看DEVELOPER.md文档时,看配置语法都正确，还以为是却少依赖关系jruby-kafka library呢，这个再logstash2.x是在用的(另外对比logstash5.x发现5版本少了不少插件。另外kafka版本写的是0.8.1.1，看来这个DEVELOPER.md没有及时更新（与后面kafka.rb文件不一致），谁要是看到了麻烦及时更新啊，虽是小问题但是也可能误导我等屁民。当然也有可能是我没有全面看文档导致的。</p>
<p>DEVELOPER.md文档结尾如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Dependencies</span><br><span class="line">====================</span><br><span class="line">* Apache Kafka version 0.8.1.1</span><br><span class="line">* jruby-kafka library</span><br></pre></td></tr></table></figure>

<p>3）开始看README.md文档，特意看了下kafka的兼容性 看来logstas-input-kafka5.0.5和logstash-output-kafka5.0.4只能用kafka0.10了。如果你想用Kafka0.9还想用Logstash5.0,你的logstash-input-kafka和logstash-output-kafka只能降级版本到4.0.0了，这里都说他是中间过渡版本了，所以还是随大流吧。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">## Kafka Compatibility</span><br><span class="line">Here&#x27;s a table that describes the compatibility matrixforKafka Broker support. Please remember that it is good advice to upgrade brokers before consumers/producers</span><br><span class="line">since brokers target backwards compatibility. The 0.9 broker will work with both the 0.8 consumer and 0.9 consumer APIs but not the other way around.</span><br><span class="line">| Kafka Broker Version | Logstash Version | Input Plugin | Output Plugin | Why? |</span><br><span class="line">|:---------------:|:------------------:|:--------------:|:---------------:|:------|</span><br><span class="line">| 0.8           | 2.0 - 2.x   | &lt; 3.0.0 | &lt;3.0.0 | Legacy, 0.8 is still popular |</span><br><span class="line">| 0.9           | 2.0 - 2.3.x   |   3.0.0 | 3.0.0  | Intermediate release before 0.10 that works with old Ruby Event API `[]`  |</span><br><span class="line">| 0.9          | 2.4, 5.0           |   4.0.0 | 4.0.0  | Intermediate release before 0.10 with new get/setAPI |</span><br><span class="line">| 0.10         | 2.4, 5.0           |   5.0.0 | 5.0.0  | Track latest Kafka release. Not compatible with 0.9 broker |</span><br></pre></td></tr></table></figure>

<p>4）现在看来只能升级kafka版本了。最后我看了下jar-dependencies发现了kafka-clients-0.10.0.1.jar ls&#x2F;usr&#x2F;share&#x2F;logstash&#x2F;vendor&#x2F;bundle&#x2F;jruby&#x2F;1.9&#x2F;gems&#x2F;logstash-input-kafka-5.0.5&#x2F;vendor&#x2F;jar-dependencies&#x2F;runtime-jars&#x2F; kafka-clients-0.10.0.1.jar log4j-1.2.17.jar lz4-1.3.0.jar slf4j-api-1.7.21.jar slf4j-log4j12-1.7.21.jar snappy-java-1.1.2.6.jar</p>
<p>5）还有一个文件没有看，怀着好奇心我看了一眼，发现之前都白费力气了，这里才是最有参考价值的的主参考文档啊，是捷径啊，隐藏的够深的，差点错过了，汗！ &#x2F;usr&#x2F;share&#x2F;logstash&#x2F;vendor&#x2F;bundle&#x2F;jruby&#x2F;1.9&#x2F;gems&#x2F;logstash-input-kafka-5.0.5&#x2F;lib&#x2F;logstash&#x2F;inputs&#x2F;kafka.rb</p>
<p>kafka.rb部分文档如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># This input will read events from a Kafka topic. It uses the the newly designed</span><br><span class="line"># 0.10 version of consumer API provided by Kafka to read messages from the broker.</span><br><span class="line">#</span><br><span class="line"># Here&#x27;s a compatibility matrix that shows the Kafka client versions that are compatible with each combination</span><br><span class="line"># of Logstash and the Kafka input plugin:</span><br><span class="line">#</span><br><span class="line"># [options=&quot;header&quot;]</span><br><span class="line"># |==========================================================</span><br><span class="line"># |Kafka Client Version |Logstash Version |Plugin Version |Security Features |Why?</span><br><span class="line"># |0.8       |2.0.0 - 2.x.x   |&lt;3.0.0 | |Legacy, 0.8 is still popular</span><br><span class="line"># |0.9       |2.0.0 - 2.3.x   | 3.x.x |Basic Auth, SSL |Works with the old Ruby Event API (`event[&#x27;product&#x27;][&#x27;price&#x27;] = 10`)</span><br><span class="line"># |0.9       |2.4.0 - 5.0.x   | 4.x.x |Basic Auth, SSL |Works with the new getter/setter APIs (`event.set(&#x27;[product][price]&#x27;, 10)`)</span><br><span class="line"># |0.10      |2.4.0 - 5.0.x   | 5.x.x |Basic Auth, SSL |Not compatible with the 0.9 broker</span><br><span class="line"># |==========================================================</span><br><span class="line">#</span><br><span class="line"># NOTE: We recommended that you use matching Kafka client and broker versions. During upgrades, you should</span><br><span class="line"># upgrade brokers before clients because brokers target backwards compatibility. For example, the 0.9 broker</span><br><span class="line"># is compatible with both the 0.8 consumer and 0.9 consumer APIs, but not the other way around.</span><br></pre></td></tr></table></figure>

<p>6）升级kafka_2.10-0.8.2.2.tgz为kafka_2.11-0.10.0.1.tgz （我看kafka-clients-0.10.0.1.jar，所以没有用最新的kafka_2.11-0.10.1.0.tgz）</p>
<p>大概步骤 关闭老kafka &#x2F;usr&#x2F;local&#x2F;kafka&#x2F;bin&#x2F;kafka-server-stop.sh &#x2F;usr&#x2F;local&#x2F;kafka&#x2F;config&#x2F;server.properties 备份老配置文件 server.properties和zookeeper.properties 删除kafka rm -rf &#x2F;usr&#x2F;local&#x2F;kafka&#x2F; rm -rf &#x2F;data&#x2F;kafkalogs&#x2F;*</p>
<p>安装配置新kafka wget <a target="_blank" rel="noopener" href="http://mirrors.hust.edu.cn/apache/kafka/0.10.0.1/kafka/_2.11-0.10.0.1.tgz">http://mirrors.hust.edu.cn/apache/kafka/0.10.0.1/kafka\_2.11-0.10.0.1.tgz</a> tar zxvf kafka_2.11-0.10.0.1.tgz -C &#x2F;usr&#x2F;local&#x2F; ln -s &#x2F;usr&#x2F;local&#x2F;kafka_2.11-0.10.0.1 &#x2F;usr&#x2F;local&#x2F;kafka diff了下server.properties和zookeeper.properties变动不大可以直接使用</p>
<p>启动新kafka &#x2F;usr&#x2F;local&#x2F;kafka&#x2F;bin&#x2F;kafka-server-start.sh &#x2F;usr&#x2F;local&#x2F;kafka&#x2F;config&#x2F;server.properties &amp;</p>
<p>7)注意几个关键配置需要修改 config :bootstrap_servers, :validate &#x3D;&gt; :string, :default &#x3D;&gt; “localhost:9092” config :group_id, :validate &#x3D;&gt; :string, :default &#x3D;&gt; “logstash” config :topics, :validate &#x3D;&gt; :array, :default &#x3D;&gt; [“logstash”] config :consumer_threads, :validate &#x3D;&gt; :number, :default &#x3D;&gt; 1</p>
<p>除了上面的几个关键配置外，kafka的topic分片信息需要重新create一份，否则KafkaMonitor监控不出Active Topic Consumer图形，但实际是在工作中。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://minminmsn.github.io/2018/12/06/2018/12/2018-12-06-centos-7-5%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2jewel%E7%89%88%E6%9C%ACceph%E9%9B%86%E7%BE%A4/index/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jerry Min">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="MinMinMsn">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/12/06/2018/12/2018-12-06-centos-7-5%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2jewel%E7%89%88%E6%9C%ACceph%E9%9B%86%E7%BE%A4/index/" class="post-title-link" itemprop="url">CentOS-7-5安装部署Jewel版本Ceph集群</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-12-06 08:00:00" itemprop="dateCreated datePublished" datetime="2018-12-06T08:00:00+08:00">2018-12-06</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2023-05-26 15:06:30" itemprop="dateModified" datetime="2023-05-26T15:06:30+08:00">2023-05-26</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a><strong>参考文档</strong></h3><blockquote>
<p><a target="_blank" rel="noopener" href="https://www.linuxidc.com/Linux/2017-09/146760.htm">https://www.linuxidc.com/Linux/2017-09/146760.htm</a> <a target="_blank" rel="noopener" href="https://www.cnblogs.com/luohaixian/p/8087591.html">https://www.cnblogs.com/luohaixian/p/8087591.html</a> <a target="_blank" rel="noopener" href="http://docs.ceph.com/docs/master/start/quick-start-preflight/#rhel-centos">http://docs.ceph.com/docs/master/start/quick-start-preflight/#rhel-centos</a></p>
</blockquote>
<h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a><strong>简介</strong></h3><blockquote>
<p>Ceph的核心组件包括Ceph OSD、Ceph Monitor、Ceph MDS和Ceph RWG。 Ceph OSD：OSD的英文全称是Object Storage Device，它的主要功能是存储数据、复制数据、平衡数据、恢复数据等，与其它OSD间进行心跳检查等，并将一些变化情况上报给Ceph Monitor。一般情况下一块硬盘对应一个OSD，由OSD来对硬盘存储进行管理，当然一个分区也可以成为一个OSD。 Ceph Monitor：由该英文名字我们可以知道它是一个监视器，负责监视Ceph集群，维护Ceph集群的健康状态，同时维护着Ceph集群中的各种Map图，比如OSD Map、Monitor Map、PG Map和CRUSH Map，这些Map统称为Cluster Map，Cluster Map是RADOS的关键数据结构，管理集群中的所有成员、关系、属性等信息以及数据的分发，比如当用户需要存储数据到Ceph集群时，OSD需要先通过Monitor获取最新的Map图，然后根据Map图和object id等计算出数据最终存储的位置。 Ceph MDS：全称是Ceph MetaData Server，主要保存的文件系统服务的元数据，但对象存储和块存储设备是不需要使用该服务的。 Ceph RWG：RGW为Rados Gateway的缩写，ceph通过RGW为互联网云服务提供商提供对象存储服务。RGW在librados之上向应用提供访问ceph集群的RestAPI， 支持Amazon S3和openstack swift两种接口。对RGW最直接的理解就是一个协议转换层，把从上层应用符合S3或Swift协议的请求转换成rados的请求， 将数据保存在rados集群中。</p>
</blockquote>
<h3 id="架构图"><a href="#架构图" class="headerlink" title="架构图"></a><strong>架构图</strong></h3><blockquote>
<p><img src="https://upload-images.jianshu.io/upload_images/7535971-a46a76f38c878dca.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></p>
</blockquote>
<h3 id="安装部署"><a href="#安装部署" class="headerlink" title="安装部署"></a><strong>安装部署</strong></h3><h4 id="一、基础环境"><a href="#一、基础环境" class="headerlink" title="一、基础环境"></a>一、基础环境</h4><p>0、服务分布</p>
<blockquote>
<p>mon ceph0、ceph2、cphe3 注意mon为奇数节点 osd ceph0、ceph1、ceph2、ceph3 rgw ceph1 deploy ceph0</p>
</blockquote>
<p>1、host解析</p>
<blockquote>
<p>[root@idcv-ceph0 ~]# cat &#x2F;etc&#x2F;hosts 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 172.20.1.138 idcv-ceph0 172.20.1.139 idcv-ceph1 172.20.1.140 idcv-ceph2 172.20.1.141 idcv-ceph3</p>
</blockquote>
<p>2、ntp时间同步</p>
<blockquote>
<p>[root@idcv-ceph0 ~]# ntpdate 172.20.0.63</p>
</blockquote>
<p>3、ssh免密码登陆</p>
<blockquote>
<p>[root@idcv-ceph0 ~]# ssh-keygen [root@idcv-ceph0 ~]# ssh-copy-id root@idcv-ceph1 [root@idcv-ceph0 ~]# ssh-copy-id root@idcv-ceph2 [root@idcv-ceph0 ~]# ssh-copy-id root@idcv-ceph3</p>
</blockquote>
<p>4、update系统</p>
<blockquote>
<p>[root@idcv-ceph0 ~]# yum update</p>
</blockquote>
<p>5、关闭selinux</p>
<blockquote>
<p>[root@idcv-ceph0 ~]# sed -i ‘s&#x2F;enforcing&#x2F;disabled&#x2F;g’ &#x2F;etc&#x2F;selinux&#x2F;config</p>
</blockquote>
<p>6、关闭iptables</p>
<blockquote>
<p>[root@idcv-ceph0 ~]# systemctl disable firewalld</p>
</blockquote>
<p>7、reboot</p>
<blockquote>
<p>[root@idcv-ceph0 ~]# reboot</p>
</blockquote>
<h4 id="二、安装部署deploy节点"><a href="#二、安装部署deploy节点" class="headerlink" title="二、安装部署deploy节点"></a>二、安装部署deploy节点</h4><p>1、设置国内yum源</p>
<blockquote>
<p>[root@idcv-ceph0 ~]# cat &#x2F;etc&#x2F;yum.repos.d&#x2F;ceph.repo [Ceph] name&#x3D;Ceph packages for $basearch baseurl&#x3D;<a target="_blank" rel="noopener" href="http://mirrors.aliyun.com/ceph/rpm-jewel/el7/$basearch">http://mirrors.aliyun.com/ceph/rpm-jewel/el7/$basearch</a> enabled&#x3D;1 gpgcheck&#x3D;1 type&#x3D;rpm-md gpgkey&#x3D;<a target="_blank" rel="noopener" href="https://mirrors.aliyun.com/ceph/keys/release.asc">https://mirrors.aliyun.com/ceph/keys/release.asc</a> priority&#x3D;1 [Ceph-noarch] name&#x3D;Ceph noarch packages baseurl&#x3D;<a target="_blank" rel="noopener" href="http://mirrors.aliyun.com/ceph/rpm-jewel/el7/noarch">http://mirrors.aliyun.com/ceph/rpm-jewel/el7/noarch</a> enabled&#x3D;1 gpgcheck&#x3D;1 type&#x3D;rpm-md gpgkey&#x3D;<a target="_blank" rel="noopener" href="https://mirrors.aliyun.com/ceph/keys/release.asc">https://mirrors.aliyun.com/ceph/keys/release.asc</a> priority&#x3D;1 [ceph-source] name&#x3D;Ceph source packages baseurl&#x3D;<a target="_blank" rel="noopener" href="http://mirrors.aliyun.com/ceph/rpm-jewel/el7/SRPMS">http://mirrors.aliyun.com/ceph/rpm-jewel/el7/SRPMS</a> enabled&#x3D;1 gpgcheck&#x3D;1 type&#x3D;rpm-md gpgkey&#x3D;<a target="_blank" rel="noopener" href="https://mirrors.aliyun.com/ceph/keys/release.asc">https://mirrors.aliyun.com/ceph/keys/release.asc</a> priority&#x3D;1</p>
</blockquote>
<p>2、安装ceph-deploy</p>
<blockquote>
<p>[root@idcv-ceph0 ~]# yum install ceph-deploy [root@idcv-ceph0 ~]# ceph-deploy –version 1.5.39 [root@idcv-ceph0 ~]# ceph -v ceph version 10.2.10 (5dc1e4c05cb68dbf62ae6fce3f0700e4654fdbbe)</p>
</blockquote>
<p>3、创建部署目录并部署集群</p>
<blockquote>
<p>[root@idcv-ceph0 ~]# mkdir cluster [root@idcv-ceph0 ~]# cd cluster [root@idcv-ceph0 cluster]# ceph-deploy new idcv-ceph0 idcv-ceph1 idcv-ceph2 idcv-ceph3 [ceph_deploy.conf][DEBUG ] found configuration file at: &#x2F;root&#x2F;.cephdeploy.conf [ceph_deploy.cli][INFO ] Invoked (1.5.39): &#x2F;usr&#x2F;bin&#x2F;ceph-deploy new idcv-ceph0 idcv-ceph1 idcv-ceph2 idcv-ceph3 [ceph_deploy.cli][INFO ] ceph-deploy options: [ceph_deploy.cli][INFO ] username : None [ceph_deploy.cli][INFO ] func : [ceph_deploy.cli][INFO ] verbose : False [ceph_deploy.cli][INFO ] overwrite_conf : False [ceph_deploy.cli][INFO ] quiet : False [ceph_deploy.cli][INFO ] cd_conf : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x7f7c5ff1bcf8&gt; [ceph_deploy.cli][INFO ] cluster : ceph [ceph_deploy.cli][INFO ] ssh_copykey : True [ceph_deploy.cli][INFO ] mon : [‘idcv-ceph0’, ‘idcv-ceph1’, ‘idcv-ceph2’, ‘idcv-ceph3’] [ceph_deploy.cli][INFO ] public_network : None [ceph_deploy.cli][INFO ] ceph_conf : None [ceph_deploy.cli][INFO ] cluster_network : None [ceph_deploy.cli][INFO ] default_release : False [ceph_deploy.cli][INFO ] fsid : None [ceph_deploy.new][DEBUG ] Creating new cluster named ceph [ceph_deploy.new][INFO ] making sure passwordless SSH succeeds [idcv-ceph0][DEBUG ] connected to host: idcv-ceph0 [idcv-ceph0][DEBUG ] detect platform information from remote host [idcv-ceph0][DEBUG ] detect machine type [idcv-ceph0][DEBUG ] find the location of an executable [idcv-ceph0][INFO ] Running command: &#x2F;usr&#x2F;sbin&#x2F;ip link show [idcv-ceph0][INFO ] Running command: &#x2F;usr&#x2F;sbin&#x2F;ip addr show [idcv-ceph0][DEBUG ] IP addresses found: [u’172.20.1.138’] [ceph_deploy.new][DEBUG ] Resolving host idcv-ceph0 [ceph_deploy.new][DEBUG ] Monitor idcv-ceph0 at 172.20.1.138 [ceph_deploy.new][INFO ] making sure passwordless SSH succeeds [idcv-ceph1][DEBUG ] connected to host: idcv-ceph0 [idcv-ceph1][INFO ] Running command: ssh -CT -o BatchMode&#x3D;yes idcv-ceph1 [idcv-ceph1][DEBUG ] connection detected need for sudo [idcv-ceph1][DEBUG ] connected to host: idcv-ceph1 [idcv-ceph1][DEBUG ] detect platform information from remote host [idcv-ceph1][DEBUG ] detect machine type [idcv-ceph1][DEBUG ] find the location of an executable [idcv-ceph1][INFO ] Running command: sudo &#x2F;usr&#x2F;sbin&#x2F;ip link show [idcv-ceph1][INFO ] Running command: sudo &#x2F;usr&#x2F;sbin&#x2F;ip addr show [idcv-ceph1][DEBUG ] IP addresses found: [u’172.20.1.139’] [ceph_deploy.new][DEBUG ] Resolving host idcv-ceph1 [ceph_deploy.new][DEBUG ] Monitor idcv-ceph1 at 172.20.1.139 [ceph_deploy.new][INFO ] making sure passwordless SSH succeeds [idcv-ceph2][DEBUG ] connected to host: idcv-ceph0 [idcv-ceph2][INFO ] Running command: ssh -CT -o BatchMode&#x3D;yes idcv-ceph2 [idcv-ceph2][DEBUG ] connection detected need for sudo [idcv-ceph2][DEBUG ] connected to host: idcv-ceph2 [idcv-ceph2][DEBUG ] detect platform information from remote host [idcv-ceph2][DEBUG ] detect machine type [idcv-ceph2][DEBUG ] find the location of an executable [idcv-ceph2][INFO ] Running command: sudo &#x2F;usr&#x2F;sbin&#x2F;ip link show [idcv-ceph2][INFO ] Running command: sudo &#x2F;usr&#x2F;sbin&#x2F;ip addr show [idcv-ceph2][DEBUG ] IP addresses found: [u’172.20.1.140’] [ceph_deploy.new][DEBUG ] Resolving host idcv-ceph2 [ceph_deploy.new][DEBUG ] Monitor idcv-ceph2 at 172.20.1.140 [ceph_deploy.new][INFO ] making sure passwordless SSH succeeds [idcv-ceph3][DEBUG ] connected to host: idcv-ceph0 [idcv-ceph3][INFO ] Running command: ssh -CT -o BatchMode&#x3D;yes idcv-ceph3 [idcv-ceph3][DEBUG ] connection detected need for sudo [idcv-ceph3][DEBUG ] connected to host: idcv-ceph3 [idcv-ceph3][DEBUG ] detect platform information from remote host [idcv-ceph3][DEBUG ] detect machine type [idcv-ceph3][DEBUG ] find the location of an executable [idcv-ceph3][INFO ] Running command: sudo &#x2F;usr&#x2F;sbin&#x2F;ip link show [idcv-ceph3][INFO ] Running command: sudo &#x2F;usr&#x2F;sbin&#x2F;ip addr show [idcv-ceph3][DEBUG ] IP addresses found: [u’172.20.1.141’] [ceph_deploy.new][DEBUG ] Resolving host idcv-ceph3 [ceph_deploy.new][DEBUG ] Monitor idcv-ceph3 at 172.20.1.141 [ceph_deploy.new][DEBUG ] Monitor initial members are [‘idcv-ceph0’, ‘idcv-ceph1’, ‘idcv-ceph2’, ‘idcv-ceph3’] [ceph_deploy.new][DEBUG ] Monitor addrs are [‘172.20.1.138’, ‘172.20.1.139’, ‘172.20.1.140’, ‘172.20.1.141’] [ceph_deploy.new][DEBUG ] Creating a random mon key… [ceph_deploy.new][DEBUG ] Writing monitor keyring to ceph.mon.keyring… [ceph_deploy.new][DEBUG ] Writing initial config to ceph.conf…</p>
</blockquote>
<h4 id="三、安装mon服务"><a href="#三、安装mon服务" class="headerlink" title="三、安装mon服务"></a>三、安装mon服务</h4><p>1、修改cpeh.conf文件 注意mon为奇数，如果为偶数，有一个不会安装，另外设置好public_network，并稍微增大mon之间时差允许范围(默认为0.05s，现改为2s)</p>
<blockquote>
<p>[root@idcv-ceph0 cluster]# cat ceph.conf [global] fsid &#x3D; 812d3acb-eaa8-4355-9a74-64f2cd5209b3 mon_initial_members &#x3D; idcv-ceph0, idcv-ceph1, idcv-ceph2, idcv-ceph3 mon_host &#x3D; 172.20.1.138,172.20.1.139,172.20.1.140,172.20.1.141 auth_cluster_required &#x3D; cephx auth_service_required &#x3D; cephx auth_client_required &#x3D; cephx public_network &#x3D; 172.20.0.0&#x2F;20 mon_clock_drift_allowed &#x3D; 2</p>
</blockquote>
<p>2、开始部署mon服务</p>
<blockquote>
<p>[root@idcv-ceph0 cluster]# ceph-deploy mon create-initial [ceph_deploy.conf][DEBUG ] found configuration file at: &#x2F;root&#x2F;.cephdeploy.conf [ceph_deploy.cli][INFO ] Invoked (1.5.39): &#x2F;usr&#x2F;bin&#x2F;ceph-deploy mon create-initial [ceph_deploy.cli][INFO ] ceph-deploy options: [ceph_deploy.cli][INFO ] username : None [ceph_deploy.cli][INFO ] verbose : False [ceph_deploy.cli][INFO ] overwrite_conf : False [ceph_deploy.cli][INFO ] subcommand : create-initial [ceph_deploy.cli][INFO ] quiet : False [ceph_deploy.cli][INFO ] cd_conf : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x7fd263377368&gt; [ceph_deploy.cli][INFO ] cluster : ceph [ceph_deploy.cli][INFO ] func : [ceph_deploy.cli][INFO ] ceph_conf : None [ceph_deploy.cli][INFO ] default_release : False [ceph_deploy.cli][INFO ] keyrings : None [ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts idcv-ceph0 idcv-ceph1 idcv-ceph2 idcv-ceph3 [ceph_deploy.mon][DEBUG ] detecting platform for host idcv-ceph0 … [idcv-ceph0][DEBUG ] connected to host: idcv-ceph0 [idcv-ceph0][DEBUG ] detect platform information from remote host [idcv-ceph0][DEBUG ] detect machine type [idcv-ceph0][DEBUG ] find the location of an executable [ceph_deploy.mon][INFO ] distro info: CentOS Linux 7.5.1804 Core [idcv-ceph0][DEBUG ] determining if provided host has same hostname in remote [idcv-ceph0][DEBUG ] get remote short hostname [idcv-ceph0][DEBUG ] deploying mon to idcv-ceph0 [idcv-ceph0][DEBUG ] get remote short hostname [idcv-ceph0][DEBUG ] remote hostname: idcv-ceph0 [idcv-ceph0][DEBUG ] write cluster configuration to &#x2F;etc&#x2F;ceph&#x2F;{cluster}.conf [idcv-ceph0][DEBUG ] create the mon path if it does not exist [idcv-ceph0][DEBUG ] checking for done path: &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;mon&#x2F;ceph-idcv-ceph0&#x2F;done [idcv-ceph0][DEBUG ] done path does not exist: &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;mon&#x2F;ceph-idcv-ceph0&#x2F;done [idcv-ceph0][INFO ] creating keyring file: &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;ceph-idcv-ceph0.mon.keyring [idcv-ceph0][DEBUG ] create the monitor keyring file [idcv-ceph0][INFO ] Running command: ceph-mon –cluster ceph –mkfs -i idcv-ceph0 –keyring &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;ceph-idcv-ceph0.mon.keyring –setuser 167 –setgroup 167 [idcv-ceph0][DEBUG ] ceph-mon: renaming mon.noname-a 172.20.1.138:6789&#x2F;0 to mon.idcv-ceph0 [idcv-ceph0][DEBUG ] ceph-mon: set fsid to 812d3acb-eaa8-4355-9a74-64f2cd5209b3 [idcv-ceph0][DEBUG ] ceph-mon: created monfs at &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;mon&#x2F;ceph-idcv-ceph0 for mon.idcv-ceph0 [idcv-ceph0][INFO ] unlinking keyring file &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;ceph-idcv-ceph0.mon.keyring [idcv-ceph0][DEBUG ] create a done file to avoid re-doing the mon deployment [idcv-ceph0][DEBUG ] create the init path if it does not exist [idcv-ceph0][INFO ] Running command: systemctl enable ceph.target [idcv-ceph0][INFO ] Running command: systemctl enable ceph-mon@idcv-ceph0 [idcv-ceph0][WARNIN] Created symlink from &#x2F;etc&#x2F;systemd&#x2F;system&#x2F;ceph-mon.target.wants&#x2F;<a href="mailto:&#99;&#x65;&#112;&#104;&#x2d;&#109;&#x6f;&#110;&#x40;&#x69;&#100;&#99;&#118;&#45;&#x63;&#x65;&#112;&#104;&#x30;&#46;&#115;&#101;&#114;&#118;&#x69;&#x63;&#x65;">&#99;&#x65;&#112;&#104;&#x2d;&#109;&#x6f;&#110;&#x40;&#x69;&#100;&#99;&#118;&#45;&#x63;&#x65;&#112;&#104;&#x30;&#46;&#115;&#101;&#114;&#118;&#x69;&#x63;&#x65;</a> to &#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;ceph-mon@.service. [idcv-ceph0][INFO ] Running command: systemctl start ceph-mon@idcv-ceph0 [idcv-ceph0][INFO ] Running command: ceph –cluster&#x3D;ceph –admin-daemon &#x2F;var&#x2F;run&#x2F;ceph&#x2F;ceph-mon.idcv-ceph0.asok mon_status [idcv-ceph0][DEBUG ] ******************************************************************************** [idcv-ceph0][DEBUG ] status for monitor: mon.idcv-ceph0 [idcv-ceph0][DEBUG ] { [idcv-ceph0][DEBUG ] “election_epoch”: 0, [idcv-ceph0][DEBUG ] “extra_probe_peers”: [ [idcv-ceph0][DEBUG ] “172.20.1.139:6789&#x2F;0”, [idcv-ceph0][DEBUG ] “172.20.1.140:6789&#x2F;0”, [idcv-ceph0][DEBUG ] “172.20.1.141:6789&#x2F;0” [idcv-ceph0][DEBUG ] ], [idcv-ceph0][DEBUG ] “monmap”: { [idcv-ceph0][DEBUG ] “created”: “2018-07-03 11:06:12.249491”, [idcv-ceph0][DEBUG ] “epoch”: 0, [idcv-ceph0][DEBUG ] “fsid”: “812d3acb-eaa8-4355-9a74-64f2cd5209b3”, [idcv-ceph0][DEBUG ] “modified”: “2018-07-03 11:06:12.249491”, [idcv-ceph0][DEBUG ] “mons”: [ [idcv-ceph0][DEBUG ] { [idcv-ceph0][DEBUG ] “addr”: “172.20.1.138:6789&#x2F;0”, [idcv-ceph0][DEBUG ] “name”: “idcv-ceph0”, [idcv-ceph0][DEBUG ] “rank”: 0 [idcv-ceph0][DEBUG ] }, [idcv-ceph0][DEBUG ] { [idcv-ceph0][DEBUG ] “addr”: “0.0.0.0:0&#x2F;1”, [idcv-ceph0][DEBUG ] “name”: “idcv-ceph1”, [idcv-ceph0][DEBUG ] “rank”: 1 [idcv-ceph0][DEBUG ] }, [idcv-ceph0][DEBUG ] { [idcv-ceph0][DEBUG ] “addr”: “0.0.0.0:0&#x2F;2”, [idcv-ceph0][DEBUG ] “name”: “idcv-ceph2”, [idcv-ceph0][DEBUG ] “rank”: 2 [idcv-ceph0][DEBUG ] }, [idcv-ceph0][DEBUG ] { [idcv-ceph0][DEBUG ] “addr”: “0.0.0.0:0&#x2F;3”, [idcv-ceph0][DEBUG ] “name”: “idcv-ceph3”, [idcv-ceph0][DEBUG ] “rank”: 3 [idcv-ceph0][DEBUG ] } [idcv-ceph0][DEBUG ] ] [idcv-ceph0][DEBUG ] }, [idcv-ceph0][DEBUG ] “name”: “idcv-ceph0”, [idcv-ceph0][DEBUG ] “outside_quorum”: [ [idcv-ceph0][DEBUG ] “idcv-ceph0” [idcv-ceph0][DEBUG ] ], [idcv-ceph0][DEBUG ] “quorum”: [], [idcv-ceph0][DEBUG ] “rank”: 0, [idcv-ceph0][DEBUG ] “state”: “probing”, [idcv-ceph0][DEBUG ] “sync_provider”: [] [idcv-ceph0][DEBUG ] } [idcv-ceph0][DEBUG ] ******************************************************************************** [idcv-ceph0][INFO ] monitor: mon.idcv-ceph0 is running [idcv-ceph0][INFO ] Running command: ceph –cluster&#x3D;ceph –admin-daemon &#x2F;var&#x2F;run&#x2F;ceph&#x2F;ceph-mon.idcv-ceph0.asok mon_status [ceph_deploy.mon][DEBUG ] detecting platform for host idcv-ceph1 … [idcv-ceph1][DEBUG ] connection detected need for sudo [idcv-ceph1][DEBUG ] connected to host: idcv-ceph1 [idcv-ceph1][DEBUG ] detect platform information from remote host [idcv-ceph1][DEBUG ] detect machine type [idcv-ceph1][DEBUG ] find the location of an executable [ceph_deploy.mon][INFO ] distro info: CentOS Linux 7.5.1804 Core [idcv-ceph1][DEBUG ] determining if provided host has same hostname in remote [idcv-ceph1][DEBUG ] get remote short hostname [idcv-ceph1][DEBUG ] deploying mon to idcv-ceph1 [idcv-ceph1][DEBUG ] get remote short hostname [idcv-ceph1][DEBUG ] remote hostname: idcv-ceph1 [idcv-ceph1][DEBUG ] write cluster configuration to &#x2F;etc&#x2F;ceph&#x2F;{cluster}.conf [ceph_deploy.mon][ERROR ] RuntimeError: config file &#x2F;etc&#x2F;ceph&#x2F;ceph.conf exists with different content; use –overwrite-conf to overwrite [ceph_deploy.mon][DEBUG ] detecting platform for host idcv-ceph2 … [idcv-ceph2][DEBUG ] connection detected need for sudo [idcv-ceph2][DEBUG ] connected to host: idcv-ceph2 [idcv-ceph2][DEBUG ] detect platform information from remote host [idcv-ceph2][DEBUG ] detect machine type [idcv-ceph2][DEBUG ] find the location of an executable [ceph_deploy.mon][INFO ] distro info: CentOS Linux 7.5.1804 Core [idcv-ceph2][DEBUG ] determining if provided host has same hostname in remote [idcv-ceph2][DEBUG ] get remote short hostname [idcv-ceph2][DEBUG ] deploying mon to idcv-ceph2 [idcv-ceph2][DEBUG ] get remote short hostname [idcv-ceph2][DEBUG ] remote hostname: idcv-ceph2 [idcv-ceph2][DEBUG ] write cluster configuration to &#x2F;etc&#x2F;ceph&#x2F;{cluster}.conf [idcv-ceph2][DEBUG ] create the mon path if it does not exist [idcv-ceph2][DEBUG ] checking for done path: &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;mon&#x2F;ceph-idcv-ceph2&#x2F;done [idcv-ceph2][DEBUG ] done path does not exist: &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;mon&#x2F;ceph-idcv-ceph2&#x2F;done [idcv-ceph2][INFO ] creating keyring file: &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;ceph-idcv-ceph2.mon.keyring [idcv-ceph2][DEBUG ] create the monitor keyring file [idcv-ceph2][INFO ] Running command: sudo ceph-mon –cluster ceph –mkfs -i idcv-ceph2 –keyring &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;ceph-idcv-ceph2.mon.keyring –setuser 167 –setgroup 167 [idcv-ceph2][DEBUG ] ceph-mon: renaming mon.noname-c 172.20.1.140:6789&#x2F;0 to mon.idcv-ceph2 [idcv-ceph2][DEBUG ] ceph-mon: set fsid to 812d3acb-eaa8-4355-9a74-64f2cd5209b3 [idcv-ceph2][DEBUG ] ceph-mon: created monfs at &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;mon&#x2F;ceph-idcv-ceph2 for mon.idcv-ceph2 [idcv-ceph2][INFO ] unlinking keyring file &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;ceph-idcv-ceph2.mon.keyring [idcv-ceph2][DEBUG ] create a done file to avoid re-doing the mon deployment [idcv-ceph2][DEBUG ] create the init path if it does not exist [idcv-ceph2][INFO ] Running command: sudo systemctl enable ceph.target [idcv-ceph2][INFO ] Running command: sudo systemctl enable ceph-mon@idcv-ceph2 [idcv-ceph2][WARNIN] Created symlink from &#x2F;etc&#x2F;systemd&#x2F;system&#x2F;ceph-mon.target.wants&#x2F;<a href="mailto:&#99;&#x65;&#112;&#104;&#45;&#109;&#111;&#110;&#64;&#x69;&#100;&#99;&#118;&#x2d;&#99;&#x65;&#x70;&#104;&#50;&#x2e;&#115;&#x65;&#x72;&#118;&#105;&#99;&#x65;">&#99;&#x65;&#112;&#104;&#45;&#109;&#111;&#110;&#64;&#x69;&#100;&#99;&#118;&#x2d;&#99;&#x65;&#x70;&#104;&#50;&#x2e;&#115;&#x65;&#x72;&#118;&#105;&#99;&#x65;</a> to &#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;ceph-mon@.service. [idcv-ceph2][INFO ] Running command: sudo systemctl start ceph-mon@idcv-ceph2 [idcv-ceph2][INFO ] Running command: sudo ceph –cluster&#x3D;ceph –admin-daemon &#x2F;var&#x2F;run&#x2F;ceph&#x2F;ceph-mon.idcv-ceph2.asok mon_status [idcv-ceph2][DEBUG ] ******************************************************************************** [idcv-ceph2][DEBUG ] status for monitor: mon.idcv-ceph2 [idcv-ceph2][DEBUG ] { [idcv-ceph2][DEBUG ] “election_epoch”: 0, [idcv-ceph2][DEBUG ] “extra_probe_peers”: [ [idcv-ceph2][DEBUG ] “172.20.1.138:6789&#x2F;0”, [idcv-ceph2][DEBUG ] “172.20.1.139:6789&#x2F;0”, [idcv-ceph2][DEBUG ] “172.20.1.141:6789&#x2F;0” [idcv-ceph2][DEBUG ] ], [idcv-ceph2][DEBUG ] “monmap”: { [idcv-ceph2][DEBUG ] “created”: “2018-07-03 11:06:15.703352”, [idcv-ceph2][DEBUG ] “epoch”: 0, [idcv-ceph2][DEBUG ] “fsid”: “812d3acb-eaa8-4355-9a74-64f2cd5209b3”, [idcv-ceph2][DEBUG ] “modified”: “2018-07-03 11:06:15.703352”, [idcv-ceph2][DEBUG ] “mons”: [ [idcv-ceph2][DEBUG ] { [idcv-ceph2][DEBUG ] “addr”: “172.20.1.138:6789&#x2F;0”, [idcv-ceph2][DEBUG ] “name”: “idcv-ceph0”, [idcv-ceph2][DEBUG ] “rank”: 0 [idcv-ceph2][DEBUG ] }, [idcv-ceph2][DEBUG ] { [idcv-ceph2][DEBUG ] “addr”: “172.20.1.140:6789&#x2F;0”, [idcv-ceph2][DEBUG ] “name”: “idcv-ceph2”, [idcv-ceph2][DEBUG ] “rank”: 1 [idcv-ceph2][DEBUG ] }, [idcv-ceph2][DEBUG ] { [idcv-ceph2][DEBUG ] “addr”: “0.0.0.0:0&#x2F;2”, [idcv-ceph2][DEBUG ] “name”: “idcv-ceph1”, [idcv-ceph2][DEBUG ] “rank”: 2 [idcv-ceph2][DEBUG ] }, [idcv-ceph2][DEBUG ] { [idcv-ceph2][DEBUG ] “addr”: “0.0.0.0:0&#x2F;3”, [idcv-ceph2][DEBUG ] “name”: “idcv-ceph3”, [idcv-ceph2][DEBUG ] “rank”: 3 [idcv-ceph2][DEBUG ] } [idcv-ceph2][DEBUG ] ] [idcv-ceph2][DEBUG ] }, [idcv-ceph2][DEBUG ] “name”: “idcv-ceph2”, [idcv-ceph2][DEBUG ] “outside_quorum”: [ [idcv-ceph2][DEBUG ] “idcv-ceph0”, [idcv-ceph2][DEBUG ] “idcv-ceph2” [idcv-ceph2][DEBUG ] ], [idcv-ceph2][DEBUG ] “quorum”: [], [idcv-ceph2][DEBUG ] “rank”: 1, [idcv-ceph2][DEBUG ] “state”: “probing”, [idcv-ceph2][DEBUG ] “sync_provider”: [] [idcv-ceph2][DEBUG ] } [idcv-ceph2][DEBUG ] ******************************************************************************** [idcv-ceph2][INFO ] monitor: mon.idcv-ceph2 is running [idcv-ceph2][INFO ] Running command: sudo ceph –cluster&#x3D;ceph –admin-daemon &#x2F;var&#x2F;run&#x2F;ceph&#x2F;ceph-mon.idcv-ceph2.asok mon_status [ceph_deploy.mon][DEBUG ] detecting platform for host idcv-ceph3 … [idcv-ceph3][DEBUG ] connection detected need for sudo [idcv-ceph3][DEBUG ] connected to host: idcv-ceph3 [idcv-ceph3][DEBUG ] detect platform information from remote host [idcv-ceph3][DEBUG ] detect machine type [idcv-ceph3][DEBUG ] find the location of an executable [ceph_deploy.mon][INFO ] distro info: CentOS Linux 7.5.1804 Core [idcv-ceph3][DEBUG ] determining if provided host has same hostname in remote [idcv-ceph3][DEBUG ] get remote short hostname [idcv-ceph3][DEBUG ] deploying mon to idcv-ceph3 [idcv-ceph3][DEBUG ] get remote short hostname [idcv-ceph3][DEBUG ] remote hostname: idcv-ceph3 [idcv-ceph3][DEBUG ] write cluster configuration to &#x2F;etc&#x2F;ceph&#x2F;{cluster}.conf [idcv-ceph3][DEBUG ] create the mon path if it does not exist [idcv-ceph3][DEBUG ] checking for done path: &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;mon&#x2F;ceph-idcv-ceph3&#x2F;done [idcv-ceph3][DEBUG ] done path does not exist: &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;mon&#x2F;ceph-idcv-ceph3&#x2F;done [idcv-ceph3][INFO ] creating keyring file: &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;ceph-idcv-ceph3.mon.keyring [idcv-ceph3][DEBUG ] create the monitor keyring file [idcv-ceph3][INFO ] Running command: sudo ceph-mon –cluster ceph –mkfs -i idcv-ceph3 –keyring &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;ceph-idcv-ceph3.mon.keyring –setuser 167 –setgroup 167 [idcv-ceph3][DEBUG ] ceph-mon: renaming mon.noname-d 172.20.1.141:6789&#x2F;0 to mon.idcv-ceph3 [idcv-ceph3][DEBUG ] ceph-mon: set fsid to 812d3acb-eaa8-4355-9a74-64f2cd5209b3 [idcv-ceph3][DEBUG ] ceph-mon: created monfs at &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;mon&#x2F;ceph-idcv-ceph3 for mon.idcv-ceph3 [idcv-ceph3][INFO ] unlinking keyring file &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;ceph-idcv-ceph3.mon.keyring [idcv-ceph3][DEBUG ] create a done file to avoid re-doing the mon deployment [idcv-ceph3][DEBUG ] create the init path if it does not exist [idcv-ceph3][INFO ] Running command: sudo systemctl enable ceph.target [idcv-ceph3][INFO ] Running command: sudo systemctl enable ceph-mon@idcv-ceph3 [idcv-ceph3][WARNIN] Created symlink from &#x2F;etc&#x2F;systemd&#x2F;system&#x2F;ceph-mon.target.wants&#x2F;<a href="mailto:&#99;&#x65;&#112;&#104;&#x2d;&#109;&#111;&#110;&#x40;&#x69;&#100;&#99;&#x76;&#x2d;&#x63;&#x65;&#112;&#104;&#x33;&#x2e;&#x73;&#101;&#x72;&#118;&#x69;&#x63;&#x65;">&#99;&#x65;&#112;&#104;&#x2d;&#109;&#111;&#110;&#x40;&#x69;&#100;&#99;&#x76;&#x2d;&#x63;&#x65;&#112;&#104;&#x33;&#x2e;&#x73;&#101;&#x72;&#118;&#x69;&#x63;&#x65;</a> to &#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;ceph-mon@.service. [idcv-ceph3][INFO ] Running command: sudo systemctl start ceph-mon@idcv-ceph3 [idcv-ceph3][INFO ] Running command: sudo ceph –cluster&#x3D;ceph –admin-daemon &#x2F;var&#x2F;run&#x2F;ceph&#x2F;ceph-mon.idcv-ceph3.asok mon_status [idcv-ceph3][DEBUG ] ******************************************************************************** [idcv-ceph3][DEBUG ] status for monitor: mon.idcv-ceph3 [idcv-ceph3][DEBUG ] { [idcv-ceph3][DEBUG ] “election_epoch”: 1, [idcv-ceph3][DEBUG ] “extra_probe_peers”: [ [idcv-ceph3][DEBUG ] “172.20.1.138:6789&#x2F;0”, [idcv-ceph3][DEBUG ] “172.20.1.139:6789&#x2F;0”, [idcv-ceph3][DEBUG ] “172.20.1.140:6789&#x2F;0” [idcv-ceph3][DEBUG ] ], [idcv-ceph3][DEBUG ] “monmap”: { [idcv-ceph3][DEBUG ] “created”: “2018-07-03 11:06:18.695039”, [idcv-ceph3][DEBUG ] “epoch”: 0, [idcv-ceph3][DEBUG ] “fsid”: “812d3acb-eaa8-4355-9a74-64f2cd5209b3”, [idcv-ceph3][DEBUG ] “modified”: “2018-07-03 11:06:18.695039”, [idcv-ceph3][DEBUG ] “mons”: [ [idcv-ceph3][DEBUG ] { [idcv-ceph3][DEBUG ] “addr”: “172.20.1.138:6789&#x2F;0”, [idcv-ceph3][DEBUG ] “name”: “idcv-ceph0”, [idcv-ceph3][DEBUG ] “rank”: 0 [idcv-ceph3][DEBUG ] }, [idcv-ceph3][DEBUG ] { [idcv-ceph3][DEBUG ] “addr”: “172.20.1.140:6789&#x2F;0”, [idcv-ceph3][DEBUG ] “name”: “idcv-ceph2”, [idcv-ceph3][DEBUG ] “rank”: 1 [idcv-ceph3][DEBUG ] }, [idcv-ceph3][DEBUG ] { [idcv-ceph3][DEBUG ] “addr”: “172.20.1.141:6789&#x2F;0”, [idcv-ceph3][DEBUG ] “name”: “idcv-ceph3”, [idcv-ceph3][DEBUG ] “rank”: 2 [idcv-ceph3][DEBUG ] }, [idcv-ceph3][DEBUG ] { [idcv-ceph3][DEBUG ] “addr”: “0.0.0.0:0&#x2F;2”, [idcv-ceph3][DEBUG ] “name”: “idcv-ceph1”, [idcv-ceph3][DEBUG ] “rank”: 3 [idcv-ceph3][DEBUG ] } [idcv-ceph3][DEBUG ] ] [idcv-ceph3][DEBUG ] }, [idcv-ceph3][DEBUG ] “name”: “idcv-ceph3”, [idcv-ceph3][DEBUG ] “outside_quorum”: [], [idcv-ceph3][DEBUG ] “quorum”: [], [idcv-ceph3][DEBUG ] “rank”: 2, [idcv-ceph3][DEBUG ] “state”: “electing”, [idcv-ceph3][DEBUG ] “sync_provider”: [] [idcv-ceph3][DEBUG ] } [idcv-ceph3][DEBUG ] ******************************************************************************** [idcv-ceph3][INFO ] monitor: mon.idcv-ceph3 is running [idcv-ceph3][INFO ] Running command: sudo ceph –cluster&#x3D;ceph –admin-daemon &#x2F;var&#x2F;run&#x2F;ceph&#x2F;ceph-mon.idcv-ceph3.asok mon_status [ceph_deploy][ERROR ] GenericError: Failed to create 1 monitors</p>
</blockquote>
<p>3、注意mon节点只能是奇数，根据上面报错有一个节点没有安装成功mon服务，需要把idcv-ceph1删掉</p>
<blockquote>
<p>[root@idcv-ceph0 cluster]# cat ceph.conf [global] fsid &#x3D; 812d3acb-eaa8-4355-9a74-64f2cd5209b3 mon_initial_members &#x3D; idcv-ceph0, idcv-ceph1, idcv-ceph2, idcv-ceph3 mon_host &#x3D; 172.20.1.138,172.20.1.139,172.20.1.140,172.20.1.141 auth_cluster_required &#x3D; cephx auth_service_required &#x3D; cephx auth_client_required &#x3D; cephx public_network &#x3D; 172.20.0.0&#x2F;20 mon_clock_drift_allowed &#x3D; 2 [root@idcv-ceph0 cluster]# ceph mon remove idcv-ceph1 removing mon.idcv-ceph1 at 0.0.0.0:0&#x2F;1, there will be 3 monitors [root@idcv-ceph0 cluster]# ceph -s cluster 812d3acb-eaa8-4355-9a74-64f2cd5209b3 health HEALTH_ERR 64 pgs are stuck inactive for more than 300 seconds 64 pgs stuck inactive 64 pgs stuck unclean no osds monmap e2: 3 mons at {idcv-ceph0&#x3D;172.20.1.138:6789&#x2F;0,idcv-ceph2&#x3D;172.20.1.140:6789&#x2F;0,idcv-ceph3&#x3D;172.20.1.141:6789&#x2F;0} election epoch 8, quorum 0,1,2 idcv-ceph0,idcv-ceph2,idcv-ceph3 osdmap e1: 0 osds: 0 up, 0 in flags sortbitwise,require_jewel_osds pgmap v2: 64 pgs, 1 pools, 0 bytes data, 0 objects 0 kB used, 0 kB &#x2F; 0 kB avail 64 creating</p>
</blockquote>
<p>4、也可以修改ceph.conf文件，再覆盖部署一次</p>
<blockquote>
<p>[root@idcv-ceph0 cluster]# cat ceph.conf [global] fsid &#x3D; 812d3acb-eaa8-4355-9a74-64f2cd5209b3 mon_initial_members &#x3D; idcv-ceph0, idcv-ceph2, idcv-ceph3 mon_host &#x3D; 172.20.1.138,172.20.1.140,172.20.1.141 auth_cluster_required &#x3D; cephx auth_service_required &#x3D; cephx auth_client_required &#x3D; cephx public_network &#x3D; 172.20.0.0&#x2F;20 mon_clock_drift_allowed &#x3D; 2 [root@idcv-ceph0 cluster]# ceph-deploy –overwrite-conf mon create-initial [ceph_deploy.conf][DEBUG ] found configuration file at: &#x2F;root&#x2F;.cephdeploy.conf [ceph_deploy.cli][INFO ] Invoked (1.5.39): &#x2F;usr&#x2F;bin&#x2F;ceph-deploy –overwrite-conf mon create-initial [ceph_deploy.cli][INFO ] ceph-deploy options: [ceph_deploy.cli][INFO ] username : None [ceph_deploy.cli][INFO ] verbose : False [ceph_deploy.cli][INFO ] overwrite_conf : True [ceph_deploy.cli][INFO ] subcommand : create-initial [ceph_deploy.cli][INFO ] quiet : False [ceph_deploy.cli][INFO ] cd_conf : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x7fce9cf7a368&gt; [ceph_deploy.cli][INFO ] cluster : ceph [ceph_deploy.cli][INFO ] func : [ceph_deploy.cli][INFO ] ceph_conf : None [ceph_deploy.cli][INFO ] default_release : False [ceph_deploy.cli][INFO ] keyrings : None [ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts idcv-ceph0 idcv-ceph2 idcv-ceph3 [ceph_deploy.mon][DEBUG ] detecting platform for host idcv-ceph0 … [idcv-ceph0][DEBUG ] connected to host: idcv-ceph0 [idcv-ceph0][DEBUG ] detect platform information from remote host [idcv-ceph0][DEBUG ] detect machine type [idcv-ceph0][DEBUG ] find the location of an executable [ceph_deploy.mon][INFO ] distro info: CentOS Linux 7.5.1804 Core [idcv-ceph0][DEBUG ] determining if provided host has same hostname in remote [idcv-ceph0][DEBUG ] get remote short hostname [idcv-ceph0][DEBUG ] deploying mon to idcv-ceph0 [idcv-ceph0][DEBUG ] get remote short hostname [idcv-ceph0][DEBUG ] remote hostname: idcv-ceph0 [idcv-ceph0][DEBUG ] write cluster configuration to &#x2F;etc&#x2F;ceph&#x2F;{cluster}.conf [idcv-ceph0][DEBUG ] create the mon path if it does not exist [idcv-ceph0][DEBUG ] checking for done path: &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;mon&#x2F;ceph-idcv-ceph0&#x2F;done [idcv-ceph0][DEBUG ] create a done file to avoid re-doing the mon deployment [idcv-ceph0][DEBUG ] create the init path if it does not exist [idcv-ceph0][INFO ] Running command: systemctl enable ceph.target [idcv-ceph0][INFO ] Running command: systemctl enable ceph-mon@idcv-ceph0 [idcv-ceph0][INFO ] Running command: systemctl start ceph-mon@idcv-ceph0 [idcv-ceph0][INFO ] Running command: ceph –cluster&#x3D;ceph –admin-daemon &#x2F;var&#x2F;run&#x2F;ceph&#x2F;ceph-mon.idcv-ceph0.asok mon_status [idcv-ceph0][DEBUG ] ******************************************************************************** [idcv-ceph0][DEBUG ] status for monitor: mon.idcv-ceph0 [idcv-ceph0][DEBUG ] { [idcv-ceph0][DEBUG ] “election_epoch”: 8, [idcv-ceph0][DEBUG ] “extra_probe_peers”: [ [idcv-ceph0][DEBUG ] “172.20.1.139:6789&#x2F;0”, [idcv-ceph0][DEBUG ] “172.20.1.140:6789&#x2F;0”, [idcv-ceph0][DEBUG ] “172.20.1.141:6789&#x2F;0” [idcv-ceph0][DEBUG ] ], [idcv-ceph0][DEBUG ] “monmap”: { [idcv-ceph0][DEBUG ] “created”: “2018-07-03 11:06:12.249491”, [idcv-ceph0][DEBUG ] “epoch”: 2, [idcv-ceph0][DEBUG ] “fsid”: “812d3acb-eaa8-4355-9a74-64f2cd5209b3”, [idcv-ceph0][DEBUG ] “modified”: “2018-07-03 11:21:27.254076”, [idcv-ceph0][DEBUG ] “mons”: [ [idcv-ceph0][DEBUG ] { [idcv-ceph0][DEBUG ] “addr”: “172.20.1.138:6789&#x2F;0”, [idcv-ceph0][DEBUG ] “name”: “idcv-ceph0”, [idcv-ceph0][DEBUG ] “rank”: 0 [idcv-ceph0][DEBUG ] }, [idcv-ceph0][DEBUG ] { [idcv-ceph0][DEBUG ] “addr”: “172.20.1.140:6789&#x2F;0”, [idcv-ceph0][DEBUG ] “name”: “idcv-ceph2”, [idcv-ceph0][DEBUG ] “rank”: 1 [idcv-ceph0][DEBUG ] }, [idcv-ceph0][DEBUG ] { [idcv-ceph0][DEBUG ] “addr”: “172.20.1.141:6789&#x2F;0”, [idcv-ceph0][DEBUG ] “name”: “idcv-ceph3”, [idcv-ceph0][DEBUG ] “rank”: 2 [idcv-ceph0][DEBUG ] } [idcv-ceph0][DEBUG ] ] [idcv-ceph0][DEBUG ] }, [idcv-ceph0][DEBUG ] “name”: “idcv-ceph0”, [idcv-ceph0][DEBUG ] “outside_quorum”: [], [idcv-ceph0][DEBUG ] “quorum”: [ [idcv-ceph0][DEBUG ] 0, [idcv-ceph0][DEBUG ] 1, [idcv-ceph0][DEBUG ] 2 [idcv-ceph0][DEBUG ] ], [idcv-ceph0][DEBUG ] “rank”: 0, [idcv-ceph0][DEBUG ] “state”: “leader”, [idcv-ceph0][DEBUG ] “sync_provider”: [] [idcv-ceph0][DEBUG ] } [idcv-ceph0][DEBUG ] ******************************************************************************** [idcv-ceph0][INFO ] monitor: mon.idcv-ceph0 is running [idcv-ceph0][INFO ] Running command: ceph –cluster&#x3D;ceph –admin-daemon &#x2F;var&#x2F;run&#x2F;ceph&#x2F;ceph-mon.idcv-ceph0.asok mon_status [ceph_deploy.mon][DEBUG ] detecting platform for host idcv-ceph2 … [idcv-ceph2][DEBUG ] connection detected need for sudo [idcv-ceph2][DEBUG ] connected to host: idcv-ceph2 [idcv-ceph2][DEBUG ] detect platform information from remote host [idcv-ceph2][DEBUG ] detect machine type [idcv-ceph2][DEBUG ] find the location of an executable [ceph_deploy.mon][INFO ] distro info: CentOS Linux 7.5.1804 Core [idcv-ceph2][DEBUG ] determining if provided host has same hostname in remote [idcv-ceph2][DEBUG ] get remote short hostname [idcv-ceph2][DEBUG ] deploying mon to idcv-ceph2 [idcv-ceph2][DEBUG ] get remote short hostname [idcv-ceph2][DEBUG ] remote hostname: idcv-ceph2 [idcv-ceph2][DEBUG ] write cluster configuration to &#x2F;etc&#x2F;ceph&#x2F;{cluster}.conf [idcv-ceph2][DEBUG ] create the mon path if it does not exist [idcv-ceph2][DEBUG ] checking for done path: &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;mon&#x2F;ceph-idcv-ceph2&#x2F;done [idcv-ceph2][DEBUG ] create a done file to avoid re-doing the mon deployment [idcv-ceph2][DEBUG ] create the init path if it does not exist [idcv-ceph2][INFO ] Running command: sudo systemctl enable ceph.target [idcv-ceph2][INFO ] Running command: sudo systemctl enable ceph-mon@idcv-ceph2 [idcv-ceph2][INFO ] Running command: sudo systemctl start ceph-mon@idcv-ceph2 [idcv-ceph2][INFO ] Running command: sudo ceph –cluster&#x3D;ceph –admin-daemon &#x2F;var&#x2F;run&#x2F;ceph&#x2F;ceph-mon.idcv-ceph2.asok mon_status [idcv-ceph2][DEBUG ] ******************************************************************************** [idcv-ceph2][DEBUG ] status for monitor: mon.idcv-ceph2 [idcv-ceph2][DEBUG ] { [idcv-ceph2][DEBUG ] “election_epoch”: 8, [idcv-ceph2][DEBUG ] “extra_probe_peers”: [ [idcv-ceph2][DEBUG ] “172.20.1.138:6789&#x2F;0”, [idcv-ceph2][DEBUG ] “172.20.1.139:6789&#x2F;0”, [idcv-ceph2][DEBUG ] “172.20.1.141:6789&#x2F;0” [idcv-ceph2][DEBUG ] ], [idcv-ceph2][DEBUG ] “monmap”: { [idcv-ceph2][DEBUG ] “created”: “2018-07-03 11:06:12.249491”, [idcv-ceph2][DEBUG ] “epoch”: 2, [idcv-ceph2][DEBUG ] “fsid”: “812d3acb-eaa8-4355-9a74-64f2cd5209b3”, [idcv-ceph2][DEBUG ] “modified”: “2018-07-03 11:21:27.254076”, [idcv-ceph2][DEBUG ] “mons”: [ [idcv-ceph2][DEBUG ] { [idcv-ceph2][DEBUG ] “addr”: “172.20.1.138:6789&#x2F;0”, [idcv-ceph2][DEBUG ] “name”: “idcv-ceph0”, [idcv-ceph2][DEBUG ] “rank”: 0 [idcv-ceph2][DEBUG ] }, [idcv-ceph2][DEBUG ] { [idcv-ceph2][DEBUG ] “addr”: “172.20.1.140:6789&#x2F;0”, [idcv-ceph2][DEBUG ] “name”: “idcv-ceph2”, [idcv-ceph2][DEBUG ] “rank”: 1 [idcv-ceph2][DEBUG ] }, [idcv-ceph2][DEBUG ] { [idcv-ceph2][DEBUG ] “addr”: “172.20.1.141:6789&#x2F;0”, [idcv-ceph2][DEBUG ] “name”: “idcv-ceph3”, [idcv-ceph2][DEBUG ] “rank”: 2 [idcv-ceph2][DEBUG ] } [idcv-ceph2][DEBUG ] ] [idcv-ceph2][DEBUG ] }, [idcv-ceph2][DEBUG ] “name”: “idcv-ceph2”, [idcv-ceph2][DEBUG ] “outside_quorum”: [], [idcv-ceph2][DEBUG ] “quorum”: [ [idcv-ceph2][DEBUG ] 0, [idcv-ceph2][DEBUG ] 1, [idcv-ceph2][DEBUG ] 2 [idcv-ceph2][DEBUG ] ], [idcv-ceph2][DEBUG ] “rank”: 1, [idcv-ceph2][DEBUG ] “state”: “peon”, [idcv-ceph2][DEBUG ] “sync_provider”: [] [idcv-ceph2][DEBUG ] } [idcv-ceph2][DEBUG ] ******************************************************************************** [idcv-ceph2][INFO ] monitor: mon.idcv-ceph2 is running [idcv-ceph2][INFO ] Running command: sudo ceph –cluster&#x3D;ceph –admin-daemon &#x2F;var&#x2F;run&#x2F;ceph&#x2F;ceph-mon.idcv-ceph2.asok mon_status [ceph_deploy.mon][DEBUG ] detecting platform for host idcv-ceph3 … [idcv-ceph3][DEBUG ] connection detected need for sudo [idcv-ceph3][DEBUG ] connected to host: idcv-ceph3 [idcv-ceph3][DEBUG ] detect platform information from remote host [idcv-ceph3][DEBUG ] detect machine type [idcv-ceph3][DEBUG ] find the location of an executable [ceph_deploy.mon][INFO ] distro info: CentOS Linux 7.5.1804 Core [idcv-ceph3][DEBUG ] determining if provided host has same hostname in remote [idcv-ceph3][DEBUG ] get remote short hostname [idcv-ceph3][DEBUG ] deploying mon to idcv-ceph3 [idcv-ceph3][DEBUG ] get remote short hostname [idcv-ceph3][DEBUG ] remote hostname: idcv-ceph3 [idcv-ceph3][DEBUG ] write cluster configuration to &#x2F;etc&#x2F;ceph&#x2F;{cluster}.conf [idcv-ceph3][DEBUG ] create the mon path if it does not exist [idcv-ceph3][DEBUG ] checking for done path: &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;mon&#x2F;ceph-idcv-ceph3&#x2F;done [idcv-ceph3][DEBUG ] create a done file to avoid re-doing the mon deployment [idcv-ceph3][DEBUG ] create the init path if it does not exist [idcv-ceph3][INFO ] Running command: sudo systemctl enable ceph.target [idcv-ceph3][INFO ] Running command: sudo systemctl enable ceph-mon@idcv-ceph3 [idcv-ceph3][INFO ] Running command: sudo systemctl start ceph-mon@idcv-ceph3 [idcv-ceph3][INFO ] Running command: sudo ceph –cluster&#x3D;ceph –admin-daemon &#x2F;var&#x2F;run&#x2F;ceph&#x2F;ceph-mon.idcv-ceph3.asok mon_status [idcv-ceph3][DEBUG ] ******************************************************************************** [idcv-ceph3][DEBUG ] status for monitor: mon.idcv-ceph3 [idcv-ceph3][DEBUG ] { [idcv-ceph3][DEBUG ] “election_epoch”: 8, [idcv-ceph3][DEBUG ] “extra_probe_peers”: [ [idcv-ceph3][DEBUG ] “172.20.1.138:6789&#x2F;0”, [idcv-ceph3][DEBUG ] “172.20.1.139:6789&#x2F;0”, [idcv-ceph3][DEBUG ] “172.20.1.140:6789&#x2F;0” [idcv-ceph3][DEBUG ] ], [idcv-ceph3][DEBUG ] “monmap”: { [idcv-ceph3][DEBUG ] “created”: “2018-07-03 11:06:12.249491”, [idcv-ceph3][DEBUG ] “epoch”: 2, [idcv-ceph3][DEBUG ] “fsid”: “812d3acb-eaa8-4355-9a74-64f2cd5209b3”, [idcv-ceph3][DEBUG ] “modified”: “2018-07-03 11:21:27.254076”, [idcv-ceph3][DEBUG ] “mons”: [ [idcv-ceph3][DEBUG ] { [idcv-ceph3][DEBUG ] “addr”: “172.20.1.138:6789&#x2F;0”, [idcv-ceph3][DEBUG ] “name”: “idcv-ceph0”, [idcv-ceph3][DEBUG ] “rank”: 0 [idcv-ceph3][DEBUG ] }, [idcv-ceph3][DEBUG ] { [idcv-ceph3][DEBUG ] “addr”: “172.20.1.140:6789&#x2F;0”, [idcv-ceph3][DEBUG ] “name”: “idcv-ceph2”, [idcv-ceph3][DEBUG ] “rank”: 1 [idcv-ceph3][DEBUG ] }, [idcv-ceph3][DEBUG ] { [idcv-ceph3][DEBUG ] “addr”: “172.20.1.141:6789&#x2F;0”, [idcv-ceph3][DEBUG ] “name”: “idcv-ceph3”, [idcv-ceph3][DEBUG ] “rank”: 2 [idcv-ceph3][DEBUG ] } [idcv-ceph3][DEBUG ] ] [idcv-ceph3][DEBUG ] }, [idcv-ceph3][DEBUG ] “name”: “idcv-ceph3”, [idcv-ceph3][DEBUG ] “outside_quorum”: [], [idcv-ceph3][DEBUG ] “quorum”: [ [idcv-ceph3][DEBUG ] 0, [idcv-ceph3][DEBUG ] 1, [idcv-ceph3][DEBUG ] 2 [idcv-ceph3][DEBUG ] ], [idcv-ceph3][DEBUG ] “rank”: 2, [idcv-ceph3][DEBUG ] “state”: “peon”, [idcv-ceph3][DEBUG ] “sync_provider”: [] [idcv-ceph3][DEBUG ] } [idcv-ceph3][DEBUG ] ******************************************************************************** [idcv-ceph3][INFO ] monitor: mon.idcv-ceph3 is running [idcv-ceph3][INFO ] Running command: sudo ceph –cluster&#x3D;ceph –admin-daemon &#x2F;var&#x2F;run&#x2F;ceph&#x2F;ceph-mon.idcv-ceph3.asok mon_status [ceph_deploy.mon][INFO ] processing monitor mon.idcv-ceph0 [idcv-ceph0][DEBUG ] connected to host: idcv-ceph0 [idcv-ceph0][DEBUG ] detect platform information from remote host [idcv-ceph0][DEBUG ] detect machine type [idcv-ceph0][DEBUG ] find the location of an executable [idcv-ceph0][INFO ] Running command: ceph –cluster&#x3D;ceph –admin-daemon &#x2F;var&#x2F;run&#x2F;ceph&#x2F;ceph-mon.idcv-ceph0.asok mon_status [ceph_deploy.mon][INFO ] mon.idcv-ceph0 monitor has reached quorum! [ceph_deploy.mon][INFO ] processing monitor mon.idcv-ceph2 [idcv-ceph2][DEBUG ] connection detected need for sudo [idcv-ceph2][DEBUG ] connected to host: idcv-ceph2 [idcv-ceph2][DEBUG ] detect platform information from remote host [idcv-ceph2][DEBUG ] detect machine type [idcv-ceph2][DEBUG ] find the location of an executable [idcv-ceph2][INFO ] Running command: sudo ceph –cluster&#x3D;ceph –admin-daemon &#x2F;var&#x2F;run&#x2F;ceph&#x2F;ceph-mon.idcv-ceph2.asok mon_status [ceph_deploy.mon][INFO ] mon.idcv-ceph2 monitor has reached quorum! [ceph_deploy.mon][INFO ] processing monitor mon.idcv-ceph3 [idcv-ceph3][DEBUG ] connection detected need for sudo [idcv-ceph3][DEBUG ] connected to host: idcv-ceph3 [idcv-ceph3][DEBUG ] detect platform information from remote host [idcv-ceph3][DEBUG ] detect machine type [idcv-ceph3][DEBUG ] find the location of an executable [idcv-ceph3][INFO ] Running command: sudo ceph –cluster&#x3D;ceph –admin-daemon &#x2F;var&#x2F;run&#x2F;ceph&#x2F;ceph-mon.idcv-ceph3.asok mon_status [ceph_deploy.mon][INFO ] mon.idcv-ceph3 monitor has reached quorum! [ceph_deploy.mon][INFO ] all initial monitors are running and have formed quorum [ceph_deploy.mon][INFO ] Running gatherkeys… [ceph_deploy.gatherkeys][INFO ] Storing keys in temp directory &#x2F;tmp&#x2F;tmpBqY1be [idcv-ceph0][DEBUG ] connected to host: idcv-ceph0 [idcv-ceph0][DEBUG ] detect platform information from remote host [idcv-ceph0][DEBUG ] detect machine type [idcv-ceph0][DEBUG ] get remote short hostname [idcv-ceph0][DEBUG ] fetch remote file [idcv-ceph0][INFO ] Running command: &#x2F;usr&#x2F;bin&#x2F;ceph –connect-timeout&#x3D;25 –cluster&#x3D;ceph –admin-daemon&#x3D;&#x2F;var&#x2F;run&#x2F;ceph&#x2F;ceph-mon.idcv-ceph0.asok mon_status [idcv-ceph0][INFO ] Running command: &#x2F;usr&#x2F;bin&#x2F;ceph –connect-timeout&#x3D;25 –cluster&#x3D;ceph –name mon. –keyring&#x3D;&#x2F;var&#x2F;lib&#x2F;ceph&#x2F;mon&#x2F;ceph-idcv-ceph0&#x2F;keyring auth get client.admin [idcv-ceph0][INFO ] Running command: &#x2F;usr&#x2F;bin&#x2F;ceph –connect-timeout&#x3D;25 –cluster&#x3D;ceph –name mon. –keyring&#x3D;&#x2F;var&#x2F;lib&#x2F;ceph&#x2F;mon&#x2F;ceph-idcv-ceph0&#x2F;keyring auth get client.bootstrap-mds [idcv-ceph0][INFO ] Running command: &#x2F;usr&#x2F;bin&#x2F;ceph –connect-timeout&#x3D;25 –cluster&#x3D;ceph –name mon. –keyring&#x3D;&#x2F;var&#x2F;lib&#x2F;ceph&#x2F;mon&#x2F;ceph-idcv-ceph0&#x2F;keyring auth get client.bootstrap-mgr [idcv-ceph0][INFO ] Running command: &#x2F;usr&#x2F;bin&#x2F;ceph –connect-timeout&#x3D;25 –cluster&#x3D;ceph –name mon. –keyring&#x3D;&#x2F;var&#x2F;lib&#x2F;ceph&#x2F;mon&#x2F;ceph-idcv-ceph0&#x2F;keyring auth get-or-create client.bootstrap-mgr mon allow profile bootstrap-mgr [idcv-ceph0][INFO ] Running command: &#x2F;usr&#x2F;bin&#x2F;ceph –connect-timeout&#x3D;25 –cluster&#x3D;ceph –name mon. –keyring&#x3D;&#x2F;var&#x2F;lib&#x2F;ceph&#x2F;mon&#x2F;ceph-idcv-ceph0&#x2F;keyring auth get client.bootstrap-osd [idcv-ceph0][INFO ] Running command: &#x2F;usr&#x2F;bin&#x2F;ceph –connect-timeout&#x3D;25 –cluster&#x3D;ceph –name mon. –keyring&#x3D;&#x2F;var&#x2F;lib&#x2F;ceph&#x2F;mon&#x2F;ceph-idcv-ceph0&#x2F;keyring auth get client.bootstrap-rgw [ceph_deploy.gatherkeys][INFO ] Storing ceph.client.admin.keyring [ceph_deploy.gatherkeys][INFO ] Storing ceph.bootstrap-mds.keyring [ceph_deploy.gatherkeys][INFO ] Storing ceph.bootstrap-mgr.keyring [ceph_deploy.gatherkeys][INFO ] keyring ‘ceph.mon.keyring’ already exists [ceph_deploy.gatherkeys][INFO ] Storing ceph.bootstrap-osd.keyring [ceph_deploy.gatherkeys][INFO ] Storing ceph.bootstrap-rgw.keyring [ceph_deploy.gatherkeys][INFO ] Destroy temp directory &#x2F;tmp&#x2F;tmpBqY1be [root@idcv-ceph0 cluster]# ls ceph.bootstrap-mds.keyring ceph.bootstrap-osd.keyring ceph.client.admin.keyring ceph-deploy-ceph.lo</p>
</blockquote>
<h4 id="五、部署OSD角色"><a href="#五、部署OSD角色" class="headerlink" title="五、部署OSD角色"></a>五、部署OSD角色</h4><p>先准备后激活 ceph-deploy –overwrite-conf osd prepare idcv-ceph0:&#x2F;dev&#x2F;sdb idcv-ceph1:&#x2F;dev&#x2F;sdb idcv-ceph2:&#x2F;dev&#x2F;sdb idcv-ceph3:&#x2F;dev&#x2F;sdb –zap-disk ceph-deploy –overwrite-conf osd activate idcv-ceph0:&#x2F;dev&#x2F;sdb1 idcv-ceph1:&#x2F;dev&#x2F;sdb1 idcv-ceph2:&#x2F;dev&#x2F;sdb1 idcv-ceph3:&#x2F;dev&#x2F;sdb1</p>
<blockquote>
<p>[root@idcv-ceph0 cluster]# ceph-deploy –overwrite-conf osd prepare idcv-ceph0:&#x2F;dev&#x2F;sdb idcv-ceph1:&#x2F;dev&#x2F;sdb idcv-ceph2:&#x2F;dev&#x2F;sdb idcv-ceph3:&#x2F;dev&#x2F;sdb –zap-disk [ceph_deploy.conf][DEBUG ] found configuration file at: &#x2F;root&#x2F;.cephdeploy.conf [ceph_deploy.cli][INFO ] Invoked (1.5.39): &#x2F;usr&#x2F;bin&#x2F;ceph-deploy –overwrite-conf osd prepare idcv-ceph0:&#x2F;dev&#x2F;sdb idcv-ceph1:&#x2F;dev&#x2F;sdb idcv-ceph2:&#x2F;dev&#x2F;sdb idcv-ceph3:&#x2F;dev&#x2F;sdb –zap-disk [ceph_deploy.cli][INFO ] ceph-deploy options: [ceph_deploy.cli][INFO ] username : None [ceph_deploy.cli][INFO ] block_db : None [ceph_deploy.cli][INFO ] disk : [(‘idcv-ceph0’, ‘&#x2F;dev&#x2F;sdb’, None), (‘idcv-ceph1’, ‘&#x2F;dev&#x2F;sdb’, None), (‘idcv-ceph2’, ‘&#x2F;dev&#x2F;sdb’, None), (‘idcv-ceph3’, ‘&#x2F;dev&#x2F;sdb’, None)] [ceph_deploy.cli][INFO ] dmcrypt : False [ceph_deploy.cli][INFO ] verbose : False [ceph_deploy.cli][INFO ] bluestore : None [ceph_deploy.cli][INFO ] block_wal : None [ceph_deploy.cli][INFO ] overwrite_conf : True [ceph_deploy.cli][INFO ] subcommand : prepare [ceph_deploy.cli][INFO ] dmcrypt_key_dir : &#x2F;etc&#x2F;ceph&#x2F;dmcrypt-keys [ceph_deploy.cli][INFO ] quiet : False [ceph_deploy.cli][INFO ] cd_conf : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x7f103c7f35a8&gt; [ceph_deploy.cli][INFO ] cluster : ceph [ceph_deploy.cli][INFO ] fs_type : xfs [ceph_deploy.cli][INFO ] filestore : None [ceph_deploy.cli][INFO ] func : [ceph_deploy.cli][INFO ] ceph_conf : None [ceph_deploy.cli][INFO ] default_release : False [ceph_deploy.cli][INFO ] zap_disk : True [ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks idcv-ceph0:&#x2F;dev&#x2F;sdb: idcv-ceph1:&#x2F;dev&#x2F;sdb: idcv-ceph2:&#x2F;dev&#x2F;sdb: idcv-ceph3:&#x2F;dev&#x2F;sdb: [idcv-ceph0][DEBUG ] connected to host: idcv-ceph0 [idcv-ceph0][DEBUG ] detect platform information from remote host [idcv-ceph0][DEBUG ] detect machine type [idcv-ceph0][DEBUG ] find the location of an executable [ceph_deploy.osd][INFO ] Distro info: CentOS Linux 7.5.1804 Core [ceph_deploy.osd][DEBUG ] Deploying osd to idcv-ceph0 [idcv-ceph0][DEBUG ] write cluster configuration to &#x2F;etc&#x2F;ceph&#x2F;{cluster}.conf [ceph_deploy.osd][DEBUG ] Preparing host idcv-ceph0 disk &#x2F;dev&#x2F;sdb journal None activate False [idcv-ceph0][DEBUG ] find the location of an executable [idcv-ceph0][INFO ] Running command: &#x2F;usr&#x2F;sbin&#x2F;ceph-disk -v prepare –zap-disk –cluster ceph –fs-type xfs – &#x2F;dev&#x2F;sdb [idcv-ceph0][WARNIN] command: Running command: &#x2F;usr&#x2F;bin&#x2F;ceph-osd –cluster&#x3D;ceph –show-config-value&#x3D;fsid [idcv-ceph0][WARNIN] command: Running command: &#x2F;usr&#x2F;bin&#x2F;ceph-osd –check-allows-journal -i 0 –log-file $run_dir&#x2F;$cluster-osd-check.log –cluster ceph –setuser ceph –setgroup ceph [idcv-ceph0][WARNIN] command: Running command: &#x2F;usr&#x2F;bin&#x2F;ceph-osd –check-wants-journal -i 0 –log-file $run_dir&#x2F;$cluster-osd-check.log –cluster ceph –setuser ceph –setgroup ceph [idcv-ceph0][WARNIN] command: Running command: &#x2F;usr&#x2F;bin&#x2F;ceph-osd –check-needs-journal -i 0 –log-file $run_dir&#x2F;$cluster-osd-check.log –cluster ceph –setuser ceph –setgroup ceph [idcv-ceph0][WARNIN] get_dm_uuid: get_dm_uuid &#x2F;dev&#x2F;sdb uuid path is &#x2F;sys&#x2F;dev&#x2F;block&#x2F;8:16&#x2F;dm&#x2F;uuid [idcv-ceph0][WARNIN] set_type: Will colocate journal with data on &#x2F;dev&#x2F;sdb [idcv-ceph0][WARNIN] command: Running command: &#x2F;usr&#x2F;bin&#x2F;ceph-osd –cluster&#x3D;ceph –show-config-value&#x3D;osd_journal_size [idcv-ceph0][WARNIN] get_dm_uuid: get_dm_uuid &#x2F;dev&#x2F;sdb uuid path is &#x2F;sys&#x2F;dev&#x2F;block&#x2F;8:16&#x2F;dm&#x2F;uuid [idcv-ceph0][WARNIN] get_dm_uuid: get_dm_uuid &#x2F;dev&#x2F;sdb uuid path is &#x2F;sys&#x2F;dev&#x2F;block&#x2F;8:16&#x2F;dm&#x2F;uuid [idcv-ceph0][WARNIN] get_dm_uuid: get_dm_uuid &#x2F;dev&#x2F;sdb uuid path is &#x2F;sys&#x2F;dev&#x2F;block&#x2F;8:16&#x2F;dm&#x2F;uuid [idcv-ceph0][WARNIN] command: Running command: &#x2F;usr&#x2F;bin&#x2F;ceph-conf –cluster&#x3D;ceph –name&#x3D;osd. –lookup osd_mkfs_options_xfs [idcv-ceph0][WARNIN] command: Running command: &#x2F;usr&#x2F;bin&#x2F;ceph-conf –cluster&#x3D;ceph –name&#x3D;osd. –lookup osd_fs_mkfs_options_xfs [idcv-ceph0][WARNIN] command: Running command: &#x2F;usr&#x2F;bin&#x2F;ceph-conf –cluster&#x3D;ceph –name&#x3D;osd. –lookup osd_mount_options_xfs [idcv-ceph0][WARNIN] command: Running command: &#x2F;usr&#x2F;bin&#x2F;ceph-conf –cluster&#x3D;ceph –name&#x3D;osd. –lookup osd_fs_mount_options_xfs [idcv-ceph0][WARNIN] get_dm_uuid: get_dm_uuid &#x2F;dev&#x2F;sdb uuid path is &#x2F;sys&#x2F;dev&#x2F;block&#x2F;8:16&#x2F;dm&#x2F;uuid [idcv-ceph0][WARNIN] zap: Zapping partition table on &#x2F;dev&#x2F;sdb [idcv-ceph0][WARNIN] command_check_call: Running command: &#x2F;usr&#x2F;sbin&#x2F;sgdisk –zap-all – &#x2F;dev&#x2F;sdb [idcv-ceph0][WARNIN] Caution: invalid backup GPT header, but valid main header; regenerating [idcv-ceph0][WARNIN] backup header from main header. [idcv-ceph0][WARNIN] [idcv-ceph0][DEBUG ] **************************************************************************** [idcv-ceph0][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk [idcv-ceph0][DEBUG ] verification and recovery are STRONGLY recommended. [idcv-ceph0][DEBUG ] **************************************************************************** [idcv-ceph0][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or [idcv-ceph0][DEBUG ] other utilities. [idcv-ceph0][WARNIN] command_check_call: Running command: &#x2F;usr&#x2F;sbin&#x2F;sgdisk –clear –mbrtogpt – &#x2F;dev&#x2F;sdb [idcv-ceph0][DEBUG ] Creating new GPT entries. [idcv-ceph0][DEBUG ] The operation has completed successfully. [idcv-ceph0][WARNIN] update_partition: Calling partprobe on zapped device &#x2F;dev&#x2F;sdb [idcv-ceph0][WARNIN] command_check_call: Running command: &#x2F;usr&#x2F;bin&#x2F;udevadm settle –timeout&#x3D;600 [idcv-ceph0][WARNIN] command: Running command: &#x2F;usr&#x2F;bin&#x2F;flock -s &#x2F;dev&#x2F;sdb &#x2F;usr&#x2F;sbin&#x2F;partprobe &#x2F;dev&#x2F;sdb [idcv-ceph0][WARNIN] command_check_call: Running command: &#x2F;usr&#x2F;bin&#x2F;udevadm settle –timeout&#x3D;600 [idcv-ceph0][WARNIN] get_dm_uuid: get_dm_uuid &#x2F;dev&#x2F;sdb uuid path is &#x2F;sys&#x2F;dev&#x2F;block&#x2F;8:16&#x2F;dm&#x2F;uuid [idcv-ceph0][WARNIN] get_dm_uuid: get_dm_uuid &#x2F;dev&#x2F;sdb uuid path is &#x2F;sys&#x2F;dev&#x2F;block&#x2F;8:16&#x2F;dm&#x2F;uuid [idcv-ceph0][WARNIN] ptype_tobe_for_name: name &#x3D; journal [idcv-ceph0][WARNIN] get_dm_uuid: get_dm_uuid &#x2F;dev&#x2F;sdb uuid path is &#x2F;sys&#x2F;dev&#x2F;block&#x2F;8:16&#x2F;dm&#x2F;uuid [idcv-ceph0][WARNIN] create_partition: Creating journal partition num 2 size 5120 on &#x2F;dev&#x2F;sdb [idcv-ceph0][WARNIN] command_check_call: Running command: &#x2F;usr&#x2F;sbin&#x2F;sgdisk –new&#x3D;2:0:+5120M –change-name&#x3D;2:ceph journal –partition-guid&#x3D;2:ca6594bd-a4b2-4be7-9aa5-69ba91ce7441 –typecode&#x3D;2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 –mbrtogpt – &#x2F;dev&#x2F;sdb [idcv-ceph0][DEBUG ] The operation has completed successfully. [idcv-ceph0][WARNIN] update_partition: Calling partprobe on created device &#x2F;dev&#x2F;sdb [idcv-ceph0][WARNIN] command_check_call: Running command: &#x2F;usr&#x2F;bin&#x2F;udevadm settle –timeout&#x3D;600 [idcv-ceph0][WARNIN] command: Running command: &#x2F;usr&#x2F;bin&#x2F;flock -s &#x2F;dev&#x2F;sdb &#x2F;usr&#x2F;sbin&#x2F;partprobe &#x2F;dev&#x2F;sdb [idcv-ceph0][WARNIN] command_check_call: Running command: &#x2F;usr&#x2F;bin&#x2F;udevadm settle –timeout&#x3D;600 [idcv-ceph0][WARNIN] get_dm_uuid: get_dm_uuid &#x2F;dev&#x2F;sdb uuid path is &#x2F;sys&#x2F;dev&#x2F;block&#x2F;8:16&#x2F;dm&#x2F;uuid [idcv-ceph0][WARNIN] get_dm_uuid: get_dm_uuid &#x2F;dev&#x2F;sdb uuid path is &#x2F;sys&#x2F;dev&#x2F;block&#x2F;8:16&#x2F;dm&#x2F;uuid [idcv-ceph0][WARNIN] get_dm_uuid: get_dm_uuid &#x2F;dev&#x2F;sdb2 uuid path is &#x2F;sys&#x2F;dev&#x2F;block&#x2F;8:18&#x2F;dm&#x2F;uuid [idcv-ceph0][WARNIN] prepare_device: Journal is GPT partition &#x2F;dev&#x2F;disk&#x2F;by-partuuid&#x2F;ca6594bd-a4b2-4be7-9aa5-69ba91ce7441 [idcv-ceph0][WARNIN] prepare_device: Journal is GPT partition &#x2F;dev&#x2F;disk&#x2F;by-partuuid&#x2F;ca6594bd-a4b2-4be7-9aa5-69ba91ce7441 [idcv-ceph0][WARNIN] get_dm_uuid: get_dm_uuid &#x2F;dev&#x2F;sdb uuid path is &#x2F;sys&#x2F;dev&#x2F;block&#x2F;8:16&#x2F;dm&#x2F;uuid [idcv-ceph0][WARNIN] set_data_partition: Creating osd partition on &#x2F;dev&#x2F;sdb [idcv-ceph0][WARNIN] get_dm_uuid: get_dm_uuid &#x2F;dev&#x2F;sdb uuid path is &#x2F;sys&#x2F;dev&#x2F;block&#x2F;8:16&#x2F;dm&#x2F;uuid [idcv-ceph0][WARNIN] ptype_tobe_for_name: name &#x3D; data [idcv-ceph0][WARNIN] get_dm_uuid: get_dm_uuid &#x2F;dev&#x2F;sdb uuid path is &#x2F;sys&#x2F;dev&#x2F;block&#x2F;8:16&#x2F;dm&#x2F;uuid [idcv-ceph0][WARNIN] create_partition: Creating data partition num 1 size 0 on &#x2F;dev&#x2F;sdb [idcv-ceph0][WARNIN] command_check_call: Running command: &#x2F;usr&#x2F;sbin&#x2F;sgdisk –largest-new&#x3D;1 –change-name&#x3D;1:ceph data –partition-guid&#x3D;1:3b210c8e-b2ac-4266-9e59-623c031ebb89 –typecode&#x3D;1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be –mbrtogpt – &#x2F;dev&#x2F;sdb [idcv-ceph0][DEBUG ] The operation has completed successfully. [idcv-ceph0][WARNIN] update_partition: Calling partprobe on created device &#x2F;dev&#x2F;sdb [idcv-ceph0][WARNIN] command_check_call: Running command: &#x2F;usr&#x2F;bin&#x2F;udevadm settle –timeout&#x3D;600 [idcv-ceph0][WARNIN] command: Running command: &#x2F;usr&#x2F;bin&#x2F;flock -s &#x2F;dev&#x2F;sdb &#x2F;usr&#x2F;sbin&#x2F;partprobe &#x2F;dev&#x2F;sdb [idcv-ceph0][WARNIN] command_check_call: Running command: &#x2F;usr&#x2F;bin&#x2F;udevadm settle –timeout&#x3D;600 [idcv-ceph0][WARNIN] get_dm_uuid: get_dm_uuid &#x2F;dev&#x2F;sdb uuid path is &#x2F;sys&#x2F;dev&#x2F;block&#x2F;8:16&#x2F;dm&#x2F;uuid [idcv-ceph0][WARNIN] get_dm_uuid: get_dm_uuid &#x2F;dev&#x2F;sdb uuid path is &#x2F;sys&#x2F;dev&#x2F;block&#x2F;8:16&#x2F;dm&#x2F;uuid [idcv-ceph0][WARNIN] get_dm_uuid: get_dm_uuid &#x2F;dev&#x2F;sdb1 uuid path is &#x2F;sys&#x2F;dev&#x2F;block&#x2F;8:17&#x2F;dm&#x2F;uuid [idcv-ceph0][WARNIN] populate_data_path_device: Creating xfs fs on &#x2F;dev&#x2F;sdb1 [idcv-ceph0][WARNIN] command_check_call: Running command: &#x2F;usr&#x2F;sbin&#x2F;mkfs -t xfs -f -i size&#x3D;2048 – &#x2F;dev&#x2F;sdb1 [idcv-ceph0][DEBUG ] meta-data&#x3D;&#x2F;dev&#x2F;sdb1 isize&#x3D;2048 agcount&#x3D;4, agsize&#x3D;6225855 blks [idcv-ceph0][DEBUG ] &#x3D; sectsz&#x3D;512 attr&#x3D;2, projid32bit&#x3D;1 [idcv-ceph0][DEBUG ] &#x3D; crc&#x3D;1 finobt&#x3D;0, sparse&#x3D;0 [idcv-ceph0][DEBUG ] data &#x3D; bsize&#x3D;4096 blocks&#x3D;24903419, imaxpct&#x3D;25 [idcv-ceph0][DEBUG ] &#x3D; sunit&#x3D;0 swidth&#x3D;0 blks [idcv-ceph0][DEBUG ] naming &#x3D;version 2 bsize&#x3D;4096 ascii-ci&#x3D;0 ftype&#x3D;1 [idcv-ceph0][DEBUG ] log &#x3D;internal log bsize&#x3D;4096 blocks&#x3D;12159, version&#x3D;2 [idcv-ceph0][DEBUG ] &#x3D; sectsz&#x3D;512 sunit&#x3D;0 blks, lazy-count&#x3D;1 [idcv-ceph0][DEBUG ] realtime &#x3D;none extsz&#x3D;4096 blocks&#x3D;0, rtextents&#x3D;0 [idcv-ceph0][WARNIN] mount: Mounting &#x2F;dev&#x2F;sdb1 on &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.kvs_nq with options noatime,inode64 [idcv-ceph0][WARNIN] command_check_call: Running command: &#x2F;usr&#x2F;bin&#x2F;mount -t xfs -o noatime,inode64 – &#x2F;dev&#x2F;sdb1 &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.kvs_nq [idcv-ceph0][WARNIN] command: Running command: &#x2F;usr&#x2F;sbin&#x2F;restorecon &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.kvs_nq [idcv-ceph0][WARNIN] populate_data_path: Preparing osd data dir &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.kvs_nq [idcv-ceph0][WARNIN] command: Running command: &#x2F;usr&#x2F;sbin&#x2F;restorecon -R &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.kvs_nq&#x2F;ceph_fsid.2933.tmp [idcv-ceph0][WARNIN] command: Running command: &#x2F;usr&#x2F;bin&#x2F;chown -R ceph:ceph &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.kvs_nq&#x2F;ceph_fsid.2933.tmp [idcv-ceph0][WARNIN] command: Running command: &#x2F;usr&#x2F;sbin&#x2F;restorecon -R &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.kvs_nq&#x2F;fsid.2933.tmp [idcv-ceph0][WARNIN] command: Running command: &#x2F;usr&#x2F;bin&#x2F;chown -R ceph:ceph &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.kvs_nq&#x2F;fsid.2933.tmp [idcv-ceph0][WARNIN] command: Running command: &#x2F;usr&#x2F;sbin&#x2F;restorecon -R &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.kvs_nq&#x2F;magic.2933.tmp [idcv-ceph0][WARNIN] command: Running command: &#x2F;usr&#x2F;bin&#x2F;chown -R ceph:ceph &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.kvs_nq&#x2F;magic.2933.tmp [idcv-ceph0][WARNIN] command: Running command: &#x2F;usr&#x2F;sbin&#x2F;restorecon -R &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.kvs_nq&#x2F;journal_uuid.2933.tmp [idcv-ceph0][WARNIN] command: Running command: &#x2F;usr&#x2F;bin&#x2F;chown -R ceph:ceph &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.kvs_nq&#x2F;journal_uuid.2933.tmp [idcv-ceph0][WARNIN] adjust_symlink: Creating symlink &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.kvs_nq&#x2F;journal -&gt; &#x2F;dev&#x2F;disk&#x2F;by-partuuid&#x2F;ca6594bd-a4b2-4be7-9aa5-69ba91ce7441 [idcv-ceph0][WARNIN] command: Running command: &#x2F;usr&#x2F;sbin&#x2F;restorecon -R &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.kvs_nq [idcv-ceph0][WARNIN] command: Running command: &#x2F;usr&#x2F;bin&#x2F;chown -R ceph:ceph &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.kvs_nq [idcv-ceph0][WARNIN] unmount: Unmounting &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.kvs_nq [idcv-ceph0][WARNIN] command_check_call: Running command: &#x2F;bin&#x2F;umount – &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.kvs_nq [idcv-ceph0][WARNIN] get_dm_uuid: get_dm_uuid &#x2F;dev&#x2F;sdb uuid path is &#x2F;sys&#x2F;dev&#x2F;block&#x2F;8:16&#x2F;dm&#x2F;uuid [idcv-ceph0][WARNIN] command_check_call: Running command: &#x2F;usr&#x2F;sbin&#x2F;sgdisk –typecode&#x3D;1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d – &#x2F;dev&#x2F;sdb [idcv-ceph0][DEBUG ] Warning: The kernel is still using the old partition table. [idcv-ceph0][DEBUG ] The new table will be used at the next reboot. [idcv-ceph0][DEBUG ] The operation has completed successfully. [idcv-ceph0][WARNIN] update_partition: Calling partprobe on prepared device &#x2F;dev&#x2F;sdb [idcv-ceph0][WARNIN] command_check_call: Running command: &#x2F;usr&#x2F;bin&#x2F;udevadm settle –timeout&#x3D;600 [idcv-ceph0][WARNIN] command: Running command: &#x2F;usr&#x2F;bin&#x2F;flock -s &#x2F;dev&#x2F;sdb &#x2F;usr&#x2F;sbin&#x2F;partprobe &#x2F;dev&#x2F;sdb [idcv-ceph0][WARNIN] command_check_call: Running command: &#x2F;usr&#x2F;bin&#x2F;udevadm settle –timeout&#x3D;600 [idcv-ceph0][WARNIN] command_check_call: Running command: &#x2F;usr&#x2F;bin&#x2F;udevadm trigger –action&#x3D;add –sysname-match sdb1 [idcv-ceph0][INFO ] checking OSD status… [idcv-ceph0][DEBUG ] find the location of an executable [idcv-ceph0][INFO ] Running command: &#x2F;bin&#x2F;ceph –cluster&#x3D;ceph osd stat –format&#x3D;json [ceph_deploy.osd][DEBUG ] Host idcv-ceph0 is now ready for osd use. [idcv-ceph1][DEBUG ] connection detected need for sudo [idcv-ceph1][DEBUG ] connected to host: idcv-ceph1 [idcv-ceph1][DEBUG ] detect platform information from remote host [idcv-ceph1][DEBUG ] detect machine type [idcv-ceph1][DEBUG ] find the location of an executable [ceph_deploy.osd][INFO ] Distro info: CentOS Linux 7.5.1804 Core [ceph_deploy.osd][DEBUG ] Deploying osd to idcv-ceph1 [idcv-ceph1][DEBUG ] write cluster configuration to &#x2F;etc&#x2F;ceph&#x2F;{cluster}.conf [ceph_deploy.osd][DEBUG ] Preparing host idcv-ceph1 disk &#x2F;dev&#x2F;sdb journal None activate False [idcv-ceph1][DEBUG ] find the location of an executable [idcv-ceph1][INFO ] Running command: sudo &#x2F;usr&#x2F;sbin&#x2F;ceph-disk -v prepare –zap-disk –cluster ceph –fs-type xfs – &#x2F;dev&#x2F;sdb [idcv-ceph1][WARNIN] command: Running command: &#x2F;usr&#x2F;bin&#x2F;ceph-osd –cluster&#x3D;ceph –show-config-value&#x3D;fsid [idcv-ceph1][WARNIN] command: Running command: &#x2F;usr&#x2F;bin&#x2F;ceph-osd –check-allows-journal -i 0 –log-file $run_dir&#x2F;$cluster-osd-check.log –cluster ceph –setuser ceph –setgroup ceph [idcv-ceph1][WARNIN] command: Running command: &#x2F;usr&#x2F;bin&#x2F;ceph-osd –check-wants-journal -i 0 –log-file $run_dir&#x2F;$cluster-osd-check.log –cluster ceph –setuser ceph –setgroup ceph [idcv-ceph1][WARNIN] command: Running command: &#x2F;usr&#x2F;bin&#x2F;ceph-osd –check-needs-journal -i 0 –log-file $run_dir&#x2F;$cluster-osd-check.log –cluster ceph –setuser ceph –setgroup ceph [idcv-ceph1][WARNIN] get_dm_uuid: get_dm_uuid &#x2F;dev&#x2F;sdb uuid path is &#x2F;sys&#x2F;dev&#x2F;block&#x2F;8:16&#x2F;dm&#x2F;uuid [idcv-ceph1][WARNIN] set_type: Will colocate journal with data on &#x2F;dev&#x2F;sdb [idcv-ceph1][WARNIN] command: Running command: &#x2F;usr&#x2F;bin&#x2F;ceph-osd –cluster&#x3D;ceph –show-config-value&#x3D;osd_journal_size [idcv-ceph1][WARNIN] get_dm_uuid: get_dm_uuid &#x2F;dev&#x2F;sdb uuid path is &#x2F;sys&#x2F;dev&#x2F;block&#x2F;8:16&#x2F;dm&#x2F;uuid [idcv-ceph1][WARNIN] get_dm_uuid: get_dm_uuid &#x2F;dev&#x2F;sdb uuid path is &#x2F;sys&#x2F;dev&#x2F;block&#x2F;8:16&#x2F;dm&#x2F;uuid [idcv-ceph1][WARNIN] get_dm_uuid: get_dm_uuid &#x2F;dev&#x2F;sdb uuid path is &#x2F;sys&#x2F;dev&#x2F;block&#x2F;8:16&#x2F;dm&#x2F;uuid [idcv-ceph1][WARNIN] command: Running command: &#x2F;usr&#x2F;bin&#x2F;ceph-conf –cluster&#x3D;ceph –name&#x3D;osd. –lookup osd_mkfs_options_xfs [idcv-ceph1][WARNIN] command: Running command: &#x2F;usr&#x2F;bin&#x2F;ceph-conf –cluster&#x3D;ceph –name&#x3D;osd. –lookup osd_fs_mkfs_options_xfs [idcv-ceph1][WARNIN] command: Running command: &#x2F;usr&#x2F;bin&#x2F;ceph-conf –cluster&#x3D;ceph –name&#x3D;osd. –lookup osd_mount_options_xfs [idcv-ceph1][WARNIN] command: Running command: &#x2F;usr&#x2F;bin&#x2F;ceph-conf –cluster&#x3D;ceph –name&#x3D;osd. –lookup osd_fs_mount_options_xfs [idcv-ceph1][WARNIN] get_dm_uuid: get_dm_uuid &#x2F;dev&#x2F;sdb uuid path is &#x2F;sys&#x2F;dev&#x2F;block&#x2F;8:16&#x2F;dm&#x2F;uuid [idcv-ceph1][WARNIN] zap: Zapping partition table on &#x2F;dev&#x2F;sdb [idcv-ceph1][WARNIN] command_check_call: Running command: &#x2F;sbin&#x2F;sgdisk –zap-all – &#x2F;dev&#x2F;sdb [idcv-ceph1][DEBUG ] Creating new GPT entries. [idcv-ceph1][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or [idcv-ceph1][DEBUG ] other utilities. [idcv-ceph1][WARNIN] command_check_call: Running command: &#x2F;sbin&#x2F;sgdisk –clear –mbrtogpt – &#x2F;dev&#x2F;sdb [idcv-ceph1][DEBUG ] Creating new GPT entries. [idcv-ceph1][DEBUG ] The operation has completed successfully. [idcv-ceph1][WARNIN] update_partition: Calling partprobe on zapped device &#x2F;dev&#x2F;sdb [idcv-ceph1][WARNIN] command_check_call: Running command: &#x2F;usr&#x2F;bin&#x2F;udevadm settle –timeout&#x3D;600 [idcv-ceph1][WARNIN] command: Running command: &#x2F;usr&#x2F;bin&#x2F;flock -s &#x2F;dev&#x2F;sdb &#x2F;sbin&#x2F;partprobe &#x2F;dev&#x2F;sdb [idcv-ceph1][WARNIN] command_check_call: Running command: &#x2F;usr&#x2F;bin&#x2F;udevadm settle –timeout&#x3D;600 [idcv-ceph1][WARNIN] get_dm_uuid: get_dm_uuid &#x2F;dev&#x2F;sdb uuid path is &#x2F;sys&#x2F;dev&#x2F;block&#x2F;8:16&#x2F;dm&#x2F;uuid [idcv-ceph1][WARNIN] get_dm_uuid: get_dm_uuid &#x2F;dev&#x2F;sdb uuid path is &#x2F;sys&#x2F;dev&#x2F;block&#x2F;8:16&#x2F;dm&#x2F;uuid [idcv-ceph1][WARNIN] ptype_tobe_for_name: name &#x3D; journal [idcv-ceph1][WARNIN] get_dm_uuid: get_dm_uuid &#x2F;dev&#x2F;sdb uuid path is &#x2F;sys&#x2F;dev&#x2F;block&#x2F;8:16&#x2F;dm&#x2F;uuid [idcv-ceph1][WARNIN] create_partition: Creating journal partition num 2 size 5120 on &#x2F;dev&#x2F;sdb [idcv-ceph1][WARNIN] command_check_call: Running command: &#x2F;sbin&#x2F;sgdisk –new&#x3D;2:0:+5120M –change-name&#x3D;2:ceph journal –partition-guid&#x3D;2:09dad07a-985e-4733-a228-f7b1105b7385 –typecode&#x3D;2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 –mbrtogpt – &#x2F;dev&#x2F;sdb [idcv-ceph1][DEBUG ] The operation has completed successfully. [idcv-ceph1][WARNIN] update_partition: Calling partprobe on created device &#x2F;dev&#x2F;sdb [idcv-ceph1][WARNIN] command_check_call: Running command: &#x2F;usr&#x2F;bin&#x2F;udevadm settle –timeout&#x3D;600 [idcv-ceph1][WARNIN] command: Running command: &#x2F;usr&#x2F;bin&#x2F;flock -s &#x2F;dev&#x2F;sdb &#x2F;sbin&#x2F;partprobe &#x2F;dev&#x2F;sdb [idcv-ceph1][WARNIN] command_check_call: Running command: &#x2F;usr&#x2F;bin&#x2F;udevadm settle –timeout&#x3D;600 [idcv-ceph1][WARNIN] get_dm_uuid: get_dm_uuid &#x2F;dev&#x2F;sdb uuid path is &#x2F;sys&#x2F;dev&#x2F;block&#x2F;8:16&#x2F;dm&#x2F;uuid [idcv-ceph1][WARNIN] get_dm_uuid: get_dm_uuid &#x2F;dev&#x2F;sdb uuid path is &#x2F;sys&#x2F;dev&#x2F;block&#x2F;8:16&#x2F;dm&#x2F;uuid [idcv-ceph1][WARNIN] get_dm_uuid: get_dm_uuid &#x2F;dev&#x2F;sdb2 uuid path is &#x2F;sys&#x2F;dev&#x2F;block&#x2F;8:18&#x2F;dm&#x2F;uuid [idcv-ceph1][WARNIN] prepare_device: Journal is GPT partition &#x2F;dev&#x2F;disk&#x2F;by-partuuid&#x2F;09dad07a-985e-4733-a228-f7b1105b7385 [idcv-ceph1][WARNIN] prepare_device: Journal is GPT partition &#x2F;dev&#x2F;disk&#x2F;by-partuuid&#x2F;09dad07a-985e-4733-a228-f7b1105b7385 [idcv-ceph1][WARNIN] get_dm_uuid: get_dm_uuid &#x2F;dev&#x2F;sdb uuid path is &#x2F;sys&#x2F;dev&#x2F;block&#x2F;8:16&#x2F;dm&#x2F;uuid [idcv-ceph1][WARNIN] set_data_partition: Creating osd partition on &#x2F;dev&#x2F;sdb [idcv-ceph1][WARNIN] get_dm_uuid: get_dm_uuid &#x2F;dev&#x2F;sdb uuid path is &#x2F;sys&#x2F;dev&#x2F;block&#x2F;8:16&#x2F;dm&#x2F;uuid [idcv-ceph1][WARNIN] ptype_tobe_for_name: name &#x3D; data [idcv-ceph1][WARNIN] get_dm_uuid: get_dm_uuid &#x2F;dev&#x2F;sdb uuid path is &#x2F;sys&#x2F;dev&#x2F;block&#x2F;8:16&#x2F;dm&#x2F;uuid [idcv-ceph1][WARNIN] create_partition: Creating data partition num 1 size 0 on &#x2F;dev&#x2F;sdb [idcv-ceph1][WARNIN] command_check_call: Running command: &#x2F;sbin&#x2F;sgdisk –largest-new&#x3D;1 –change-name&#x3D;1:ceph data –partition-guid&#x3D;1:2809f370-e6ad-4d29-bf6b-57fe1f2004c6 –typecode&#x3D;1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be –mbrtogpt – &#x2F;dev&#x2F;sdb [idcv-ceph1][DEBUG ] The operation has completed successfully. [idcv-ceph1][WARNIN] update_partition: Calling partprobe on created device &#x2F;dev&#x2F;sdb [idcv-ceph1][WARNIN] command_check_call: Running command: &#x2F;usr&#x2F;bin&#x2F;udevadm settle –timeout&#x3D;600 [idcv-ceph1][WARNIN] command: Running command: &#x2F;usr&#x2F;bin&#x2F;flock -s &#x2F;dev&#x2F;sdb &#x2F;sbin&#x2F;partprobe &#x2F;dev&#x2F;sdb [idcv-ceph1][WARNIN] command_check_call: Running command: &#x2F;usr&#x2F;bin&#x2F;udevadm settle –timeout&#x3D;600 [idcv-ceph1][WARNIN] get_dm_uuid: get_dm_uuid &#x2F;dev&#x2F;sdb uuid path is &#x2F;sys&#x2F;dev&#x2F;block&#x2F;8:16&#x2F;dm&#x2F;uuid [idcv-ceph1][WARNIN] get_dm_uuid: get_dm_uuid &#x2F;dev&#x2F;sdb uuid path is &#x2F;sys&#x2F;dev&#x2F;block&#x2F;8:16&#x2F;dm&#x2F;uuid [idcv-ceph1][WARNIN] get_dm_uuid: get_dm_uuid &#x2F;dev&#x2F;sdb1 uuid path is &#x2F;sys&#x2F;dev&#x2F;block&#x2F;8:17&#x2F;dm&#x2F;uuid [idcv-ceph1][WARNIN] populate_data_path_device: Creating xfs fs on &#x2F;dev&#x2F;sdb1 [idcv-ceph1][WARNIN] command_check_call: Running command: &#x2F;sbin&#x2F;mkfs -t xfs -f -i size&#x3D;2048 – &#x2F;dev&#x2F;sdb1 [idcv-ceph1][DEBUG ] meta-data&#x3D;&#x2F;dev&#x2F;sdb1 isize&#x3D;2048 agcount&#x3D;4, agsize&#x3D;6225855 blks [idcv-ceph1][DEBUG ] &#x3D; sectsz&#x3D;512 attr&#x3D;2, projid32bit&#x3D;1 [idcv-ceph1][DEBUG ] &#x3D; crc&#x3D;1 finobt&#x3D;0, sparse&#x3D;0 [idcv-ceph1][DEBUG ] data &#x3D; bsize&#x3D;4096 blocks&#x3D;24903419, imaxpct&#x3D;25 [idcv-ceph1][DEBUG ] &#x3D; sunit&#x3D;0 swidth&#x3D;0 blks [idcv-ceph1][DEBUG ] naming &#x3D;version 2 bsize&#x3D;4096 ascii-ci&#x3D;0 ftype&#x3D;1 [idcv-ceph1][DEBUG ] log &#x3D;internal log bsize&#x3D;4096 blocks&#x3D;12159, version&#x3D;2 [idcv-ceph1][DEBUG ] &#x3D; sectsz&#x3D;512 sunit&#x3D;0 blks, lazy-count&#x3D;1 [idcv-ceph1][DEBUG ] realtime &#x3D;none extsz&#x3D;4096 blocks&#x3D;0, rtextents&#x3D;0 [idcv-ceph1][WARNIN] mount: Mounting &#x2F;dev&#x2F;sdb1 on &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.HAg1vC with options noatime,inode64 [idcv-ceph1][WARNIN] command_check_call: Running command: &#x2F;usr&#x2F;bin&#x2F;mount -t xfs -o noatime,inode64 – &#x2F;dev&#x2F;sdb1 &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.HAg1vC [idcv-ceph1][WARNIN] command: Running command: &#x2F;sbin&#x2F;restorecon &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.HAg1vC [idcv-ceph1][WARNIN] populate_data_path: Preparing osd data dir &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.HAg1vC [idcv-ceph1][WARNIN] command: Running command: &#x2F;sbin&#x2F;restorecon -R &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.HAg1vC&#x2F;ceph_fsid.2415.tmp [idcv-ceph1][WARNIN] command: Running command: &#x2F;usr&#x2F;bin&#x2F;chown -R ceph:ceph &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.HAg1vC&#x2F;ceph_fsid.2415.tmp [idcv-ceph1][WARNIN] command: Running command: &#x2F;sbin&#x2F;restorecon -R &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.HAg1vC&#x2F;fsid.2415.tmp [idcv-ceph1][WARNIN] command: Running command: &#x2F;usr&#x2F;bin&#x2F;chown -R ceph:ceph &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.HAg1vC&#x2F;fsid.2415.tmp [idcv-ceph1][WARNIN] command: Running command: &#x2F;sbin&#x2F;restorecon -R &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.HAg1vC&#x2F;magic.2415.tmp [idcv-ceph1][WARNIN] command: Running command: &#x2F;usr&#x2F;bin&#x2F;chown -R ceph:ceph &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.HAg1vC&#x2F;magic.2415.tmp [idcv-ceph1][WARNIN] command: Running command: &#x2F;sbin&#x2F;restorecon -R &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.HAg1vC&#x2F;journal_uuid.2415.tmp [idcv-ceph1][WARNIN] command: Running command: &#x2F;usr&#x2F;bin&#x2F;chown -R ceph:ceph &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.HAg1vC&#x2F;journal_uuid.2415.tmp [idcv-ceph1][WARNIN] adjust_symlink: Creating symlink &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.HAg1vC&#x2F;journal -&gt; &#x2F;dev&#x2F;disk&#x2F;by-partuuid&#x2F;09dad07a-985e-4733-a228-f7b1105b7385 [idcv-ceph1][WARNIN] command: Running command: &#x2F;sbin&#x2F;restorecon -R &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.HAg1vC [idcv-ceph1][WARNIN] command: Running command: &#x2F;usr&#x2F;bin&#x2F;chown -R ceph:ceph &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.HAg1vC [idcv-ceph1][WARNIN] unmount: Unmounting &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.HAg1vC [idcv-ceph1][WARNIN] command_check_call: Running command: &#x2F;bin&#x2F;umount – &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.HAg1vC [idcv-ceph1][WARNIN] get_dm_uuid: get_dm_uuid &#x2F;dev&#x2F;sdb uuid path is &#x2F;sys&#x2F;dev&#x2F;block&#x2F;8:16&#x2F;dm&#x2F;uuid [idcv-ceph1][WARNIN] command_check_call: Running command: &#x2F;sbin&#x2F;sgdisk –typecode&#x3D;1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d – &#x2F;dev&#x2F;sdb [idcv-ceph1][DEBUG ] The operation has completed successfully. [idcv-ceph1][WARNIN] update_partition: Calling partprobe on prepared device &#x2F;dev&#x2F;sdb [idcv-ceph1][WARNIN] command_check_call: Running command: &#x2F;usr&#x2F;bin&#x2F;udevadm settle –timeout&#x3D;600 [idcv-ceph1][WARNIN] command: Running command: &#x2F;usr&#x2F;bin&#x2F;flock -s &#x2F;dev&#x2F;sdb &#x2F;sbin&#x2F;partprobe &#x2F;dev&#x2F;sdb [idcv-ceph1][WARNIN] command_check_call: Running command: &#x2F;usr&#x2F;bin&#x2F;udevadm settle –timeout&#x3D;600 [idcv-ceph1][WARNIN] command_check_call: Running command: &#x2F;usr&#x2F;bin&#x2F;udevadm trigger –action&#x3D;add –sysname-match sdb1 [idcv-ceph1][INFO ] checking OSD status… [idcv-ceph1][DEBUG ] find the location of an executable [idcv-ceph1][INFO ] Running command: sudo &#x2F;bin&#x2F;ceph –cluster&#x3D;ceph osd stat –format&#x3D;json [ceph_deploy.osd][DEBUG ] Host idcv-ceph1 is now ready for osd use. [idcv-ceph2][DEBUG ] connection detected need for sudo [idcv-ceph2][DEBUG ] connected to host: idcv-ceph2 [idcv-ceph2][DEBUG ] detect platform information from remote host [idcv-ceph2][DEBUG ] detect machine type [idcv-ceph2][DEBUG ] find the location of an executable [ceph_deploy.osd][INFO ] Distro info: CentOS Linux 7.5.1804 Core [ceph_deploy.osd][DEBUG ] Deploying osd to idcv-ceph2 [idcv-ceph2][DEBUG ] write cluster configuration to &#x2F;etc&#x2F;ceph&#x2F;{cluster}.conf [ceph_deploy.osd][DEBUG ] Preparing host idcv-ceph2 disk &#x2F;dev&#x2F;sdb journal None activate False [idcv-ceph2][DEBUG ] find the location of an executable [idcv-ceph2][INFO ] Running command: sudo &#x2F;usr&#x2F;sbin&#x2F;ceph-disk -v prepare –zap-disk –cluster ceph –fs-type xfs – &#x2F;dev&#x2F;sdb [idcv-ceph2][WARNIN] command: Running command: &#x2F;usr&#x2F;bin&#x2F;ceph-osd –cluster&#x3D;ceph –show-config-value&#x3D;fsid [idcv-ceph2][WARNIN] command: Running command: &#x2F;usr&#x2F;bin&#x2F;ceph-osd –check-allows-journal -i 0 –log-file $run_dir&#x2F;$cluster-osd-check.log –cluster ceph –setuser ceph –setgroup ceph [idcv-ceph2][WARNIN] command: Running command: &#x2F;usr&#x2F;bin&#x2F;ceph-osd –check-wants-journal -i 0 –log-file $run_dir&#x2F;$cluster-osd-check.log –cluster ceph –setuser ceph –setgroup ceph [idcv-ceph2][WARNIN] command: Running command: &#x2F;usr&#x2F;bin&#x2F;ceph-osd –check-needs-journal -i 0 –log-file $run_dir&#x2F;$cluster-osd-check.log –cluster ceph –setuser ceph –setgroup ceph [idcv-ceph2][WARNIN] get_dm_uuid: get_dm_uuid &#x2F;dev&#x2F;sdb uuid path is &#x2F;sys&#x2F;dev&#x2F;block&#x2F;8:16&#x2F;dm&#x2F;uuid [idcv-ceph2][WARNIN] set_type: Will colocate journal with data on &#x2F;dev&#x2F;sdb [idcv-ceph2][WARNIN] command: Running command: &#x2F;usr&#x2F;bin&#x2F;ceph-osd –cluster&#x3D;ceph –show-config-value&#x3D;osd_journal_size [idcv-ceph2][WARNIN] get_dm_uuid: get_dm_uuid &#x2F;dev&#x2F;sdb uuid path is &#x2F;sys&#x2F;dev&#x2F;block&#x2F;8:16&#x2F;dm&#x2F;uuid [idcv-ceph2][WARNIN] get_dm_uuid: get_dm_uuid &#x2F;dev&#x2F;sdb uuid path is &#x2F;sys&#x2F;dev&#x2F;block&#x2F;8:16&#x2F;dm&#x2F;uuid [idcv-ceph2][WARNIN] get_dm_uuid: get_dm_uuid &#x2F;dev&#x2F;sdb uuid path is &#x2F;sys&#x2F;dev&#x2F;block&#x2F;8:16&#x2F;dm&#x2F;uuid [idcv-ceph2][WARNIN] command: Running command: &#x2F;usr&#x2F;bin&#x2F;ceph-conf –cluster&#x3D;ceph –name&#x3D;osd. –lookup osd_mkfs_options_xfs [idcv-ceph2][WARNIN] command: Running command: &#x2F;usr&#x2F;bin&#x2F;ceph-conf –cluster&#x3D;ceph –name&#x3D;osd. –lookup osd_fs_mkfs_options_xfs [idcv-ceph2][WARNIN] command: Running command: &#x2F;usr&#x2F;bin&#x2F;ceph-conf –cluster&#x3D;ceph –name&#x3D;osd. –lookup osd_mount_options_xfs [idcv-ceph2][WARNIN] command: Running command: &#x2F;usr&#x2F;bin&#x2F;ceph-conf –cluster&#x3D;ceph –name&#x3D;osd. –lookup osd_fs_mount_options_xfs [idcv-ceph2][WARNIN] get_dm_uuid: get_dm_uuid &#x2F;dev&#x2F;sdb uuid path is &#x2F;sys&#x2F;dev&#x2F;block&#x2F;8:16&#x2F;dm&#x2F;uuid [idcv-ceph2][WARNIN] zap: Zapping partition table on &#x2F;dev&#x2F;sdb [idcv-ceph2][WARNIN] command_check_call: Running command: &#x2F;sbin&#x2F;sgdisk –zap-all – &#x2F;dev&#x2F;sdb [idcv-ceph2][DEBUG ] Creating new GPT entries. [idcv-ceph2][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or [idcv-ceph2][DEBUG ] other utilities. [idcv-ceph2][WARNIN] command_check_call: Running command: &#x2F;sbin&#x2F;sgdisk –clear –mbrtogpt – &#x2F;dev&#x2F;sdb [idcv-ceph2][DEBUG ] Creating new GPT entries. [idcv-ceph2][DEBUG ] The operation has completed successfully. [idcv-ceph2][WARNIN] update_partition: Calling partprobe on zapped device &#x2F;dev&#x2F;sdb [idcv-ceph2][WARNIN] command_check_call: Running command: &#x2F;usr&#x2F;bin&#x2F;udevadm settle –timeout&#x3D;600 [idcv-ceph2][WARNIN] command: Running command: &#x2F;usr&#x2F;bin&#x2F;flock -s &#x2F;dev&#x2F;sdb &#x2F;sbin&#x2F;partprobe &#x2F;dev&#x2F;sdb [idcv-ceph2][WARNIN] command_check_call: Running command: &#x2F;usr&#x2F;bin&#x2F;udevadm settle –timeout&#x3D;600 [idcv-ceph2][WARNIN] get_dm_uuid: get_dm_uuid &#x2F;dev&#x2F;sdb uuid path is &#x2F;sys&#x2F;dev&#x2F;block&#x2F;8:16&#x2F;dm&#x2F;uuid [idcv-ceph2][WARNIN] get_dm_uuid: get_dm_uuid &#x2F;dev&#x2F;sdb uuid path is &#x2F;sys&#x2F;dev&#x2F;block&#x2F;8:16&#x2F;dm&#x2F;uuid [idcv-ceph2][WARNIN] ptype_tobe_for_name: name &#x3D; journal [idcv-ceph2][WARNIN] get_dm_uuid: get_dm_uuid &#x2F;dev&#x2F;sdb uuid path is &#x2F;sys&#x2F;dev&#x2F;block&#x2F;8:16&#x2F;dm&#x2F;uuid [idcv-ceph2][WARNIN] create_partition: Creating journal partition num 2 size 5120 on &#x2F;dev&#x2F;sdb [idcv-ceph2][WARNIN] command_check_call: Running command: &#x2F;sbin&#x2F;sgdisk –new&#x3D;2:0:+5120M –change-name&#x3D;2:ceph journal –partition-guid&#x3D;2:857f0966-30d5-4ad1-9e0c-abff0fbbbc4e –typecode&#x3D;2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 –mbrtogpt – &#x2F;dev&#x2F;sdb [idcv-ceph2][DEBUG ] The operation has completed successfully. [idcv-ceph2][WARNIN] update_partition: Calling partprobe on created device &#x2F;dev&#x2F;sdb [idcv-ceph2][WARNIN] command_check_call: Running command: &#x2F;usr&#x2F;bin&#x2F;udevadm settle –timeout&#x3D;600 [idcv-ceph2][WARNIN] command: Running command: &#x2F;usr&#x2F;bin&#x2F;flock -s &#x2F;dev&#x2F;sdb &#x2F;sbin&#x2F;partprobe &#x2F;dev&#x2F;sdb [idcv-ceph2][WARNIN] command_check_call: Running command: &#x2F;usr&#x2F;bin&#x2F;udevadm settle –timeout&#x3D;600 [idcv-ceph2][WARNIN] get_dm_uuid: get_dm_uuid &#x2F;dev&#x2F;sdb uuid path is &#x2F;sys&#x2F;dev&#x2F;block&#x2F;8:16&#x2F;dm&#x2F;uuid [idcv-ceph2][WARNIN] get_dm_uuid: get_dm_uuid &#x2F;dev&#x2F;sdb uuid path is &#x2F;sys&#x2F;dev&#x2F;block&#x2F;8:16&#x2F;dm&#x2F;uuid [idcv-ceph2][WARNIN] get_dm_uuid: get_dm_uuid &#x2F;dev&#x2F;sdb2 uuid path is &#x2F;sys&#x2F;dev&#x2F;block&#x2F;8:18&#x2F;dm&#x2F;uuid [idcv-ceph2][WARNIN] prepare_device: Journal is GPT partition &#x2F;dev&#x2F;disk&#x2F;by-partuuid&#x2F;857f0966-30d5-4ad1-9e0c-abff0fbbbc4e [idcv-ceph2][WARNIN] prepare_device: Journal is GPT partition &#x2F;dev&#x2F;disk&#x2F;by-partuuid&#x2F;857f0966-30d5-4ad1-9e0c-abff0fbbbc4e [idcv-ceph2][WARNIN] get_dm_uuid: get_dm_uuid &#x2F;dev&#x2F;sdb uuid path is &#x2F;sys&#x2F;dev&#x2F;block&#x2F;8:16&#x2F;dm&#x2F;uuid [idcv-ceph2][WARNIN] set_data_partition: Creating osd partition on &#x2F;dev&#x2F;sdb [idcv-ceph2][WARNIN] get_dm_uuid: get_dm_uuid &#x2F;dev&#x2F;sdb uuid path is &#x2F;sys&#x2F;dev&#x2F;block&#x2F;8:16&#x2F;dm&#x2F;uuid [idcv-ceph2][WARNIN] ptype_tobe_for_name: name &#x3D; data [idcv-ceph2][WARNIN] get_dm_uuid: get_dm_uuid &#x2F;dev&#x2F;sdb uuid path is &#x2F;sys&#x2F;dev&#x2F;block&#x2F;8:16&#x2F;dm&#x2F;uuid [idcv-ceph2][WARNIN] create_partition: Creating data partition num 1 size 0 on &#x2F;dev&#x2F;sdb [idcv-ceph2][WARNIN] command_check_call: Running command: &#x2F;sbin&#x2F;sgdisk –largest-new&#x3D;1 –change-name&#x3D;1:ceph data –partition-guid&#x3D;1:dac63cc2-6876-4004-ba3b-7786be39d392 –typecode&#x3D;1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be –mbrtogpt – &#x2F;dev&#x2F;sdb [idcv-ceph2][DEBUG ] The operation has completed successfully. [idcv-ceph2][WARNIN] update_partition: Calling partprobe on created device &#x2F;dev&#x2F;sdb [idcv-ceph2][WARNIN] command_check_call: Running command: &#x2F;usr&#x2F;bin&#x2F;udevadm settle –timeout&#x3D;600 [idcv-ceph2][WARNIN] command: Running command: &#x2F;usr&#x2F;bin&#x2F;flock -s &#x2F;dev&#x2F;sdb &#x2F;sbin&#x2F;partprobe &#x2F;dev&#x2F;sdb [idcv-ceph2][WARNIN] command_check_call: Running command: &#x2F;usr&#x2F;bin&#x2F;udevadm settle –timeout&#x3D;600 [idcv-ceph2][WARNIN] get_dm_uuid: get_dm_uuid &#x2F;dev&#x2F;sdb uuid path is &#x2F;sys&#x2F;dev&#x2F;block&#x2F;8:16&#x2F;dm&#x2F;uuid [idcv-ceph2][WARNIN] get_dm_uuid: get_dm_uuid &#x2F;dev&#x2F;sdb uuid path is &#x2F;sys&#x2F;dev&#x2F;block&#x2F;8:16&#x2F;dm&#x2F;uuid [idcv-ceph2][WARNIN] get_dm_uuid: get_dm_uuid &#x2F;dev&#x2F;sdb1 uuid path is &#x2F;sys&#x2F;dev&#x2F;block&#x2F;8:17&#x2F;dm&#x2F;uuid [idcv-ceph2][WARNIN] populate_data_path_device: Creating xfs fs on &#x2F;dev&#x2F;sdb1 [idcv-ceph2][WARNIN] command_check_call: Running command: &#x2F;sbin&#x2F;mkfs -t xfs -f -i size&#x3D;2048 – &#x2F;dev&#x2F;sdb1 [idcv-ceph2][DEBUG ] meta-data&#x3D;&#x2F;dev&#x2F;sdb1 isize&#x3D;2048 agcount&#x3D;4, agsize&#x3D;6225855 blks [idcv-ceph2][DEBUG ] &#x3D; sectsz&#x3D;512 attr&#x3D;2, projid32bit&#x3D;1 [idcv-ceph2][DEBUG ] &#x3D; crc&#x3D;1 finobt&#x3D;0, sparse&#x3D;0 [idcv-ceph2][DEBUG ] data &#x3D; bsize&#x3D;4096 blocks&#x3D;24903419, imaxpct&#x3D;25 [idcv-ceph2][DEBUG ] &#x3D; sunit&#x3D;0 swidth&#x3D;0 blks [idcv-ceph2][DEBUG ] naming &#x3D;version 2 bsize&#x3D;4096 ascii-ci&#x3D;0 ftype&#x3D;1 [idcv-ceph2][DEBUG ] log &#x3D;internal log bsize&#x3D;4096 blocks&#x3D;12159, version&#x3D;2 [idcv-ceph2][DEBUG ] &#x3D; sectsz&#x3D;512 sunit&#x3D;0 blks, lazy-count&#x3D;1 [idcv-ceph2][WARNIN] mount: Mounting &#x2F;dev&#x2F;sdb1 on &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.jhzVmR with options noatime,inode64 [idcv-ceph2][DEBUG ] realtime &#x3D;none extsz&#x3D;4096 blocks&#x3D;0, rtextents&#x3D;0 [idcv-ceph2][WARNIN] command_check_call: Running command: &#x2F;usr&#x2F;bin&#x2F;mount -t xfs -o noatime,inode64 – &#x2F;dev&#x2F;sdb1 &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.jhzVmR [idcv-ceph2][WARNIN] command: Running command: &#x2F;sbin&#x2F;restorecon &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.jhzVmR [idcv-ceph2][WARNIN] populate_data_path: Preparing osd data dir &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.jhzVmR [idcv-ceph2][WARNIN] command: Running command: &#x2F;sbin&#x2F;restorecon -R &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.jhzVmR&#x2F;ceph_fsid.2354.tmp [idcv-ceph2][WARNIN] command: Running command: &#x2F;usr&#x2F;bin&#x2F;chown -R ceph:ceph &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.jhzVmR&#x2F;ceph_fsid.2354.tmp [idcv-ceph2][WARNIN] command: Running command: &#x2F;sbin&#x2F;restorecon -R &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.jhzVmR&#x2F;fsid.2354.tmp [idcv-ceph2][WARNIN] command: Running command: &#x2F;usr&#x2F;bin&#x2F;chown -R ceph:ceph &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.jhzVmR&#x2F;fsid.2354.tmp [idcv-ceph2][WARNIN] command: Running command: &#x2F;sbin&#x2F;restorecon -R &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.jhzVmR&#x2F;magic.2354.tmp [idcv-ceph2][WARNIN] command: Running command: &#x2F;usr&#x2F;bin&#x2F;chown -R ceph:ceph &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.jhzVmR&#x2F;magic.2354.tmp [idcv-ceph2][WARNIN] command: Running command: &#x2F;sbin&#x2F;restorecon -R &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.jhzVmR&#x2F;journal_uuid.2354.tmp [idcv-ceph2][WARNIN] command: Running command: &#x2F;usr&#x2F;bin&#x2F;chown -R ceph:ceph &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.jhzVmR&#x2F;journal_uuid.2354.tmp [idcv-ceph2][WARNIN] adjust_symlink: Creating symlink &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.jhzVmR&#x2F;journal -&gt; &#x2F;dev&#x2F;disk&#x2F;by-partuuid&#x2F;857f0966-30d5-4ad1-9e0c-abff0fbbbc4e [idcv-ceph2][WARNIN] command: Running command: &#x2F;sbin&#x2F;restorecon -R &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.jhzVmR [idcv-ceph2][WARNIN] command: Running command: &#x2F;usr&#x2F;bin&#x2F;chown -R ceph:ceph &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.jhzVmR [idcv-ceph2][WARNIN] unmount: Unmounting &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.jhzVmR [idcv-ceph2][WARNIN] command_check_call: Running command: &#x2F;bin&#x2F;umount – &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.jhzVmR [idcv-ceph2][WARNIN] get_dm_uuid: get_dm_uuid &#x2F;dev&#x2F;sdb uuid path is &#x2F;sys&#x2F;dev&#x2F;block&#x2F;8:16&#x2F;dm&#x2F;uuid [idcv-ceph2][WARNIN] command_check_call: Running command: &#x2F;sbin&#x2F;sgdisk –typecode&#x3D;1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d – &#x2F;dev&#x2F;sdb [idcv-ceph2][DEBUG ] Warning: The kernel is still using the old partition table. [idcv-ceph2][DEBUG ] The new table will be used at the next reboot. [idcv-ceph2][DEBUG ] The operation has completed successfully. [idcv-ceph2][WARNIN] update_partition: Calling partprobe on prepared device &#x2F;dev&#x2F;sdb [idcv-ceph2][WARNIN] command_check_call: Running command: &#x2F;usr&#x2F;bin&#x2F;udevadm settle –timeout&#x3D;600 [idcv-ceph2][WARNIN] command: Running command: &#x2F;usr&#x2F;bin&#x2F;flock -s &#x2F;dev&#x2F;sdb &#x2F;sbin&#x2F;partprobe &#x2F;dev&#x2F;sdb [idcv-ceph2][WARNIN] command_check_call: Running command: &#x2F;usr&#x2F;bin&#x2F;udevadm settle –timeout&#x3D;600 [idcv-ceph2][WARNIN] command_check_call: Running command: &#x2F;usr&#x2F;bin&#x2F;udevadm trigger –action&#x3D;add –sysname-match sdb1 [idcv-ceph2][INFO ] checking OSD status… [idcv-ceph2][DEBUG ] find the location of an executable [idcv-ceph2][INFO ] Running command: sudo &#x2F;bin&#x2F;ceph –cluster&#x3D;ceph osd stat –format&#x3D;json [ceph_deploy.osd][DEBUG ] Host idcv-ceph2 is now ready for osd use. [idcv-ceph3][DEBUG ] connection detected need for sudo [idcv-ceph3][DEBUG ] connected to host: idcv-ceph3 [idcv-ceph3][DEBUG ] detect platform information from remote host [idcv-ceph3][DEBUG ] detect machine type [idcv-ceph3][DEBUG ] find the location of an executable [ceph_deploy.osd][INFO ] Distro info: CentOS Linux 7.5.1804 Core [ceph_deploy.osd][DEBUG ] Deploying osd to idcv-ceph3 [idcv-ceph3][DEBUG ] write cluster configuration to &#x2F;etc&#x2F;ceph&#x2F;{cluster}.conf [ceph_deploy.osd][DEBUG ] Preparing host idcv-ceph3 disk &#x2F;dev&#x2F;sdb journal None activate False [idcv-ceph3][DEBUG ] find the location of an executable [idcv-ceph3][INFO ] Running command: sudo &#x2F;usr&#x2F;sbin&#x2F;ceph-disk -v prepare –zap-disk –cluster ceph –fs-type xfs – &#x2F;dev&#x2F;sdb [idcv-ceph3][WARNIN] command: Running command: &#x2F;usr&#x2F;bin&#x2F;ceph-osd –cluster&#x3D;ceph –show-config-value&#x3D;fsid [idcv-ceph3][WARNIN] command: Running command: &#x2F;usr&#x2F;bin&#x2F;ceph-osd –check-allows-journal -i 0 –log-file $run_dir&#x2F;$cluster-osd-check.log –cluster ceph –setuser ceph –setgroup ceph [idcv-ceph3][WARNIN] command: Running command: &#x2F;usr&#x2F;bin&#x2F;ceph-osd –check-wants-journal -i 0 –log-file $run_dir&#x2F;$cluster-osd-check.log –cluster ceph –setuser ceph –setgroup ceph [idcv-ceph3][WARNIN] command: Running command: &#x2F;usr&#x2F;bin&#x2F;ceph-osd –check-needs-journal -i 0 –log-file $run_dir&#x2F;$cluster-osd-check.log –cluster ceph –setuser ceph –setgroup ceph [idcv-ceph3][WARNIN] get_dm_uuid: get_dm_uuid &#x2F;dev&#x2F;sdb uuid path is &#x2F;sys&#x2F;dev&#x2F;block&#x2F;8:16&#x2F;dm&#x2F;uuid [idcv-ceph3][WARNIN] set_type: Will colocate journal with data on &#x2F;dev&#x2F;sdb [idcv-ceph3][WARNIN] command: Running command: &#x2F;usr&#x2F;bin&#x2F;ceph-osd –cluster&#x3D;ceph –show-config-value&#x3D;osd_journal_size [idcv-ceph3][WARNIN] get_dm_uuid: get_dm_uuid &#x2F;dev&#x2F;sdb uuid path is &#x2F;sys&#x2F;dev&#x2F;block&#x2F;8:16&#x2F;dm&#x2F;uuid [idcv-ceph3][WARNIN] get_dm_uuid: get_dm_uuid &#x2F;dev&#x2F;sdb uuid path is &#x2F;sys&#x2F;dev&#x2F;block&#x2F;8:16&#x2F;dm&#x2F;uuid [idcv-ceph3][WARNIN] get_dm_uuid: get_dm_uuid &#x2F;dev&#x2F;sdb uuid path is &#x2F;sys&#x2F;dev&#x2F;block&#x2F;8:16&#x2F;dm&#x2F;uuid [idcv-ceph3][WARNIN] command: Running command: &#x2F;usr&#x2F;bin&#x2F;ceph-conf –cluster&#x3D;ceph –name&#x3D;osd. –lookup osd_mkfs_options_xfs [idcv-ceph3][WARNIN] command: Running command: &#x2F;usr&#x2F;bin&#x2F;ceph-conf –cluster&#x3D;ceph –name&#x3D;osd. –lookup osd_fs_mkfs_options_xfs [idcv-ceph3][WARNIN] command: Running command: &#x2F;usr&#x2F;bin&#x2F;ceph-conf –cluster&#x3D;ceph –name&#x3D;osd. –lookup osd_mount_options_xfs [idcv-ceph3][WARNIN] command: Running command: &#x2F;usr&#x2F;bin&#x2F;ceph-conf –cluster&#x3D;ceph –name&#x3D;osd. –lookup osd_fs_mount_options_xfs [idcv-ceph3][WARNIN] get_dm_uuid: get_dm_uuid &#x2F;dev&#x2F;sdb uuid path is &#x2F;sys&#x2F;dev&#x2F;block&#x2F;8:16&#x2F;dm&#x2F;uuid [idcv-ceph3][WARNIN] zap: Zapping partition table on &#x2F;dev&#x2F;sdb [idcv-ceph3][WARNIN] command_check_call: Running command: &#x2F;sbin&#x2F;sgdisk –zap-all – &#x2F;dev&#x2F;sdb [idcv-ceph3][DEBUG ] Creating new GPT entries. [idcv-ceph3][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or [idcv-ceph3][DEBUG ] other utilities. [idcv-ceph3][WARNIN] command_check_call: Running command: &#x2F;sbin&#x2F;sgdisk –clear –mbrtogpt – &#x2F;dev&#x2F;sdb [idcv-ceph3][DEBUG ] Creating new GPT entries. [idcv-ceph3][DEBUG ] The operation has completed successfully. [idcv-ceph3][WARNIN] update_partition: Calling partprobe on zapped device &#x2F;dev&#x2F;sdb [idcv-ceph3][WARNIN] command_check_call: Running command: &#x2F;usr&#x2F;bin&#x2F;udevadm settle –timeout&#x3D;600 [idcv-ceph3][WARNIN] command: Running command: &#x2F;usr&#x2F;bin&#x2F;flock -s &#x2F;dev&#x2F;sdb &#x2F;sbin&#x2F;partprobe &#x2F;dev&#x2F;sdb [idcv-ceph3][WARNIN] command_check_call: Running command: &#x2F;usr&#x2F;bin&#x2F;udevadm settle –timeout&#x3D;600 [idcv-ceph3][WARNIN] get_dm_uuid: get_dm_uuid &#x2F;dev&#x2F;sdb uuid path is &#x2F;sys&#x2F;dev&#x2F;block&#x2F;8:16&#x2F;dm&#x2F;uuid [idcv-ceph3][WARNIN] get_dm_uuid: get_dm_uuid &#x2F;dev&#x2F;sdb uuid path is &#x2F;sys&#x2F;dev&#x2F;block&#x2F;8:16&#x2F;dm&#x2F;uuid [idcv-ceph3][WARNIN] ptype_tobe_for_name: name &#x3D; journal [idcv-ceph3][WARNIN] get_dm_uuid: get_dm_uuid &#x2F;dev&#x2F;sdb uuid path is &#x2F;sys&#x2F;dev&#x2F;block&#x2F;8:16&#x2F;dm&#x2F;uuid [idcv-ceph3][WARNIN] create_partition: Creating journal partition num 2 size 5120 on &#x2F;dev&#x2F;sdb [idcv-ceph3][WARNIN] command_check_call: Running command: &#x2F;sbin&#x2F;sgdisk –new&#x3D;2:0:+5120M –change-name&#x3D;2:ceph journal –partition-guid&#x3D;2:52677a68-3cf4-4d9a-b2d4-8c823e1cb901 –typecode&#x3D;2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 –mbrtogpt – &#x2F;dev&#x2F;sdb [idcv-ceph3][DEBUG ] The operation has completed successfully. [idcv-ceph3][WARNIN] update_partition: Calling partprobe on created device &#x2F;dev&#x2F;sdb [idcv-ceph3][WARNIN] command_check_call: Running command: &#x2F;usr&#x2F;bin&#x2F;udevadm settle –timeout&#x3D;600 [idcv-ceph3][WARNIN] command: Running command: &#x2F;usr&#x2F;bin&#x2F;flock -s &#x2F;dev&#x2F;sdb &#x2F;sbin&#x2F;partprobe &#x2F;dev&#x2F;sdb [idcv-ceph3][WARNIN] command_check_call: Running command: &#x2F;usr&#x2F;bin&#x2F;udevadm settle –timeout&#x3D;600 [idcv-ceph3][WARNIN] get_dm_uuid: get_dm_uuid &#x2F;dev&#x2F;sdb uuid path is &#x2F;sys&#x2F;dev&#x2F;block&#x2F;8:16&#x2F;dm&#x2F;uuid [idcv-ceph3][WARNIN] get_dm_uuid: get_dm_uuid &#x2F;dev&#x2F;sdb uuid path is &#x2F;sys&#x2F;dev&#x2F;block&#x2F;8:16&#x2F;dm&#x2F;uuid [idcv-ceph3][WARNIN] get_dm_uuid: get_dm_uuid &#x2F;dev&#x2F;sdb2 uuid path is &#x2F;sys&#x2F;dev&#x2F;block&#x2F;8:18&#x2F;dm&#x2F;uuid [idcv-ceph3][WARNIN] prepare_device: Journal is GPT partition &#x2F;dev&#x2F;disk&#x2F;by-partuuid&#x2F;52677a68-3cf4-4d9a-b2d4-8c823e1cb901 [idcv-ceph3][WARNIN] prepare_device: Journal is GPT partition &#x2F;dev&#x2F;disk&#x2F;by-partuuid&#x2F;52677a68-3cf4-4d9a-b2d4-8c823e1cb901 [idcv-ceph3][WARNIN] get_dm_uuid: get_dm_uuid &#x2F;dev&#x2F;sdb uuid path is &#x2F;sys&#x2F;dev&#x2F;block&#x2F;8:16&#x2F;dm&#x2F;uuid [idcv-ceph3][WARNIN] set_data_partition: Creating osd partition on &#x2F;dev&#x2F;sdb [idcv-ceph3][WARNIN] get_dm_uuid: get_dm_uuid &#x2F;dev&#x2F;sdb uuid path is &#x2F;sys&#x2F;dev&#x2F;block&#x2F;8:16&#x2F;dm&#x2F;uuid [idcv-ceph3][WARNIN] ptype_tobe_for_name: name &#x3D; data [idcv-ceph3][WARNIN] get_dm_uuid: get_dm_uuid &#x2F;dev&#x2F;sdb uuid path is &#x2F;sys&#x2F;dev&#x2F;block&#x2F;8:16&#x2F;dm&#x2F;uuid [idcv-ceph3][WARNIN] create_partition: Creating data partition num 1 size 0 on &#x2F;dev&#x2F;sdb [idcv-ceph3][WARNIN] command_check_call: Running command: &#x2F;sbin&#x2F;sgdisk –largest-new&#x3D;1 –change-name&#x3D;1:ceph data –partition-guid&#x3D;1:a85b0288-85ce-4887-8249-497ba880fe10 –typecode&#x3D;1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be –mbrtogpt – &#x2F;dev&#x2F;sdb [idcv-ceph3][DEBUG ] The operation has completed successfully. [idcv-ceph3][WARNIN] update_partition: Calling partprobe on created device &#x2F;dev&#x2F;sdb [idcv-ceph3][WARNIN] command_check_call: Running command: &#x2F;usr&#x2F;bin&#x2F;udevadm settle –timeout&#x3D;600 [idcv-ceph3][WARNIN] command: Running command: &#x2F;usr&#x2F;bin&#x2F;flock -s &#x2F;dev&#x2F;sdb &#x2F;sbin&#x2F;partprobe &#x2F;dev&#x2F;sdb [idcv-ceph3][WARNIN] command_check_call: Running command: &#x2F;usr&#x2F;bin&#x2F;udevadm settle –timeout&#x3D;600 [idcv-ceph3][WARNIN] get_dm_uuid: get_dm_uuid &#x2F;dev&#x2F;sdb uuid path is &#x2F;sys&#x2F;dev&#x2F;block&#x2F;8:16&#x2F;dm&#x2F;uuid [idcv-ceph3][WARNIN] get_dm_uuid: get_dm_uuid &#x2F;dev&#x2F;sdb uuid path is &#x2F;sys&#x2F;dev&#x2F;block&#x2F;8:16&#x2F;dm&#x2F;uuid [idcv-ceph3][WARNIN] get_dm_uuid: get_dm_uuid &#x2F;dev&#x2F;sdb1 uuid path is &#x2F;sys&#x2F;dev&#x2F;block&#x2F;8:17&#x2F;dm&#x2F;uuid [idcv-ceph3][WARNIN] populate_data_path_device: Creating xfs fs on &#x2F;dev&#x2F;sdb1 [idcv-ceph3][WARNIN] command_check_call: Running command: &#x2F;sbin&#x2F;mkfs -t xfs -f -i size&#x3D;2048 – &#x2F;dev&#x2F;sdb1 [idcv-ceph3][DEBUG ] meta-data&#x3D;&#x2F;dev&#x2F;sdb1 isize&#x3D;2048 agcount&#x3D;4, agsize&#x3D;6225855 blks [idcv-ceph3][DEBUG ] &#x3D; sectsz&#x3D;512 attr&#x3D;2, projid32bit&#x3D;1 [idcv-ceph3][DEBUG ] &#x3D; crc&#x3D;1 finobt&#x3D;0, sparse&#x3D;0 [idcv-ceph3][DEBUG ] data &#x3D; bsize&#x3D;4096 blocks&#x3D;24903419, imaxpct&#x3D;25 [idcv-ceph3][DEBUG ] &#x3D; sunit&#x3D;0 swidth&#x3D;0 blks [idcv-ceph3][DEBUG ] naming &#x3D;version 2 bsize&#x3D;4096 ascii-ci&#x3D;0 ftype&#x3D;1 [idcv-ceph3][DEBUG ] log &#x3D;internal log bsize&#x3D;4096 blocks&#x3D;12159, version&#x3D;2 [idcv-ceph3][DEBUG ] &#x3D; sectsz&#x3D;512 sunit&#x3D;0 blks, lazy-count&#x3D;1 [idcv-ceph3][DEBUG ] realtime &#x3D;none extsz&#x3D;4096 blocks&#x3D;0, rtextents&#x3D;0 [idcv-ceph3][WARNIN] mount: Mounting &#x2F;dev&#x2F;sdb1 on &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.gjITlj with options noatime,inode64 [idcv-ceph3][WARNIN] command_check_call: Running command: &#x2F;usr&#x2F;bin&#x2F;mount -t xfs -o noatime,inode64 – &#x2F;dev&#x2F;sdb1 &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.gjITlj [idcv-ceph3][WARNIN] command: Running command: &#x2F;sbin&#x2F;restorecon &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.gjITlj [idcv-ceph3][WARNIN] populate_data_path: Preparing osd data dir &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.gjITlj [idcv-ceph3][WARNIN] command: Running command: &#x2F;sbin&#x2F;restorecon -R &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.gjITlj&#x2F;ceph_fsid.2372.tmp [idcv-ceph3][WARNIN] command: Running command: &#x2F;usr&#x2F;bin&#x2F;chown -R ceph:ceph &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.gjITlj&#x2F;ceph_fsid.2372.tmp [idcv-ceph3][WARNIN] command: Running command: &#x2F;sbin&#x2F;restorecon -R &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.gjITlj&#x2F;fsid.2372.tmp [idcv-ceph3][WARNIN] command: Running command: &#x2F;usr&#x2F;bin&#x2F;chown -R ceph:ceph &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.gjITlj&#x2F;fsid.2372.tmp [idcv-ceph3][WARNIN] command: Running command: &#x2F;sbin&#x2F;restorecon -R &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.gjITlj&#x2F;magic.2372.tmp [idcv-ceph3][WARNIN] command: Running command: &#x2F;usr&#x2F;bin&#x2F;chown -R ceph:ceph &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.gjITlj&#x2F;magic.2372.tmp [idcv-ceph3][WARNIN] command: Running command: &#x2F;sbin&#x2F;restorecon -R &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.gjITlj&#x2F;journal_uuid.2372.tmp [idcv-ceph3][WARNIN] command: Running command: &#x2F;usr&#x2F;bin&#x2F;chown -R ceph:ceph &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.gjITlj&#x2F;journal_uuid.2372.tmp [idcv-ceph3][WARNIN] adjust_symlink: Creating symlink &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.gjITlj&#x2F;journal -&gt; &#x2F;dev&#x2F;disk&#x2F;by-partuuid&#x2F;52677a68-3cf4-4d9a-b2d4-8c823e1cb901 [idcv-ceph3][WARNIN] command: Running command: &#x2F;sbin&#x2F;restorecon -R &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.gjITlj [idcv-ceph3][WARNIN] command: Running command: &#x2F;usr&#x2F;bin&#x2F;chown -R ceph:ceph &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.gjITlj [idcv-ceph3][WARNIN] unmount: Unmounting &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.gjITlj [idcv-ceph3][WARNIN] command_check_call: Running command: &#x2F;bin&#x2F;umount – &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.gjITlj [idcv-ceph3][WARNIN] get_dm_uuid: get_dm_uuid &#x2F;dev&#x2F;sdb uuid path is &#x2F;sys&#x2F;dev&#x2F;block&#x2F;8:16&#x2F;dm&#x2F;uuid [idcv-ceph3][WARNIN] command_check_call: Running command: &#x2F;sbin&#x2F;sgdisk –typecode&#x3D;1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d – &#x2F;dev&#x2F;sdb [idcv-ceph3][DEBUG ] Warning: The kernel is still using the old partition table. [idcv-ceph3][DEBUG ] The new table will be used at the next reboot. [idcv-ceph3][DEBUG ] The operation has completed successfully. [idcv-ceph3][WARNIN] update_partition: Calling partprobe on prepared device &#x2F;dev&#x2F;sdb [idcv-ceph3][WARNIN] command_check_call: Running command: &#x2F;usr&#x2F;bin&#x2F;udevadm settle –timeout&#x3D;600 [idcv-ceph3][WARNIN] command: Running command: &#x2F;usr&#x2F;bin&#x2F;flock -s &#x2F;dev&#x2F;sdb &#x2F;sbin&#x2F;partprobe &#x2F;dev&#x2F;sdb [idcv-ceph3][WARNIN] command_check_call: Running command: &#x2F;usr&#x2F;bin&#x2F;udevadm settle –timeout&#x3D;600 [idcv-ceph3][WARNIN] command_check_call: Running command: &#x2F;usr&#x2F;bin&#x2F;udevadm trigger –action&#x3D;add –sysname-match sdb1 [idcv-ceph3][INFO ] checking OSD status… [idcv-ceph3][DEBUG ] find the location of an executable [idcv-ceph3][INFO ] Running command: sudo &#x2F;bin&#x2F;ceph –cluster&#x3D;ceph osd stat –format&#x3D;json [ceph_deploy.osd][DEBUG ] Host idcv-ceph3 is now ready for osd use. [root@idcv-ceph0 cluster]# ceph-deploy –overwrite-conf osd activate idcv-ceph0:&#x2F;dev&#x2F;sdb1 idcv-ceph1:&#x2F;dev&#x2F;sdb1 idcv-ceph2:&#x2F;dev&#x2F;sdb1 idcv-ceph3:&#x2F;dev&#x2F;sdb1 [ceph_deploy.conf][DEBUG ] found configuration file at: &#x2F;root&#x2F;.cephdeploy.conf [ceph_deploy.cli][INFO ] Invoked (1.5.39): &#x2F;usr&#x2F;bin&#x2F;ceph-deploy –overwrite-conf osd activate idcv-ceph0:&#x2F;dev&#x2F;sdb1 idcv-ceph1:&#x2F;dev&#x2F;sdb1 idcv-ceph2:&#x2F;dev&#x2F;sdb1 idcv-ceph3:&#x2F;dev&#x2F;sdb1 [ceph_deploy.cli][INFO ] ceph-deploy options: [ceph_deploy.cli][INFO ] username : None [ceph_deploy.cli][INFO ] verbose : False [ceph_deploy.cli][INFO ] overwrite_conf : True [ceph_deploy.cli][INFO ] subcommand : activate [ceph_deploy.cli][INFO ] quiet : False [ceph_deploy.cli][INFO ] cd_conf : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x7fc94a47f5a8&gt; [ceph_deploy.cli][INFO ] cluster : ceph [ceph_deploy.cli][INFO ] func : [ceph_deploy.cli][INFO ] ceph_conf : None [ceph_deploy.cli][INFO ] default_release : False [ceph_deploy.cli][INFO ] disk : [(‘idcv-ceph0’, ‘&#x2F;dev&#x2F;sdb1’, None), (‘idcv-ceph1’, ‘&#x2F;dev&#x2F;sdb1’, None), (‘idcv-ceph2’, ‘&#x2F;dev&#x2F;sdb1’, None), (‘idcv-ceph3’, ‘&#x2F;dev&#x2F;sdb1’, None)] [ceph_deploy.osd][DEBUG ] Activating cluster ceph disks idcv-ceph0:&#x2F;dev&#x2F;sdb1: idcv-ceph1:&#x2F;dev&#x2F;sdb1: idcv-ceph2:&#x2F;dev&#x2F;sdb1: idcv-ceph3:&#x2F;dev&#x2F;sdb1: [idcv-ceph0][DEBUG ] connected to host: idcv-ceph0 [idcv-ceph0][DEBUG ] detect platform information from remote host [idcv-ceph0][DEBUG ] detect machine type [idcv-ceph0][DEBUG ] find the location of an executable [ceph_deploy.osd][INFO ] Distro info: CentOS Linux 7.5.1804 Core [ceph_deploy.osd][DEBUG ] activating host idcv-ceph0 disk &#x2F;dev&#x2F;sdb1 [ceph_deploy.osd][DEBUG ] will use init type: systemd [idcv-ceph0][DEBUG ] find the location of an executable [idcv-ceph0][INFO ] Running command: &#x2F;usr&#x2F;sbin&#x2F;ceph-disk -v activate –mark-init systemd –mount &#x2F;dev&#x2F;sdb1 [idcv-ceph0][WARNIN] main_activate: path &#x3D; &#x2F;dev&#x2F;sdb1 [idcv-ceph0][WARNIN] get_dm_uuid: get_dm_uuid &#x2F;dev&#x2F;sdb1 uuid path is &#x2F;sys&#x2F;dev&#x2F;block&#x2F;8:17&#x2F;dm&#x2F;uuid [idcv-ceph0][WARNIN] command: Running command: &#x2F;usr&#x2F;sbin&#x2F;blkid -o udev -p &#x2F;dev&#x2F;sdb1 [idcv-ceph0][WARNIN] command: Running command: &#x2F;sbin&#x2F;blkid -p -s TYPE -o value – &#x2F;dev&#x2F;sdb1 [idcv-ceph0][WARNIN] command: Running command: &#x2F;usr&#x2F;bin&#x2F;ceph-conf –cluster&#x3D;ceph –name&#x3D;osd. –lookup osd_mount_options_xfs [idcv-ceph0][WARNIN] command: Running command: &#x2F;usr&#x2F;bin&#x2F;ceph-conf –cluster&#x3D;ceph –name&#x3D;osd. –lookup osd_fs_mount_options_xfs [idcv-ceph0][WARNIN] mount: Mounting &#x2F;dev&#x2F;sdb1 on &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.X6wbv9 with options noatime,inode64 [idcv-ceph0][WARNIN] command_check_call: Running command: &#x2F;usr&#x2F;bin&#x2F;mount -t xfs -o noatime,inode64 – &#x2F;dev&#x2F;sdb1 &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.X6wbv9 [idcv-ceph0][WARNIN] command: Running command: &#x2F;usr&#x2F;sbin&#x2F;restorecon &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.X6wbv9 [idcv-ceph0][WARNIN] activate: Cluster uuid is 812d3acb-eaa8-4355-9a74-64f2cd5209b3 [idcv-ceph0][WARNIN] command: Running command: &#x2F;usr&#x2F;bin&#x2F;ceph-osd –cluster&#x3D;ceph –show-config-value&#x3D;fsid [idcv-ceph0][WARNIN] activate: Cluster name is ceph [idcv-ceph0][WARNIN] activate: OSD uuid is 3b210c8e-b2ac-4266-9e59-623c031ebb89 [idcv-ceph0][WARNIN] activate: OSD id is 0 [idcv-ceph0][WARNIN] activate: Marking with init system systemd [idcv-ceph0][WARNIN] command: Running command: &#x2F;usr&#x2F;sbin&#x2F;restorecon -R &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.X6wbv9&#x2F;systemd [idcv-ceph0][WARNIN] command: Running command: &#x2F;usr&#x2F;bin&#x2F;chown -R ceph:ceph &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.X6wbv9&#x2F;systemd [idcv-ceph0][WARNIN] activate: ceph osd.0 data dir is ready at &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.X6wbv9 [idcv-ceph0][WARNIN] mount_activate: ceph osd.0 already mounted in position; unmounting ours. [idcv-ceph0][WARNIN] unmount: Unmounting &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.X6wbv9 [idcv-ceph0][WARNIN] command_check_call: Running command: &#x2F;bin&#x2F;umount – &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.X6wbv9 [idcv-ceph0][WARNIN] start_daemon: Starting ceph osd.0… [idcv-ceph0][WARNIN] command_check_call: Running command: &#x2F;usr&#x2F;bin&#x2F;systemctl disable ceph-osd@0 [idcv-ceph0][WARNIN] Removed symlink &#x2F;etc&#x2F;systemd&#x2F;system&#x2F;ceph-osd.target.wants&#x2F;<a href="mailto:&#x63;&#101;&#x70;&#x68;&#x2d;&#x6f;&#115;&#100;&#64;&#x30;&#46;&#115;&#x65;&#114;&#118;&#x69;&#x63;&#x65;">&#x63;&#101;&#x70;&#x68;&#x2d;&#x6f;&#115;&#100;&#64;&#x30;&#46;&#115;&#x65;&#114;&#118;&#x69;&#x63;&#x65;</a>. [idcv-ceph0][WARNIN] command_check_call: Running command: &#x2F;usr&#x2F;bin&#x2F;systemctl disable ceph-osd@0 –runtime [idcv-ceph0][WARNIN] command_check_call: Running command: &#x2F;usr&#x2F;bin&#x2F;systemctl enable ceph-osd@0 [idcv-ceph0][WARNIN] Created symlink from &#x2F;etc&#x2F;systemd&#x2F;system&#x2F;ceph-osd.target.wants&#x2F;<a href="mailto:&#x63;&#101;&#x70;&#104;&#45;&#111;&#115;&#100;&#64;&#48;&#46;&#x73;&#101;&#x72;&#x76;&#x69;&#99;&#x65;">&#x63;&#101;&#x70;&#104;&#45;&#111;&#115;&#100;&#64;&#48;&#46;&#x73;&#101;&#x72;&#x76;&#x69;&#99;&#x65;</a> to &#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;ceph-osd@.service. [idcv-ceph0][WARNIN] command_check_call: Running command: &#x2F;usr&#x2F;bin&#x2F;systemctl start ceph-osd@0 [idcv-ceph0][INFO ] checking OSD status… [idcv-ceph0][DEBUG ] find the location of an executable [idcv-ceph0][INFO ] Running command: &#x2F;bin&#x2F;ceph –cluster&#x3D;ceph osd stat –format&#x3D;json [idcv-ceph0][INFO ] Running command: systemctl enable ceph.target [idcv-ceph1][DEBUG ] connection detected need for sudo [idcv-ceph1][DEBUG ] connected to host: idcv-ceph1 [idcv-ceph1][DEBUG ] detect platform information from remote host [idcv-ceph1][DEBUG ] detect machine type [idcv-ceph1][DEBUG ] find the location of an executable [ceph_deploy.osd][INFO ] Distro info: CentOS Linux 7.5.1804 Core [ceph_deploy.osd][DEBUG ] activating host idcv-ceph1 disk &#x2F;dev&#x2F;sdb1 [ceph_deploy.osd][DEBUG ] will use init type: systemd [idcv-ceph1][DEBUG ] find the location of an executable [idcv-ceph1][INFO ] Running command: sudo &#x2F;usr&#x2F;sbin&#x2F;ceph-disk -v activate –mark-init systemd –mount &#x2F;dev&#x2F;sdb1 [idcv-ceph1][WARNIN] main_activate: path &#x3D; &#x2F;dev&#x2F;sdb1 [idcv-ceph1][WARNIN] get_dm_uuid: get_dm_uuid &#x2F;dev&#x2F;sdb1 uuid path is &#x2F;sys&#x2F;dev&#x2F;block&#x2F;8:17&#x2F;dm&#x2F;uuid [idcv-ceph1][WARNIN] command: Running command: &#x2F;sbin&#x2F;blkid -o udev -p &#x2F;dev&#x2F;sdb1 [idcv-ceph1][WARNIN] command: Running command: &#x2F;sbin&#x2F;blkid -p -s TYPE -o value – &#x2F;dev&#x2F;sdb1 [idcv-ceph1][WARNIN] command: Running command: &#x2F;usr&#x2F;bin&#x2F;ceph-conf –cluster&#x3D;ceph –name&#x3D;osd. –lookup osd_mount_options_xfs [idcv-ceph1][WARNIN] command: Running command: &#x2F;usr&#x2F;bin&#x2F;ceph-conf –cluster&#x3D;ceph –name&#x3D;osd. –lookup osd_fs_mount_options_xfs [idcv-ceph1][WARNIN] mount: Mounting &#x2F;dev&#x2F;sdb1 on &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.zUV3_1 with options noatime,inode64 [idcv-ceph1][WARNIN] command_check_call: Running command: &#x2F;usr&#x2F;bin&#x2F;mount -t xfs -o noatime,inode64 – &#x2F;dev&#x2F;sdb1 &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.zUV3_1 [idcv-ceph1][WARNIN] command: Running command: &#x2F;sbin&#x2F;restorecon &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.zUV3_1 [idcv-ceph1][WARNIN] activate: Cluster uuid is 812d3acb-eaa8-4355-9a74-64f2cd5209b3 [idcv-ceph1][WARNIN] command: Running command: &#x2F;usr&#x2F;bin&#x2F;ceph-osd –cluster&#x3D;ceph –show-config-value&#x3D;fsid [idcv-ceph1][WARNIN] activate: Cluster name is ceph [idcv-ceph1][WARNIN] activate: OSD uuid is 2809f370-e6ad-4d29-bf6b-57fe1f2004c6 [idcv-ceph1][WARNIN] allocate_osd_id: Allocating OSD id… [idcv-ceph1][WARNIN] command: Running command: &#x2F;usr&#x2F;bin&#x2F;ceph –cluster ceph –name client.bootstrap-osd –keyring &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;bootstrap-osd&#x2F;ceph.keyring osd create –concise 2809f370-e6ad-4d29-bf6b-57fe1f2004c6 [idcv-ceph1][WARNIN] mount_activate: Failed to activate [idcv-ceph1][WARNIN] unmount: Unmounting &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.zUV3_1 [idcv-ceph1][WARNIN] command_check_call: Running command: &#x2F;bin&#x2F;umount – &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.zUV3_1 [idcv-ceph1][WARNIN] Traceback (most recent call last): [idcv-ceph1][WARNIN] File “&#x2F;usr&#x2F;sbin&#x2F;ceph-disk”, line 9, in [idcv-ceph1][WARNIN] load_entry_point(‘ceph-disk&#x3D;&#x3D;1.0.0’, ‘console_scripts’, ‘ceph-disk’)() [idcv-ceph1][WARNIN] File “&#x2F;usr&#x2F;lib&#x2F;python2.7&#x2F;site-packages&#x2F;ceph_disk&#x2F;main.py”, line 5371, in run [idcv-ceph1][WARNIN] main(sys.argv[1:]) [idcv-ceph1][WARNIN] File “&#x2F;usr&#x2F;lib&#x2F;python2.7&#x2F;site-packages&#x2F;ceph_disk&#x2F;main.py”, line 5322, in main [idcv-ceph1][WARNIN] args.func(args) [idcv-ceph1][WARNIN] File “&#x2F;usr&#x2F;lib&#x2F;python2.7&#x2F;site-packages&#x2F;ceph_disk&#x2F;main.py”, line 3445, in main_activate [idcv-ceph1][WARNIN] reactivate&#x3D;args.reactivate, [idcv-ceph1][WARNIN] File “&#x2F;usr&#x2F;lib&#x2F;python2.7&#x2F;site-packages&#x2F;ceph_disk&#x2F;main.py”, line 3202, in mount_activate [idcv-ceph1][WARNIN] (osd_id, cluster) &#x3D; activate(path, activate_key_template, init) [idcv-ceph1][WARNIN] File “&#x2F;usr&#x2F;lib&#x2F;python2.7&#x2F;site-packages&#x2F;ceph_disk&#x2F;main.py”, line 3365, in activate [idcv-ceph1][WARNIN] keyring&#x3D;keyring, [idcv-ceph1][WARNIN] File “&#x2F;usr&#x2F;lib&#x2F;python2.7&#x2F;site-packages&#x2F;ceph_disk&#x2F;main.py”, line 1013, in allocate_osd_id [idcv-ceph1][WARNIN] raise Error(‘ceph osd create failed’, e, e.output) [idcv-ceph1][WARNIN] ceph_disk.main.Error: Error: ceph osd create failed: Command ‘&#x2F;usr&#x2F;bin&#x2F;ceph’ returned non-zero exit status 1: 2018-07-03 11:47:35.463545 7f8310450700 0 librados: client.bootstrap-osd authentication error (1) Operation not permitted [idcv-ceph1][WARNIN] Error connecting to cluster: PermissionError [idcv-ceph1][WARNIN] [idcv-ceph1][ERROR ] RuntimeError: command returned non-zero exit status: 1 [ceph_deploy][ERROR ] RuntimeError: Failed to execute command: &#x2F;usr&#x2F;sbin&#x2F;ceph-disk -v activate –mark-init systemd –mount &#x2F;dev&#x2F;sdb1</p>
</blockquote>
<p>2、查看了下idcv-ceph1没有加上去</p>
<blockquote>
<p>[root@idcv-ceph0 cluster]# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT fd0 2:0 1 4K 0 disk sda 8:0 0 100G 0 disk ├─sda1 8:1 0 500M 0 part &#x2F;boot └─sda2 8:2 0 99.5G 0 part └─centos-root 253:0 0 99.5G 0 lvm &#x2F; sdb 8:16 0 100G 0 disk ├─sdb1 8:17 0 95G 0 part &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;osd&#x2F;ceph-0 └─sdb2 8:18 0 5G 0 part sr0 11:0 1 1024M 0 rom<br>[root@idcv-ceph0 cluster]# ceph -s cluster 812d3acb-eaa8-4355-9a74-64f2cd5209b3 health HEALTH_OK monmap e2: 3 mons at {idcv-ceph0&#x3D;172.20.1.138:6789&#x2F;0,idcv-ceph2&#x3D;172.20.1.140:6789&#x2F;0,idcv-ceph3&#x3D;172.20.1.141:6789&#x2F;0} election epoch 8, quorum 0,1,2 idcv-ceph0,idcv-ceph2,idcv-ceph3 osdmap e14: 3 osds: 3 up, 3 in flags sortbitwise,require_jewel_osds pgmap v27: 64 pgs, 1 pools, 0 bytes data, 0 objects 100 MB used, 284 GB &#x2F; 284 GB avail 64 active+clean [root@idcv-ceph0 cluster]#</p>
</blockquote>
<p>3、使用这个方法赋予角色OSD</p>
<blockquote>
<p>[root@idcv-ceph0 cluster]# ceph-deploy install –no-adjust-repos –osd idcv-ceph1 [ceph_deploy.conf][DEBUG ] found configuration file at: &#x2F;root&#x2F;.cephdeploy.conf [ceph_deploy.cli][INFO ] Invoked (1.5.39): &#x2F;usr&#x2F;bin&#x2F;ceph-deploy install –no-adjust-repos –osd idcv-ceph1 [ceph_deploy.cli][INFO ] ceph-deploy options: [ceph_deploy.cli][INFO ] verbose : False [ceph_deploy.cli][INFO ] testing : None [ceph_deploy.cli][INFO ] cd_conf : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x7f19c0ebd440&gt; [ceph_deploy.cli][INFO ] cluster : ceph [ceph_deploy.cli][INFO ] dev_commit : None [ceph_deploy.cli][INFO ] install_mds : False [ceph_deploy.cli][INFO ] stable : None [ceph_deploy.cli][INFO ] default_release : False [ceph_deploy.cli][INFO ] username : None [ceph_deploy.cli][INFO ] adjust_repos : False [ceph_deploy.cli][INFO ] func : [ceph_deploy.cli][INFO ] install_mgr : False [ceph_deploy.cli][INFO ] install_all : False [ceph_deploy.cli][INFO ] repo : False [ceph_deploy.cli][INFO ] host : [‘idcv-ceph1’] [ceph_deploy.cli][INFO ] install_rgw : False [ceph_deploy.cli][INFO ] install_tests : False [ceph_deploy.cli][INFO ] repo_url : None [ceph_deploy.cli][INFO ] ceph_conf : None [ceph_deploy.cli][INFO ] install_osd : True [ceph_deploy.cli][INFO ] version_kind : stable [ceph_deploy.cli][INFO ] install_common : False [ceph_deploy.cli][INFO ] overwrite_conf : False [ceph_deploy.cli][INFO ] quiet : False [ceph_deploy.cli][INFO ] dev : master [ceph_deploy.cli][INFO ] nogpgcheck : False [ceph_deploy.cli][INFO ] local_mirror : None [ceph_deploy.cli][INFO ] release : None [ceph_deploy.cli][INFO ] install_mon : False [ceph_deploy.cli][INFO ] gpg_url : None [ceph_deploy.install][DEBUG ] Installing stable version jewel on cluster ceph hosts idcv-ceph1 [ceph_deploy.install][DEBUG ] Detecting platform for host idcv-ceph1 … [idcv-ceph1][DEBUG ] connection detected need for sudo [idcv-ceph1][DEBUG ] connected to host: idcv-ceph1 [idcv-ceph1][DEBUG ] detect platform information from remote host [idcv-ceph1][DEBUG ] detect machine type [ceph_deploy.install][INFO ] Distro info: CentOS Linux 7.5.1804 Core [idcv-ceph1][INFO ] installing Ceph on idcv-ceph1 [idcv-ceph1][INFO ] Running command: sudo yum clean all [idcv-ceph1][DEBUG ] Loaded plugins: fastestmirror, priorities [idcv-ceph1][DEBUG ] Cleaning repos: Ceph Ceph-noarch base ceph-source epel extras updates [idcv-ceph1][DEBUG ] Cleaning up everything [idcv-ceph1][DEBUG ] Maybe you want: rm -rf &#x2F;var&#x2F;cache&#x2F;yum, to also free up space taken by orphaned data from disabled or removed repos [idcv-ceph1][DEBUG ] Cleaning up list of fastest mirrors [idcv-ceph1][INFO ] Running command: sudo yum -y install ceph [idcv-ceph1][DEBUG ] Loaded plugins: fastestmirror, priorities [idcv-ceph1][DEBUG ] Determining fastest mirrors [idcv-ceph1][DEBUG ] * base: mirrors.tuna.tsinghua.edu.cn [idcv-ceph1][DEBUG ] * epel: mirrors.huaweicloud.com [idcv-ceph1][DEBUG ] * extras: mirror.bit.edu.cn [idcv-ceph1][DEBUG ] * updates: mirrors.huaweicloud.com [idcv-ceph1][DEBUG ] 12 packages excluded due to repository priority protections [idcv-ceph1][DEBUG ] Package 1:ceph-10.2.10-0.el7.x86_64 already installed and latest version [idcv-ceph1][DEBUG ] Nothing to do [idcv-ceph1][INFO ] Running command: sudo ceph –version [idcv-ceph1][DEBUG ] ceph version 10.2.10 (5dc1e4c05cb68dbf62ae6fce3f0700e4654fdbbe)</p>
</blockquote>
<p>4、节点cpeh1 还是安装不上osd角色，这边准备初始化ceph1重新添加</p>
<blockquote>
<p>ceph-deploy purge 节点 ceph-deploy purgedata 节点 清楚安装包和残余数据 ceph-dpeloy install –no-adjust-repos –osd ceph1 直接装包 赋予OSD存储角色之后在添加OSD 具体步骤如下： ceph-deploy purge idcv-ceph1 ceph-deploy purgedata idcv-ceph1 ceph-deploy –overwrite-conf osd prepare idcv-ceph1:&#x2F;dev&#x2F;sdb<br>ceph-deploy –overwrite-conf osd activate idcv-ceph1:&#x2F;dev&#x2F;sdb1</p>
</blockquote>
<p>5、部署成功osd查看集群状态</p>
<blockquote>
<p>[root@idcv-ceph0 cluster]# ceph -s cluster 812d3acb-eaa8-4355-9a74-64f2cd5209b3 health HEALTH_OK monmap e2: 3 mons at {idcv-ceph0&#x3D;172.20.1.138:6789&#x2F;0,idcv-ceph2&#x3D;172.20.1.140:6789&#x2F;0,idcv-ceph3&#x3D;172.20.1.141:6789&#x2F;0} election epoch 8, quorum 0,1,2 idcv-ceph0,idcv-ceph2,idcv-ceph3 osdmap e27: 4 osds: 4 up, 4 in flags sortbitwise,require_jewel_osds pgmap v64: 104 pgs, 6 pools, 1588 bytes data, 171 objects 138 MB used, 379 GB &#x2F; 379 GB avail 104 active+clean</p>
</blockquote>
<h4 id="六、部署RGW服务"><a href="#六、部署RGW服务" class="headerlink" title="六、部署RGW服务"></a>六、部署RGW服务</h4><p>1、部署cdph1为对象网关</p>
<blockquote>
<p>[root@idcv-ceph0 cluster]# ceph-deploy install –no-adjust-repos –rgw idcv-ceph1 [ceph_deploy.conf][DEBUG ] found configuration file at: &#x2F;root&#x2F;.cephdeploy.conf [ceph_deploy.cli][INFO ] Invoked (1.5.39): &#x2F;usr&#x2F;bin&#x2F;ceph-deploy install –no-adjust-repos –rgw idcv-ceph1 [ceph_deploy.cli][INFO ] ceph-deploy options: [ceph_deploy.cli][INFO ] verbose : False [ceph_deploy.cli][INFO ] testing : None [ceph_deploy.cli][INFO ] cd_conf : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x7fba6af12440&gt; [ceph_deploy.cli][INFO ] cluster : ceph [ceph_deploy.cli][INFO ] dev_commit : None [ceph_deploy.cli][INFO ] install_mds : False [ceph_deploy.cli][INFO ] stable : None [ceph_deploy.cli][INFO ] default_release : False [ceph_deploy.cli][INFO ] username : None [ceph_deploy.cli][INFO ] adjust_repos : False [ceph_deploy.cli][INFO ] func : [ceph_deploy.cli][INFO ] install_mgr : False [ceph_deploy.cli][INFO ] install_all : False [ceph_deploy.cli][INFO ] repo : False [ceph_deploy.cli][INFO ] host : [‘idcv-ceph1’] [ceph_deploy.cli][INFO ] install_rgw : True [ceph_deploy.cli][INFO ] install_tests : False [ceph_deploy.cli][INFO ] repo_url : None [ceph_deploy.cli][INFO ] ceph_conf : None [ceph_deploy.cli][INFO ] install_osd : False [ceph_deploy.cli][INFO ] version_kind : stable [ceph_deploy.cli][INFO ] install_common : False [ceph_deploy.cli][INFO ] overwrite_conf : False [ceph_deploy.cli][INFO ] quiet : False [ceph_deploy.cli][INFO ] dev : master [ceph_deploy.cli][INFO ] nogpgcheck : False [ceph_deploy.cli][INFO ] local_mirror : None [ceph_deploy.cli][INFO ] release : None [ceph_deploy.cli][INFO ] install_mon : False [ceph_deploy.cli][INFO ] gpg_url : None [ceph_deploy.install][DEBUG ] Installing stable version jewel on cluster ceph hosts idcv-ceph1 [ceph_deploy.install][DEBUG ] Detecting platform for host idcv-ceph1 … [idcv-ceph1][DEBUG ] connection detected need for sudo [idcv-ceph1][DEBUG ] connected to host: idcv-ceph1 [idcv-ceph1][DEBUG ] detect platform information from remote host [idcv-ceph1][DEBUG ] detect machine type [ceph_deploy.install][INFO ] Distro info: CentOS Linux 7.5.1804 Core [idcv-ceph1][INFO ] installing Ceph on idcv-ceph1 [idcv-ceph1][INFO ] Running command: sudo yum clean all [idcv-ceph1][DEBUG ] Loaded plugins: fastestmirror, priorities [idcv-ceph1][DEBUG ] Cleaning repos: Ceph Ceph-noarch base ceph-source epel extras updates [idcv-ceph1][DEBUG ] Cleaning up everything [idcv-ceph1][DEBUG ] Maybe you want: rm -rf &#x2F;var&#x2F;cache&#x2F;yum, to also free up space taken by orphaned data from disabled or removed repos [idcv-ceph1][DEBUG ] Cleaning up list of fastest mirrors [idcv-ceph1][INFO ] Running command: sudo yum -y install ceph-radosgw [idcv-ceph1][DEBUG ] Loaded plugins: fastestmirror, priorities [idcv-ceph1][DEBUG ] Determining fastest mirrors [idcv-ceph1][DEBUG ] * base: mirrors.aliyun.com [idcv-ceph1][DEBUG ] * epel: mirrors.aliyun.com [idcv-ceph1][DEBUG ] * extras: mirrors.aliyun.com [idcv-ceph1][DEBUG ] * updates: mirror.bit.edu.cn [idcv-ceph1][DEBUG ] 12 packages excluded due to repository priority protections [idcv-ceph1][DEBUG ] Resolving Dependencies [idcv-ceph1][DEBUG ] –&gt; Running transaction check [idcv-ceph1][DEBUG ] —&gt; Package ceph-radosgw.x86_64 1:10.2.10-0.el7 will be installed [idcv-ceph1][DEBUG ] –&gt; Finished Dependency Resolution [idcv-ceph1][DEBUG ] [idcv-ceph1][DEBUG ] Dependencies Resolved [idcv-ceph1][DEBUG ] [idcv-ceph1][DEBUG ] &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; [idcv-ceph1][DEBUG ] Package Arch Version Repository Size [idcv-ceph1][DEBUG ] &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; [idcv-ceph1][DEBUG ] Installing: [idcv-ceph1][DEBUG ] ceph-radosgw x86_64 1:10.2.10-0.el7 Ceph 266 k [idcv-ceph1][DEBUG ] [idcv-ceph1][DEBUG ] Transaction Summary [idcv-ceph1][DEBUG ] &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; [idcv-ceph1][DEBUG ] Install 1 Package [idcv-ceph1][DEBUG ] [idcv-ceph1][DEBUG ] Total download size: 266 k [idcv-ceph1][DEBUG ] Installed size: 795 k [idcv-ceph1][DEBUG ] Downloading packages: [idcv-ceph1][DEBUG ] Running transaction check [idcv-ceph1][DEBUG ] Running transaction test [idcv-ceph1][DEBUG ] Transaction test succeeded [idcv-ceph1][DEBUG ] Running transaction [idcv-ceph1][DEBUG ] Installing : 1:ceph-radosgw-10.2.10-0.el7.x86_64 1&#x2F;1 [idcv-ceph1][DEBUG ] Verifying : 1:ceph-radosgw-10.2.10-0.el7.x86_64 1&#x2F;1 [idcv-ceph1][DEBUG ] [idcv-ceph1][DEBUG ] Installed: [idcv-ceph1][DEBUG ] ceph-radosgw.x86_64 1:10.2.10-0.el7<br>[idcv-ceph1][DEBUG ] [idcv-ceph1][DEBUG ] Complete! [idcv-ceph1][INFO ] Running command: sudo ceph –version [idcv-ceph1][DEBUG ] ceph version 10.2.10 (5dc1e4c05cb68dbf62ae6fce3f0700e4654fdbbe)</p>
</blockquote>
<p>2、设置idcv-ceph1为管理网关</p>
<blockquote>
<p>[root@idcv-ceph0 cluster]# ceph-deploy admin idcv-ceph1 [ceph_deploy.conf][DEBUG ] found configuration file at: &#x2F;root&#x2F;.cephdeploy.conf [ceph_deploy.cli][INFO ] Invoked (1.5.39): &#x2F;usr&#x2F;bin&#x2F;ceph-deploy admin idcv-ceph1 [ceph_deploy.cli][INFO ] ceph-deploy options: [ceph_deploy.cli][INFO ] username : None [ceph_deploy.cli][INFO ] verbose : False [ceph_deploy.cli][INFO ] overwrite_conf : False [ceph_deploy.cli][INFO ] quiet : False [ceph_deploy.cli][INFO ] cd_conf : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x7f5f91222fc8&gt; [ceph_deploy.cli][INFO ] cluster : ceph [ceph_deploy.cli][INFO ] client : [‘idcv-ceph1’] [ceph_deploy.cli][INFO ] func : [ceph_deploy.cli][INFO ] ceph_conf : None [ceph_deploy.cli][INFO ] default_release : False [ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to idcv-ceph1 [idcv-ceph1][DEBUG ] connection detected need for sudo [idcv-ceph1][DEBUG ] connected to host: idcv-ceph1 [idcv-ceph1][DEBUG ] detect platform information from remote host [idcv-ceph1][DEBUG ] detect machine type [idcv-ceph1][DEBUG ] write cluster configuration to &#x2F;etc&#x2F;ceph&#x2F;{cluster}.conf</p>
</blockquote>
<p>3、创建生成网关实例idcv-ceph1</p>
<blockquote>
<p>[root@idcv-ceph0 cluster]# ceph-deploy rgw create idcv-ceph1 [ceph_deploy.conf][DEBUG ] found configuration file at: &#x2F;root&#x2F;.cephdeploy.conf [ceph_deploy.cli][INFO ] Invoked (1.5.39): &#x2F;usr&#x2F;bin&#x2F;ceph-deploy rgw create idcv-ceph1 [ceph_deploy.cli][INFO ] ceph-deploy options: [ceph_deploy.cli][INFO ] username : None [ceph_deploy.cli][INFO ] verbose : False [ceph_deploy.cli][INFO ] rgw : [(‘idcv-ceph1’, ‘rgw.idcv-ceph1’)] [ceph_deploy.cli][INFO ] overwrite_conf : False [ceph_deploy.cli][INFO ] subcommand : create [ceph_deploy.cli][INFO ] quiet : False [ceph_deploy.cli][INFO ] cd_conf : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x7f6c86f85128&gt; [ceph_deploy.cli][INFO ] cluster : ceph [ceph_deploy.cli][INFO ] func : [ceph_deploy.cli][INFO ] ceph_conf : None [ceph_deploy.cli][INFO ] default_release : False [ceph_deploy.rgw][DEBUG ] Deploying rgw, cluster ceph hosts idcv-ceph1:rgw.idcv-ceph1 [idcv-ceph1][DEBUG ] connection detected need for sudo [idcv-ceph1][DEBUG ] connected to host: idcv-ceph1 [idcv-ceph1][DEBUG ] detect platform information from remote host [idcv-ceph1][DEBUG ] detect machine type [ceph_deploy.rgw][INFO ] Distro info: CentOS Linux 7.5.1804 Core [ceph_deploy.rgw][DEBUG ] remote host will use systemd [ceph_deploy.rgw][DEBUG ] deploying rgw bootstrap to idcv-ceph1 [idcv-ceph1][DEBUG ] write cluster configuration to &#x2F;etc&#x2F;ceph&#x2F;{cluster}.conf [idcv-ceph1][WARNIN] rgw keyring does not exist yet, creating one [idcv-ceph1][DEBUG ] create a keyring file [idcv-ceph1][DEBUG ] create path recursively if it doesn’t exist [idcv-ceph1][INFO ] Running command: sudo ceph –cluster ceph –name client.bootstrap-rgw –keyring &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;bootstrap-rgw&#x2F;ceph.keyring auth get-or-create client.rgw.idcv-ceph1 osd allow rwx mon allow rw -o &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;radosgw&#x2F;ceph-rgw.idcv-ceph1&#x2F;keyring [idcv-ceph1][INFO ] Running command: sudo systemctl enable <a href="mailto:&#x63;&#101;&#112;&#x68;&#x2d;&#114;&#97;&#x64;&#111;&#115;&#103;&#119;&#x40;&#114;&#x67;&#x77;&#46;&#x69;&#x64;&#x63;&#x76;&#x2d;&#99;&#101;&#x70;&#x68;&#x31;">&#x63;&#101;&#112;&#x68;&#x2d;&#114;&#97;&#x64;&#111;&#115;&#103;&#119;&#x40;&#114;&#x67;&#x77;&#46;&#x69;&#x64;&#x63;&#x76;&#x2d;&#99;&#101;&#x70;&#x68;&#x31;</a> [idcv-ceph1][WARNIN] Created symlink from &#x2F;etc&#x2F;systemd&#x2F;system&#x2F;ceph-radosgw.target.wants&#x2F;<a href="mailto:&#x63;&#101;&#x70;&#x68;&#45;&#x72;&#x61;&#100;&#111;&#115;&#103;&#x77;&#64;&#114;&#103;&#x77;&#x2e;&#105;&#x64;&#99;&#118;&#45;&#99;&#101;&#112;&#104;&#49;&#46;&#x73;&#x65;&#x72;&#x76;&#x69;&#x63;&#x65;">&#x63;&#101;&#x70;&#x68;&#45;&#x72;&#x61;&#100;&#111;&#115;&#103;&#x77;&#64;&#114;&#103;&#x77;&#x2e;&#105;&#x64;&#99;&#118;&#45;&#99;&#101;&#112;&#104;&#49;&#46;&#x73;&#x65;&#x72;&#x76;&#x69;&#x63;&#x65;</a> to &#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;ceph-radosgw@.service. [idcv-ceph1][INFO ] Running command: sudo systemctl start <a href="mailto:&#x63;&#x65;&#112;&#x68;&#45;&#114;&#97;&#100;&#x6f;&#x73;&#103;&#119;&#x40;&#x72;&#x67;&#119;&#46;&#x69;&#100;&#99;&#118;&#45;&#x63;&#x65;&#x70;&#104;&#49;">&#x63;&#x65;&#112;&#x68;&#45;&#114;&#97;&#100;&#x6f;&#x73;&#103;&#119;&#x40;&#x72;&#x67;&#119;&#46;&#x69;&#100;&#99;&#118;&#45;&#x63;&#x65;&#x70;&#104;&#49;</a> [idcv-ceph1][INFO ] Running command: sudo systemctl enable ceph.target [ceph_deploy.rgw][INFO ] The Ceph Object Gateway (RGW) is now running on host idcv-ceph1 and default port 7480</p>
</blockquote>
<p>4、测试网关服务</p>
<blockquote>
<p>[root@idcv-ceph0 cluster]# curl 172.20.1.139:7480 anonymous</p>
</blockquote>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a><strong>总结</strong></h3><p>到此所有需要相关服务已经部署完毕，如果对ceph.conf比较了解，设置正确参数，部署应该会比较顺利，下一篇将会测试osd块存储功能及rgw对象存储功能。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://minminmsn.github.io/2018/12/06/2018/12/2018-12-06-ceph%E9%9B%86%E7%BE%A4%E7%94%B1jewel%E7%89%88%E6%9C%AC%E5%8D%87%E7%BA%A7%E5%88%B0luminous%E7%89%88%E6%9C%AC/index/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jerry Min">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="MinMinMsn">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/12/06/2018/12/2018-12-06-ceph%E9%9B%86%E7%BE%A4%E7%94%B1jewel%E7%89%88%E6%9C%AC%E5%8D%87%E7%BA%A7%E5%88%B0luminous%E7%89%88%E6%9C%AC/index/" class="post-title-link" itemprop="url">Ceph集群由Jewel版本升级到Luminous版本</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-12-06 08:00:00" itemprop="dateCreated datePublished" datetime="2018-12-06T08:00:00+08:00">2018-12-06</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2023-05-26 15:06:30" itemprop="dateModified" datetime="2023-05-26T15:06:30+08:00">2023-05-26</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a><strong>参考文档</strong></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">https://www.virtualtothecore.com/en/upgrade-ceph-cluster-luminous/</span><br><span class="line">http://www.chinastor.com/distristor/11033L502017.html</span><br></pre></td></tr></table></figure>

<h3 id="缘起"><a href="#缘起" class="headerlink" title="缘起"></a><strong>缘起</strong></h3><blockquote>
<p>首先看之前安装版本链接及测试 <a target="_blank" rel="noopener" href="https://www.jianshu.com/p/379d313c2bf9">https://www.jianshu.com/p/379d313c2bf9</a> <a target="_blank" rel="noopener" href="https://www.jianshu.com/p/b11144ea407f">https://www.jianshu.com/p/b11144ea407f</a> mon ceph0、ceph2、cphe3 osd ceph0、ceph1、ceph2、ceph3 rgw ceph1 deploy ceph0 之前在Centos7.5上测试了Jewel版本的集群，随着对Ceph了解深入，计划线上运行比较新的LTS版本Ceph集群，最终选择了Luminous版本。 本来计划重新部署Luminous版本，看到这是测试环境数据丢失风险小就想尝试升级Jewel版本到Luminous版本，由于之前是Yum安装的根据之前经验原理是更新二进制文件，最后重启服务即可。看介绍文档升级步骤比较简单，但是测试中发现国内用户肯定会遇到一个坑，升级过程中会自动修改yum源到国外的站点，由于网络延迟大300s反应不及时会自动断开连接停止升级服务，故后续的操作我改成了国内源，并找出rpm包手工升级，由于本身有依赖关系故最终就收到yum install ceph ceph-radosgw即可全部升级所有的ceph包，最后重启相关服务即完成升级，最终数据没有丢失，各个功能也正常。</p>
</blockquote>
<h3 id="升级过程"><a href="#升级过程" class="headerlink" title="升级过程"></a><strong>升级过程</strong></h3><p>参照官方的升级指南一步一步的小心操作，要注意，升级时候要确保系统是健康运行的状态。 1、登录，确认sortbitwise是enabled状态：</p>
<blockquote>
<p>[root@idcv-ceph0 yum.repos.d]# ceph osd set sortbitwise set sortbitwise</p>
</blockquote>
<p>2、设置noout标志，告诉Ceph不要重新去做集群的负载均衡，虽然这是可选项，但是建议设置一下，避免每次停止节点的时候，Ceph就尝试通过复制数据到其他可用节点来重新平衡集群。</p>
<blockquote>
<p>[root@idcv-ceph0 yum.repos.d]# ceph osd set noout set noout</p>
</blockquote>
<p>3、升级时，可以选择手工升级每个节点，也可以使用使用Ceph-deploy实现自动升级。如果选择手动升级，在CentOS系统里，你需要先编辑Ceph yum repo获取新的Luminous版本来替换老版本Jewel，这就需要一个简单的文本替换操作：</p>
<blockquote>
<p>[root@idcv-ceph0 yum.repos.d]# sed -i ‘s&#x2F;jewel&#x2F;luminous&#x2F;‘ &#x2F;etc&#x2F;yum.repos.d&#x2F;ceph.repo</p>
</blockquote>
<p>4、使用Ceph-deploy可以实现一个命令完成集群的自动升级</p>
<blockquote>
<p>[root@idcv-ceph0 yum.repos.d]# yum install ceph-deploy python-pushy Running transaction Updating : ceph-deploy-2.0.0-0.noarch 1&#x2F;2 Cleanup : ceph-deploy-1.5.39-0.noarch 2&#x2F;2 Verifying : ceph-deploy-2.0.0-0.noarch 1&#x2F;2 Verifying : ceph-deploy-1.5.39-0.noarch 2&#x2F;2 Updated: ceph-deploy.noarch 0:2.0.0-0<br>Complete! [root@idcv-ceph0 yum.repos.d]# rpm -qa |grep ceph-deploy ceph-deploy-2.0.0-0.noarch</p>
</blockquote>
<p>5、一旦Ceph-deploy升级完成，首先要做的是在同一台机器上升级Ceph。 发现从这个时候开始按照官网的步骤在国内行不通了，主要原因是yum源改成了国外的网速达不到，后续就是手工升级结合官网步骤了。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br></pre></td><td class="code"><pre><span class="line">[root@idcv-ceph0 yum.repos.d]# ceph -s</span><br><span class="line">    cluster 812d3acb-eaa8-4355-9a74-64f2cd5209b3</span><br><span class="line">     health HEALTH_WARN</span><br><span class="line">            noout flag(s) set</span><br><span class="line">     monmap e2: 3 mons at &#123;idcv-ceph0=172.20.1.138:6789/0,idcv-ceph2=172.20.1.140:6789/0,idcv-ceph3=172.20.1.141:6789/0&#125;</span><br><span class="line">            election epoch 8, quorum 0,1,2 idcv-ceph0,idcv-ceph2,idcv-ceph3</span><br><span class="line">     osdmap e49: 4 osds: 4 up, 4 in</span><br><span class="line">            flags noout,sortbitwise,require_jewel_osds</span><br><span class="line">      pgmap v53288: 272 pgs, 12 pools, 97496 MB data, 1785 kobjects</span><br><span class="line">            296 GB used, 84824 MB / 379 GB avail</span><br><span class="line">                 272 active+clean</span><br><span class="line">[root@idcv-ceph0 yum.repos.d]# ceph -v</span><br><span class="line">ceph version 10.2.10 (5dc1e4c05cb68dbf62ae6fce3f0700e4654fdbbe)</span><br><span class="line">[root@idcv-ceph0 yum.repos.d]# cd /root/cluster/</span><br><span class="line">[root@idcv-ceph0 cluster]# ls</span><br><span class="line">ceph.bootstrap-mds.keyring  ceph.bootstrap-osd.keyring  ceph.client.admin.keyring  ceph-deploy-ceph.log</span><br><span class="line">ceph.bootstrap-mgr.keyring  ceph.bootstrap-rgw.keyring  ceph.conf               </span><br><span class="line">[root@idcv-ceph0 cluster]# ceph-deploy install --release luminous idcv-ceph0</span><br><span class="line">[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf</span><br><span class="line">[ceph_deploy.cli][INFO  ] Invoked (2.0.0): /usr/bin/ceph-deploy install --release luminous idcv-ceph0</span><br><span class="line">[ceph_deploy.cli][INFO  ] ceph-deploy options:</span><br><span class="line">[ceph_deploy.cli][INFO  ]  verbose                       : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  testing                       : None</span><br><span class="line">[ceph_deploy.cli][INFO  ]  cd_conf                       : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x7f38ae7a1d40&gt;</span><br><span class="line">[ceph_deploy.cli][INFO  ]  cluster                       : ceph</span><br><span class="line">[ceph_deploy.cli][INFO  ]  dev_commit                    : None</span><br><span class="line">[ceph_deploy.cli][INFO  ]  install_mds                   : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  stable                        : None</span><br><span class="line">[ceph_deploy.cli][INFO  ]  default_release               : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  username                      : None</span><br><span class="line">[ceph_deploy.cli][INFO  ]  adjust_repos                  : True</span><br><span class="line">[ceph_deploy.cli][INFO  ]  func                          : &lt;function install at 0x7f38ae9d8ed8&gt;</span><br><span class="line">[ceph_deploy.cli][INFO  ]  install_mgr                   : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  install_all                   : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  repo                          : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  host                          : [&#x27;idcv-ceph0&#x27;]</span><br><span class="line">[ceph_deploy.cli][INFO  ]  install_rgw                   : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  install_tests                 : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  repo_url                      : None</span><br><span class="line">[ceph_deploy.cli][INFO  ]  ceph_conf                     : None</span><br><span class="line">[ceph_deploy.cli][INFO  ]  install_osd                   : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  version_kind                  : stable</span><br><span class="line">[ceph_deploy.cli][INFO  ]  install_common                : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  overwrite_conf                : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  quiet                         : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  dev                           : master</span><br><span class="line">[ceph_deploy.cli][INFO  ]  nogpgcheck                    : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  local_mirror                  : None</span><br><span class="line">[ceph_deploy.cli][INFO  ]  release                       : luminous</span><br><span class="line">[ceph_deploy.cli][INFO  ]  install_mon                   : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  gpg_url                       : None</span><br><span class="line">[ceph_deploy.install][DEBUG ] Installing stable version luminous on cluster ceph hosts idcv-ceph0</span><br><span class="line">[ceph_deploy.install][DEBUG ] Detecting platform for host idcv-ceph0 ...</span><br><span class="line">[idcv-ceph0][DEBUG ] connected to host: idcv-ceph0 </span><br><span class="line">[idcv-ceph0][DEBUG ] detect platform information from remote host</span><br><span class="line">[idcv-ceph0][DEBUG ] detect machine type</span><br><span class="line">[ceph_deploy.install][INFO  ] Distro info: CentOS Linux 7.5.1804 Core</span><br><span class="line">[idcv-ceph0][INFO  ] installing Ceph on idcv-ceph0</span><br><span class="line">[idcv-ceph0][INFO  ] Running command: yum clean all</span><br><span class="line">[idcv-ceph0][DEBUG ] Loaded plugins: fastestmirror, priorities</span><br><span class="line">[idcv-ceph0][DEBUG ] Cleaning repos: Ceph Ceph-noarch base ceph-source epel extras updates</span><br><span class="line">[idcv-ceph0][DEBUG ] Cleaning up everything</span><br><span class="line">[idcv-ceph0][DEBUG ] Maybe you want: rm -rf /var/cache/yum, to also free up space taken by orphaned data from disabled or removed repos</span><br><span class="line">[idcv-ceph0][DEBUG ] Cleaning up list of fastest mirrors</span><br><span class="line">[idcv-ceph0][INFO  ] Running command: yum -y install epel-release</span><br><span class="line">[idcv-ceph0][DEBUG ] Loaded plugins: fastestmirror, priorities</span><br><span class="line">[idcv-ceph0][DEBUG ] Determining fastest mirrors</span><br><span class="line">[idcv-ceph0][DEBUG ]  * base: mirrors.huaweicloud.com</span><br><span class="line">[idcv-ceph0][DEBUG ]  * epel: mirrors.huaweicloud.com</span><br><span class="line">[idcv-ceph0][DEBUG ]  * extras: mirrors.neusoft.edu.cn</span><br><span class="line">[idcv-ceph0][DEBUG ]  * updates: mirrors.huaweicloud.com</span><br><span class="line">[idcv-ceph0][DEBUG ] 8 packages excluded due to repository priority protections</span><br><span class="line">[idcv-ceph0][DEBUG ] Package epel-release-7-11.noarch already installed and latest version</span><br><span class="line">[idcv-ceph0][DEBUG ] Nothing to do</span><br><span class="line">[idcv-ceph0][INFO  ] Running command: yum -y install yum-plugin-priorities</span><br><span class="line">[idcv-ceph0][DEBUG ] Loaded plugins: fastestmirror, priorities</span><br><span class="line">[idcv-ceph0][DEBUG ] Loading mirror speeds from cached hostfile</span><br><span class="line">[idcv-ceph0][DEBUG ]  * base: mirrors.huaweicloud.com</span><br><span class="line">[idcv-ceph0][DEBUG ]  * epel: mirrors.huaweicloud.com</span><br><span class="line">[idcv-ceph0][DEBUG ]  * extras: mirrors.neusoft.edu.cn</span><br><span class="line">[idcv-ceph0][DEBUG ]  * updates: mirrors.huaweicloud.com</span><br><span class="line">[idcv-ceph0][DEBUG ] 8 packages excluded due to repository priority protections</span><br><span class="line">[idcv-ceph0][DEBUG ] Package yum-plugin-priorities-1.1.31-45.el7.noarch already installed and latest version</span><br><span class="line">[idcv-ceph0][DEBUG ] Nothing to do</span><br><span class="line">[idcv-ceph0][DEBUG ] Configure Yum priorities to include obsoletes</span><br><span class="line">[idcv-ceph0][WARNIN] check_obsoletes has been enabled for Yum priorities plugin</span><br><span class="line">[idcv-ceph0][INFO  ] Running command: rpm --import https://download.ceph.com/keys/release.asc</span><br><span class="line">[idcv-ceph0][INFO  ] Running command: yum remove -y ceph-release</span><br><span class="line">[idcv-ceph0][DEBUG ] Loaded plugins: fastestmirror, priorities</span><br><span class="line">[idcv-ceph0][DEBUG ] Resolving Dependencies</span><br><span class="line">[idcv-ceph0][DEBUG ] --&gt; Running transaction check</span><br><span class="line">[idcv-ceph0][DEBUG ] ---&gt; Package ceph-release.noarch 0:1-1.el7 will be erased</span><br><span class="line">[idcv-ceph0][DEBUG ] --&gt; Finished Dependency Resolution</span><br><span class="line">[idcv-ceph0][DEBUG ] </span><br><span class="line">[idcv-ceph0][DEBUG ] Dependencies Resolved</span><br><span class="line">[idcv-ceph0][DEBUG ] </span><br><span class="line">[idcv-ceph0][DEBUG ] ================================================================================</span><br><span class="line">[idcv-ceph0][DEBUG ]  Package              Arch           Version            Repository         Size</span><br><span class="line">[idcv-ceph0][DEBUG ] ================================================================================</span><br><span class="line">[idcv-ceph0][DEBUG ] Removing:</span><br><span class="line">[idcv-ceph0][DEBUG ]  ceph-release         noarch         1-1.el7            installed         535  </span><br><span class="line">[idcv-ceph0][DEBUG ] </span><br><span class="line">[idcv-ceph0][DEBUG ] Transaction Summary</span><br><span class="line">[idcv-ceph0][DEBUG ] ================================================================================</span><br><span class="line">[idcv-ceph0][DEBUG ] Remove  1 Package</span><br><span class="line">[idcv-ceph0][DEBUG ] </span><br><span class="line">[idcv-ceph0][DEBUG ] Installed size: 535  </span><br><span class="line">[idcv-ceph0][DEBUG ] Downloading packages:</span><br><span class="line">[idcv-ceph0][DEBUG ] Running transaction check</span><br><span class="line">[idcv-ceph0][DEBUG ] Running transaction test</span><br><span class="line">[idcv-ceph0][DEBUG ] Transaction test succeeded</span><br><span class="line">[idcv-ceph0][DEBUG ] Running transaction</span><br><span class="line">[idcv-ceph0][DEBUG ]   Erasing    : ceph-release-1-1.el7.noarch                                  1/1 </span><br><span class="line">[idcv-ceph0][DEBUG ] warning: /etc/yum.repos.d/ceph.repo saved as /etc/yum.repos.d/ceph.repo.rpmsave</span><br><span class="line">[idcv-ceph0][DEBUG ]   Verifying  : ceph-release-1-1.el7.noarch                                  1/1 </span><br><span class="line">[idcv-ceph0][DEBUG ] </span><br><span class="line">[idcv-ceph0][DEBUG ] Removed:</span><br><span class="line">[idcv-ceph0][DEBUG ]   ceph-release.noarch 0:1-1.el7                                                 </span><br><span class="line">[idcv-ceph0][DEBUG ] </span><br><span class="line">[idcv-ceph0][DEBUG ] Complete!</span><br><span class="line">[idcv-ceph0][INFO  ] Running command: yum install -y https://download.ceph.com/rpm-luminous/el7/noarch/ceph-release-1-0.el7.noarch.rpm</span><br><span class="line">[idcv-ceph0][DEBUG ] Loaded plugins: fastestmirror, priorities</span><br><span class="line">[idcv-ceph0][DEBUG ] Examining /var/tmp/yum-root-dPpRu6/ceph-release-1-0.el7.noarch.rpm: ceph-release-1-1.el7.noarch</span><br><span class="line">[idcv-ceph0][DEBUG ] Marking /var/tmp/yum-root-dPpRu6/ceph-release-1-0.el7.noarch.rpm to be installed</span><br><span class="line">[idcv-ceph0][DEBUG ] Resolving Dependencies</span><br><span class="line">[idcv-ceph0][DEBUG ] --&gt; Running transaction check</span><br><span class="line">[idcv-ceph0][DEBUG ] ---&gt; Package ceph-release.noarch 0:1-1.el7 will be installed</span><br><span class="line">[idcv-ceph0][DEBUG ] --&gt; Finished Dependency Resolution</span><br><span class="line">[idcv-ceph0][DEBUG ] </span><br><span class="line">[idcv-ceph0][DEBUG ] Dependencies Resolved</span><br><span class="line">[idcv-ceph0][DEBUG ] </span><br><span class="line">[idcv-ceph0][DEBUG ] ================================================================================</span><br><span class="line">[idcv-ceph0][DEBUG ]  Package          Arch       Version     Repository                        Size</span><br><span class="line">[idcv-ceph0][DEBUG ] ================================================================================</span><br><span class="line">[idcv-ceph0][DEBUG ] Installing:</span><br><span class="line">[idcv-ceph0][DEBUG ]  ceph-release     noarch     1-1.el7     /ceph-release-1-0.el7.noarch     544  </span><br><span class="line">[idcv-ceph0][DEBUG ] </span><br><span class="line">[idcv-ceph0][DEBUG ] Transaction Summary</span><br><span class="line">[idcv-ceph0][DEBUG ] ================================================================================</span><br><span class="line">[idcv-ceph0][DEBUG ] Install  1 Package</span><br><span class="line">[idcv-ceph0][DEBUG ] </span><br><span class="line">[idcv-ceph0][DEBUG ] Total size: 544  </span><br><span class="line">[idcv-ceph0][DEBUG ] Installed size: 544  </span><br><span class="line">[idcv-ceph0][DEBUG ] Downloading packages:</span><br><span class="line">[idcv-ceph0][DEBUG ] Running transaction check</span><br><span class="line">[idcv-ceph0][DEBUG ] Running transaction test</span><br><span class="line">[idcv-ceph0][DEBUG ] Transaction test succeeded</span><br><span class="line">[idcv-ceph0][DEBUG ] Running transaction</span><br><span class="line">[idcv-ceph0][DEBUG ]   Installing : ceph-release-1-1.el7.noarch                                  1/1 </span><br><span class="line">[idcv-ceph0][DEBUG ]   Verifying  : ceph-release-1-1.el7.noarch                                  1/1 </span><br><span class="line">[idcv-ceph0][DEBUG ] </span><br><span class="line">[idcv-ceph0][DEBUG ] Installed:</span><br><span class="line">[idcv-ceph0][DEBUG ]   ceph-release.noarch 0:1-1.el7                                                 </span><br><span class="line">[idcv-ceph0][DEBUG ] </span><br><span class="line">[idcv-ceph0][DEBUG ] Complete!</span><br><span class="line">[idcv-ceph0][WARNIN] ensuring that /etc/yum.repos.d/ceph.repo contains a high priority</span><br><span class="line">[idcv-ceph0][WARNIN] altered ceph.repo priorities to contain: priority=1</span><br><span class="line">[idcv-ceph0][INFO  ] Running command: yum -y install ceph ceph-radosgw</span><br><span class="line">[idcv-ceph0][DEBUG ] Loaded plugins: fastestmirror, priorities</span><br><span class="line">[idcv-ceph0][DEBUG ] Loading mirror speeds from cached hostfile</span><br><span class="line">[idcv-ceph0][DEBUG ]  * base: mirrors.huaweicloud.com</span><br><span class="line">[idcv-ceph0][DEBUG ]  * epel: mirrors.huaweicloud.com</span><br><span class="line">[idcv-ceph0][DEBUG ]  * extras: mirrors.neusoft.edu.cn</span><br><span class="line">[idcv-ceph0][DEBUG ]  * updates: mirrors.huaweicloud.com</span><br><span class="line">[idcv-ceph0][DEBUG ] 8 packages excluded due to repository priority protections</span><br><span class="line">[idcv-ceph0][DEBUG ] Resolving Dependencies</span><br><span class="line">[idcv-ceph0][DEBUG ] --&gt; Running transaction check</span><br><span class="line">[idcv-ceph0][DEBUG ] ---&gt; Package ceph.x86_64 1:10.2.10-0.el7 will be updated</span><br><span class="line">[idcv-ceph0][DEBUG ] ---&gt; Package ceph.x86_64 2:12.2.5-0.el7 will be an update</span><br><span class="line">[idcv-ceph0][DEBUG ] --&gt; Processing Dependency: ceph-osd = 2:12.2.5-0.el7 for package: 2:ceph-12.2.5-0.el7.x86_64</span><br><span class="line">[idcv-ceph0][DEBUG ] --&gt; Processing Dependency: ceph-mon = 2:12.2.5-0.el7 for package: 2:ceph-12.2.5-0.el7.x86_64</span><br><span class="line">[idcv-ceph0][DEBUG ] --&gt; Processing Dependency: ceph-mgr = 2:12.2.5-0.el7 for package: 2:ceph-12.2.5-0.el7.x86_64</span><br><span class="line">[idcv-ceph0][DEBUG ] --&gt; Processing Dependency: ceph-mds = 2:12.2.5-0.el7 for package: 2:ceph-12.2.5-0.el7.x86_64</span><br><span class="line">[idcv-ceph0][DEBUG ] ---&gt; Package ceph-radosgw.x86_64 1:10.2.10-0.el7 will be updated</span><br><span class="line">[idcv-ceph0][DEBUG ] ---&gt; Package ceph-radosgw.x86_64 2:12.2.5-0.el7 will be an update</span><br><span class="line">[idcv-ceph0][DEBUG ] --&gt; Processing Dependency: ceph-selinux = 2:12.2.5-0.el7 for package: 2:ceph-radosgw-12.2.5-0.el7.x86_64</span><br><span class="line">[idcv-ceph0][DEBUG ] --&gt; Processing Dependency: librgw2 = 2:12.2.5-0.el7 for package: 2:ceph-radosgw-12.2.5-0.el7.x86_64</span><br><span class="line">[idcv-ceph0][DEBUG ] --&gt; Processing Dependency: librados2 = 2:12.2.5-0.el7 for package: 2:ceph-radosgw-12.2.5-0.el7.x86_64</span><br><span class="line">[idcv-ceph0][DEBUG ] --&gt; Processing Dependency: ceph-common = 2:12.2.5-0.el7 for package: 2:ceph-radosgw-12.2.5-0.el7.x86_64</span><br><span class="line">[idcv-ceph0][DEBUG ] --&gt; Processing Dependency: libibverbs.so.1()(64bit) for package: 2:ceph-radosgw-12.2.5-0.el7.x86_64</span><br><span class="line">[idcv-ceph0][DEBUG ] --&gt; Processing Dependency: libceph-common.so.0()(64bit) for package: 2:ceph-radosgw-12.2.5-0.el7.x86_64</span><br><span class="line">[idcv-ceph0][DEBUG ] --&gt; Running transaction check</span><br><span class="line">[idcv-ceph0][DEBUG ] ---&gt; Package ceph-common.x86_64 1:10.2.10-0.el7 will be updated</span><br><span class="line">[idcv-ceph0][DEBUG ] --&gt; Processing Dependency: ceph-common = 1:10.2.10-0.el7 for package: 1:ceph-base-10.2.10-0.el7.x86_64</span><br><span class="line">[idcv-ceph0][DEBUG ] ---&gt; Package ceph-common.x86_64 2:12.2.5-0.el7 will be an update</span><br><span class="line">[idcv-ceph0][DEBUG ] --&gt; Processing Dependency: python-rbd = 2:12.2.5-0.el7 for package: 2:ceph-common-12.2.5-0.el7.x86_64</span><br><span class="line">[idcv-ceph0][DEBUG ] --&gt; Processing Dependency: libcephfs2 = 2:12.2.5-0.el7 for package: 2:ceph-common-12.2.5-0.el7.x86_64</span><br><span class="line">[idcv-ceph0][DEBUG ] --&gt; Processing Dependency: python-rgw = 2:12.2.5-0.el7 for package: 2:ceph-common-12.2.5-0.el7.x86_64</span><br><span class="line">[idcv-ceph0][DEBUG ] --&gt; Processing Dependency: python-cephfs = 2:12.2.5-0.el7 for package: 2:ceph-common-12.2.5-0.el7.x86_64</span><br><span class="line">[idcv-ceph0][DEBUG ] --&gt; Processing Dependency: python-rados = 2:12.2.5-0.el7 for package: 2:ceph-common-12.2.5-0.el7.x86_64</span><br><span class="line">[idcv-ceph0][DEBUG ] --&gt; Processing Dependency: librbd1 = 2:12.2.5-0.el7 for package: 2:ceph-common-12.2.5-0.el7.x86_64</span><br><span class="line">[idcv-ceph0][DEBUG ] --&gt; Processing Dependency: python-prettytable for package: 2:ceph-common-12.2.5-0.el7.x86_64</span><br><span class="line">[idcv-ceph0][DEBUG ] --&gt; Processing Dependency: libcephfs.so.2()(64bit) for package: 2:ceph-common-12.2.5-0.el7.x86_64</span><br><span class="line">[idcv-ceph0][DEBUG ] ---&gt; Package ceph-mds.x86_64 1:10.2.10-0.el7 will be updated</span><br><span class="line">[idcv-ceph0][DEBUG ] ---&gt; Package ceph-mds.x86_64 2:12.2.5-0.el7 will be an update</span><br><span class="line">[idcv-ceph0][DEBUG ] ---&gt; Package ceph-mgr.x86_64 2:12.2.5-0.el7 will be installed</span><br><span class="line">[idcv-ceph0][DEBUG ] --&gt; Processing Dependency: python-cherrypy for package: 2:ceph-mgr-12.2.5-0.el7.x86_64</span><br><span class="line">[idcv-ceph0][DEBUG ] --&gt; Processing Dependency: pyOpenSSL for package: 2:ceph-mgr-12.2.5-0.el7.x86_64</span><br><span class="line">[idcv-ceph0][DEBUG ] --&gt; Processing Dependency: python-pecan for package: 2:ceph-mgr-12.2.5-0.el7.x86_64</span><br><span class="line">[idcv-ceph0][DEBUG ] ---&gt; Package ceph-mon.x86_64 1:10.2.10-0.el7 will be updated</span><br><span class="line">[idcv-ceph0][DEBUG ] ---&gt; Package ceph-mon.x86_64 2:12.2.5-0.el7 will be an update</span><br><span class="line">[idcv-ceph0][DEBUG ] ---&gt; Package ceph-osd.x86_64 1:10.2.10-0.el7 will be updated</span><br><span class="line">[idcv-ceph0][DEBUG ] ---&gt; Package ceph-osd.x86_64 2:12.2.5-0.el7 will be an update</span><br><span class="line">[idcv-ceph0][DEBUG ] ---&gt; Package ceph-selinux.x86_64 1:10.2.10-0.el7 will be updated</span><br><span class="line">[idcv-ceph0][DEBUG ] ---&gt; Package ceph-selinux.x86_64 2:12.2.5-0.el7 will be an update</span><br><span class="line">[idcv-ceph0][DEBUG ] ---&gt; Package libibverbs.x86_64 0:15-7.el7_5 will be installed</span><br><span class="line">[idcv-ceph0][DEBUG ] --&gt; Processing Dependency: rdma-core(x86-64) = 15-7.el7_5 for package: libibverbs-15-7.el7_5.x86_64</span><br><span class="line">[idcv-ceph0][DEBUG ] ---&gt; Package librados2.x86_64 1:10.2.10-0.el7 will be updated</span><br><span class="line">[idcv-ceph0][DEBUG ] --&gt; Processing Dependency: librados2 = 1:10.2.10-0.el7 for package: 1:rbd-nbd-10.2.10-0.el7.x86_64</span><br><span class="line">[idcv-ceph0][DEBUG ] --&gt; Processing Dependency: librados2 = 1:10.2.10-0.el7 for package: 1:libradosstriper1-10.2.10-0.el7.x86_64</span><br><span class="line">[idcv-ceph0][DEBUG ] ---&gt; Package librados2.x86_64 2:12.2.5-0.el7 will be an update</span><br><span class="line">[idcv-ceph0][DEBUG ] ---&gt; Package librgw2.x86_64 1:10.2.10-0.el7 will be updated</span><br><span class="line">[idcv-ceph0][DEBUG ] ---&gt; Package librgw2.x86_64 2:12.2.5-0.el7 will be an update</span><br><span class="line">[idcv-ceph0][DEBUG ] --&gt; Running transaction check</span><br><span class="line">[idcv-ceph0][DEBUG ] ---&gt; Package ceph-base.x86_64 1:10.2.10-0.el7 will be updated</span><br><span class="line">[idcv-ceph0][DEBUG ] ---&gt; Package ceph-base.x86_64 2:12.2.5-0.el7 will be an update</span><br><span class="line">[idcv-ceph0][DEBUG ] ---&gt; Package libcephfs1.x86_64 1:10.2.10-0.el7 will be obsoleted</span><br><span class="line">[idcv-ceph0][DEBUG ] ---&gt; Package libcephfs2.x86_64 2:12.2.5-0.el7 will be obsoleting</span><br><span class="line">[idcv-ceph0][DEBUG ] ---&gt; Package libradosstriper1.x86_64 1:10.2.10-0.el7 will be updated</span><br><span class="line">[idcv-ceph0][DEBUG ] ---&gt; Package libradosstriper1.x86_64 2:12.2.5-0.el7 will be an update</span><br><span class="line">[idcv-ceph0][DEBUG ] ---&gt; Package librbd1.x86_64 1:10.2.10-0.el7 will be updated</span><br><span class="line">[idcv-ceph0][DEBUG ] ---&gt; Package librbd1.x86_64 2:12.2.5-0.el7 will be an update</span><br><span class="line">[idcv-ceph0][DEBUG ] ---&gt; Package pyOpenSSL.x86_64 0:0.13.1-3.el7 will be installed</span><br><span class="line">[idcv-ceph0][DEBUG ] ---&gt; Package python-cephfs.x86_64 1:10.2.10-0.el7 will be updated</span><br><span class="line">[idcv-ceph0][DEBUG ] ---&gt; Package python-cephfs.x86_64 2:12.2.5-0.el7 will be an update</span><br><span class="line">[idcv-ceph0][DEBUG ] ---&gt; Package python-cherrypy.noarch 0:3.2.2-4.el7 will be installed</span><br><span class="line">[idcv-ceph0][DEBUG ] ---&gt; Package python-pecan.noarch 0:0.4.5-2.el7 will be installed</span><br><span class="line">[idcv-ceph0][DEBUG ] --&gt; Processing Dependency: python-webtest &gt;= 1.3.1 for package: python-pecan-0.4.5-2.el7.noarch</span><br><span class="line">[idcv-ceph0][DEBUG ] --&gt; Processing Dependency: python-webob &gt;= 1.2 for package: python-pecan-0.4.5-2.el7.noarch</span><br><span class="line">[idcv-ceph0][DEBUG ] --&gt; Processing Dependency: python-simplegeneric &gt;= 0.8 for package: python-pecan-0.4.5-2.el7.noarch</span><br><span class="line">[idcv-ceph0][DEBUG ] --&gt; Processing Dependency: python-mako &gt;= 0.4.0 for package: python-pecan-0.4.5-2.el7.noarch</span><br><span class="line">[idcv-ceph0][DEBUG ] --&gt; Processing Dependency: python-singledispatch for package: python-pecan-0.4.5-2.el7.noarch</span><br><span class="line">[idcv-ceph0][DEBUG ] ---&gt; Package python-prettytable.noarch 0:0.7.2-3.el7 will be installed</span><br><span class="line">[idcv-ceph0][DEBUG ] ---&gt; Package python-rados.x86_64 1:10.2.10-0.el7 will be updated</span><br><span class="line">[idcv-ceph0][DEBUG ] ---&gt; Package python-rados.x86_64 2:12.2.5-0.el7 will be an update</span><br><span class="line">[idcv-ceph0][DEBUG ] ---&gt; Package python-rbd.x86_64 1:10.2.10-0.el7 will be updated</span><br><span class="line">[idcv-ceph0][DEBUG ] ---&gt; Package python-rbd.x86_64 2:12.2.5-0.el7 will be an update</span><br><span class="line">[idcv-ceph0][DEBUG ] ---&gt; Package python-rgw.x86_64 2:12.2.5-0.el7 will be installed</span><br><span class="line">[idcv-ceph0][DEBUG ] ---&gt; Package rbd-nbd.x86_64 1:10.2.10-0.el7 will be updated</span><br><span class="line">[idcv-ceph0][DEBUG ] ---&gt; Package rbd-nbd.x86_64 2:12.2.5-0.el7 will be an update</span><br><span class="line">[idcv-ceph0][DEBUG ] ---&gt; Package rdma-core.x86_64 0:15-7.el7_5 will be installed</span><br><span class="line">[idcv-ceph0][DEBUG ] --&gt; Processing Dependency: pciutils for package: rdma-core-15-7.el7_5.x86_64</span><br><span class="line">[idcv-ceph0][DEBUG ] --&gt; Running transaction check</span><br><span class="line">[idcv-ceph0][DEBUG ] ---&gt; Package pciutils.x86_64 0:3.5.1-3.el7 will be installed</span><br><span class="line">[idcv-ceph0][DEBUG ] ---&gt; Package python-mako.noarch 0:0.8.1-2.el7 will be installed</span><br><span class="line">[idcv-ceph0][DEBUG ] --&gt; Processing Dependency: python-beaker for package: python-mako-0.8.1-2.el7.noarch</span><br><span class="line">[idcv-ceph0][DEBUG ] ---&gt; Package python-simplegeneric.noarch 0:0.8-7.el7 will be installed</span><br><span class="line">[idcv-ceph0][DEBUG ] ---&gt; Package python-singledispatch.noarch 0:3.4.0.2-2.el7 will be installed</span><br><span class="line">[idcv-ceph0][DEBUG ] ---&gt; Package python-webob.noarch 0:1.2.3-7.el7 will be installed</span><br><span class="line">[idcv-ceph0][DEBUG ] ---&gt; Package python-webtest.noarch 0:1.3.4-6.el7 will be installed</span><br><span class="line">[idcv-ceph0][DEBUG ] --&gt; Running transaction check</span><br><span class="line">[idcv-ceph0][DEBUG ] ---&gt; Package python-beaker.noarch 0:1.5.4-10.el7 will be installed</span><br><span class="line">[idcv-ceph0][DEBUG ] --&gt; Processing Dependency: python-paste for package: python-beaker-1.5.4-10.el7.noarch</span><br><span class="line">[idcv-ceph0][DEBUG ] --&gt; Running transaction check</span><br><span class="line">[idcv-ceph0][DEBUG ] ---&gt; Package python-paste.noarch 0:1.7.5.1-9.20111221hg1498.el7 will be installed</span><br><span class="line">[idcv-ceph0][DEBUG ] --&gt; Processing Dependency: python-tempita for package: python-paste-1.7.5.1-9.20111221hg1498.el7.noarch</span><br><span class="line">[idcv-ceph0][DEBUG ] --&gt; Running transaction check</span><br><span class="line">[idcv-ceph0][DEBUG ] ---&gt; Package python-tempita.noarch 0:0.5.1-6.el7 will be installed</span><br><span class="line">[idcv-ceph0][DEBUG ] --&gt; Finished Dependency Resolution</span><br><span class="line">[idcv-ceph0][DEBUG ] </span><br><span class="line">[idcv-ceph0][DEBUG ] Dependencies Resolved</span><br><span class="line">[idcv-ceph0][DEBUG ] </span><br><span class="line">[idcv-ceph0][DEBUG ] ================================================================================</span><br><span class="line">[idcv-ceph0][DEBUG ]  Package                Arch    Version                          Repository</span><br><span class="line">[idcv-ceph0][DEBUG ]                                                                            Size</span><br><span class="line">[idcv-ceph0][DEBUG ] ================================================================================</span><br><span class="line">[idcv-ceph0][DEBUG ] Installing:</span><br><span class="line">[idcv-ceph0][DEBUG ]  libcephfs2             x86_64  2:12.2.5-0.el7                   Ceph     432 k</span><br><span class="line">[idcv-ceph0][DEBUG ]      replacing  libcephfs1.x86_64 1:10.2.10-0.el7</span><br><span class="line">[idcv-ceph0][DEBUG ] Updating:</span><br><span class="line">[idcv-ceph0][DEBUG ]  ceph                   x86_64  2:12.2.5-0.el7                   Ceph     3.0 k</span><br><span class="line">[idcv-ceph0][DEBUG ]  ceph-radosgw           x86_64  2:12.2.5-0.el7                   Ceph     3.8 M</span><br><span class="line">[idcv-ceph0][DEBUG ] Installing for dependencies:</span><br><span class="line">[idcv-ceph0][DEBUG ]  ceph-mgr               x86_64  2:12.2.5-0.el7                   Ceph     3.6 M</span><br><span class="line">[idcv-ceph0][DEBUG ]  libibverbs             x86_64  15-7.el7_5                       updates  224 k</span><br><span class="line">[idcv-ceph0][DEBUG ]  pciutils               x86_64  3.5.1-3.el7                      base      93 k</span><br><span class="line">[idcv-ceph0][DEBUG ]  pyOpenSSL              x86_64  0.13.1-3.el7                     base     133 k</span><br><span class="line">[idcv-ceph0][DEBUG ]  python-beaker          noarch  1.5.4-10.el7                     base      80 k</span><br><span class="line">[idcv-ceph0][DEBUG ]  python-cherrypy        noarch  3.2.2-4.el7                      base     422 k</span><br><span class="line">[idcv-ceph0][DEBUG ]  python-mako            noarch  0.8.1-2.el7                      base     307 k</span><br><span class="line">[idcv-ceph0][DEBUG ]  python-paste           noarch  1.7.5.1-9.20111221hg1498.el7     base     866 k</span><br><span class="line">[idcv-ceph0][DEBUG ]  python-pecan           noarch  0.4.5-2.el7                      epel     255 k</span><br><span class="line">[idcv-ceph0][DEBUG ]  python-prettytable     noarch  0.7.2-3.el7                      base      37 k</span><br><span class="line">[idcv-ceph0][DEBUG ]  python-rgw             x86_64  2:12.2.5-0.el7                   Ceph      73 k</span><br><span class="line">[idcv-ceph0][DEBUG ]  python-simplegeneric   noarch  0.8-7.el7                        epel      12 k</span><br><span class="line">[idcv-ceph0][DEBUG ]  python-singledispatch  noarch  3.4.0.2-2.el7                    epel      18 k</span><br><span class="line">[idcv-ceph0][DEBUG ]  python-tempita         noarch  0.5.1-6.el7                      base      33 k</span><br><span class="line">[idcv-ceph0][DEBUG ]  python-webob           noarch  1.2.3-7.el7                      base     202 k</span><br><span class="line">[idcv-ceph0][DEBUG ]  python-webtest         noarch  1.3.4-6.el7                      base     102 k</span><br><span class="line">[idcv-ceph0][DEBUG ]  rdma-core              x86_64  15-7.el7_5                       updates   48 k</span><br><span class="line">[idcv-ceph0][DEBUG ] Updating for dependencies:</span><br><span class="line">[idcv-ceph0][DEBUG ]  ceph-base              x86_64  2:12.2.5-0.el7                   Ceph     3.9 M</span><br><span class="line">[idcv-ceph0][DEBUG ]  ceph-common            x86_64  2:12.2.5-0.el7                   Ceph      15 M</span><br><span class="line">[idcv-ceph0][DEBUG ]  ceph-mds               x86_64  2:12.2.5-0.el7                   Ceph     3.6 M</span><br><span class="line">[idcv-ceph0][DEBUG ]  ceph-mon               x86_64  2:12.2.5-0.el7                   Ceph     5.0 M</span><br><span class="line">[idcv-ceph0][DEBUG ]  ceph-osd               x86_64  2:12.2.5-0.el7                   Ceph      13 M</span><br><span class="line">[idcv-ceph0][DEBUG ]  ceph-selinux           x86_64  2:12.2.5-0.el7                   Ceph      20 k</span><br><span class="line">[idcv-ceph0][DEBUG ]  librados2              x86_64  2:12.2.5-0.el7                   Ceph     2.9 M</span><br><span class="line">[idcv-ceph0][DEBUG ]  libradosstriper1       x86_64  2:12.2.5-0.el7                   Ceph     330 k</span><br><span class="line">[idcv-ceph0][DEBUG ]  librbd1                x86_64  2:12.2.5-0.el7                   Ceph     1.1 M</span><br><span class="line">[idcv-ceph0][DEBUG ]  librgw2                x86_64  2:12.2.5-0.el7                   Ceph     1.7 M</span><br><span class="line">[idcv-ceph0][DEBUG ]  python-cephfs          x86_64  2:12.2.5-0.el7                   Ceph      82 k</span><br><span class="line">[idcv-ceph0][DEBUG ]  python-rados           x86_64  2:12.2.5-0.el7                   Ceph     172 k</span><br><span class="line">[idcv-ceph0][DEBUG ]  python-rbd             x86_64  2:12.2.5-0.el7                   Ceph     105 k</span><br><span class="line">[idcv-ceph0][DEBUG ]  rbd-nbd                x86_64  2:12.2.5-0.el7                   Ceph      81 k</span><br><span class="line">[idcv-ceph0][DEBUG ] </span><br><span class="line">[idcv-ceph0][DEBUG ] Transaction Summary</span><br><span class="line">[idcv-ceph0][DEBUG ] ================================================================================</span><br><span class="line">[idcv-ceph0][DEBUG ] Install  1 Package  (+17 Dependent packages)</span><br><span class="line">[idcv-ceph0][DEBUG ] Upgrade  2 Packages (+14 Dependent packages)</span><br><span class="line">[idcv-ceph0][DEBUG ] </span><br><span class="line">[idcv-ceph0][DEBUG ] Total download size: 57 M</span><br><span class="line">[idcv-ceph0][DEBUG ] Downloading packages:</span><br><span class="line">[idcv-ceph0][DEBUG ] Delta RPMs disabled because /usr/bin/applydeltarpm not installed.</span><br><span class="line">[idcv-ceph0][WARNIN] No data was received after 300 seconds, disconnecting...</span><br><span class="line">[idcv-ceph0][INFO  ] Running command: ceph --version</span><br><span class="line">[idcv-ceph0][DEBUG ] ceph version 10.2.10 (5dc1e4c05cb68dbf62ae6fce3f0700e4654fdbbe)</span><br></pre></td></tr></table></figure>

<p>报错一</p>
<blockquote>
<p>Delta RPMs disabled because &#x2F;usr&#x2F;bin&#x2F;applydeltarpm not installed.</p>
</blockquote>
<p>解决方案</p>
<blockquote>
<p>[root@idcv-ceph0 cluster]# yum install deltarpm -y Loaded plugins: fastestmirror, priorities Existing lock &#x2F;var&#x2F;run&#x2F;yum.pid: another copy is running as pid 90654. Another app is currently holding the yum lock; waiting for it to exit… The other application is: yum Memory : 132 M RSS (523 MB VSZ) Started: Tue Jul 10 16:15:59 2018 - 10:22 ago State : Sleeping, pid: 90654 Another app is currently holding the yum lock; waiting for it to exit… The other application is: yum Memory : 132 M RSS (523 MB VSZ) Started: Tue Jul 10 16:15:59 2018 - 10:24 ago State : Sleeping, pid: 90654 ^C Exiting on user cancel. [root@idcv-ceph0 cluster]# kill -9 90654 [root@idcv-ceph0 cluster]# yum install deltarpm -y</p>
</blockquote>
<p>报错二</p>
<blockquote>
<p>No data was received after 300 seconds, disconnecting…</p>
</blockquote>
<p>解决方案</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">另外需要修改ceph源为国内yum源，比如阿里yum,否则会报错No data was received after 300 seconds, disconnecting...</span><br><span class="line">[root@idcv-ceph0 cluster]# sed -i &#x27;s#download.ceph.com#mirrors.aliyun.com/ceph#g&#x27; /etc/yum.repos.d/ceph.repo</span><br><span class="line">但是命令会自动改成国外源，这里的解决方案是rpm -qa |grep cphe相关，修改成国内yum源后直接手动安装yum install或者使用ceph-deploy install服务</span><br><span class="line">[root@idcv-ceph0 ceph]# cat /etc/yum.repos.d/ceph.repo</span><br><span class="line">[Ceph]</span><br><span class="line">name=Ceph packages for $basearch</span><br><span class="line">baseurl=http://mirrors.aliyun.com/ceph/rpm-luminous/el7/$basearch</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">type=rpm-md</span><br><span class="line">gpgkey=https://mirrors.aliyun.com/ceph/keys/release.asc</span><br><span class="line">priority=1</span><br><span class="line"></span><br><span class="line">[Ceph-noarch]</span><br><span class="line">name=Ceph noarch packages</span><br><span class="line">baseurl=http://mirrors.aliyun.com/ceph/rpm-luminous/el7/noarch</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">type=rpm-md</span><br><span class="line">gpgkey=https://mirrors.aliyun.com/ceph/keys/release.asc</span><br><span class="line">priority=1</span><br><span class="line"></span><br><span class="line">[ceph-source]</span><br><span class="line">name=Ceph source packages</span><br><span class="line">baseurl=http://mirrors.aliyun.com/ceph/rpm-luminous/el7/SRPMS</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">type=rpm-md</span><br><span class="line">gpgkey=https://mirrors.aliyun.com/ceph/keys/release.asc</span><br><span class="line">priority=1</span><br><span class="line">[root@idcv-ceph0 yum.repos.d]#yum -y install ceph ceph-radosgw</span><br><span class="line">[root@idcv-ceph0 yum.repos.d]# rpm -qa |grep ceph</span><br><span class="line">ceph-deploy-2.0.0-0.noarch</span><br><span class="line">libcephfs2-12.2.5-0.el7.x86_64</span><br><span class="line">python-cephfs-12.2.5-0.el7.x86_64</span><br><span class="line">ceph-selinux-12.2.5-0.el7.x86_64</span><br><span class="line">ceph-radosgw-12.2.5-0.el7.x86_64</span><br><span class="line">ceph-release-1-1.el7.noarch</span><br><span class="line">ceph-base-12.2.5-0.el7.x86_64</span><br><span class="line">ceph-mon-12.2.5-0.el7.x86_64</span><br><span class="line">ceph-osd-12.2.5-0.el7.x86_64</span><br><span class="line">ceph-12.2.5-0.el7.x86_64</span><br><span class="line">ceph-common-12.2.5-0.el7.x86_64</span><br><span class="line">ceph-mds-12.2.5-0.el7.x86_64</span><br><span class="line">ceph-mgr-12.2.5-0.el7.x86_64</span><br><span class="line">[root@idcv-ceph0 yum.repos.d]# ceph -s</span><br><span class="line">    cluster 812d3acb-eaa8-4355-9a74-64f2cd5209b3</span><br><span class="line">     health HEALTH_WARN</span><br><span class="line">            noout flag(s) set</span><br><span class="line">     monmap e2: 3 mons at &#123;idcv-ceph0=172.20.1.138:6789/0,idcv-ceph2=172.20.1.140:6789/0,idcv-ceph3=172.20.1.141:6789/0&#125;</span><br><span class="line">            election epoch 8, quorum 0,1,2 idcv-ceph0,idcv-ceph2,idcv-ceph3</span><br><span class="line">     osdmap e49: 4 osds: 4 up, 4 in</span><br><span class="line">            flags noout,sortbitwise,require_jewel_osds</span><br><span class="line">      pgmap v53473: 272 pgs, 12 pools, 97496 MB data, 1785 kobjects</span><br><span class="line">            296 GB used, 84819 MB / 379 GB avail</span><br><span class="line">                 272 active+clean</span><br></pre></td></tr></table></figure>

<p>6、在每一个监控节点，需要重启mon服务，命令如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">[root@idcv-ceph0 cluster]# systemctl restart ceph-mon.target</span><br><span class="line">[root@idcv-ceph0 cluster]# systemctl status ceph-mon.target</span><br><span class="line">● ceph-mon.target - ceph target allowing to start/stop all ceph-mon@.service instances at once</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/ceph-mon.target; enabled; vendor preset: enabled)</span><br><span class="line">   Active: active since Tue 2018-07-10 17:27:39 CST; 11s agoJul 10 17:27:39 idcv-ceph0 systemd[1]: Reached target ceph target allowing to start/stop all ceph-mon@.service instances at once.</span><br><span class="line">Jul 10 17:27:39 idcv-ceph0 systemd[1]: Starting ceph target allowing to start/stop all ceph-mon@.service instances at once.</span><br><span class="line">[root@idcv-ceph0 cluster]# ceph -v</span><br><span class="line">ceph version 12.2.5 (cad919881333ac92274171586c827e01f554a70a) luminous (stable)</span><br><span class="line">[root@idcv-ceph0 cluster]# ceph -s</span><br><span class="line">  cluster:</span><br><span class="line">    id:     812d3acb-eaa8-4355-9a74-64f2cd5209b3</span><br><span class="line">    health: HEALTH_WARN</span><br><span class="line">            too many PGs per OSD (204 &gt; max 200)</span><br><span class="line">            noout flag(s) set</span><br><span class="line"></span><br><span class="line">  services:</span><br><span class="line">    mon: 3 daemons, quorum idcv-ceph0,idcv-ceph2,idcv-ceph3</span><br><span class="line">    mgr: no daemons active</span><br><span class="line">    osd: 4 osds: 4 up, 4 in</span><br><span class="line">         flags noout</span><br><span class="line"></span><br><span class="line">  data:</span><br><span class="line">    pools:   12 pools, 272 pgs</span><br><span class="line">    objects: 1785k objects, 97496 MB</span><br><span class="line">    usage:   296 GB used, 84817 MB / 379 GB avail</span><br><span class="line">    pgs:     272 active+clean</span><br></pre></td></tr></table></figure>


<p>7、在Kraken版本里，曾介绍过有一个Ceph管理器，在Luninous版本之后，这个ceph-mgr进程是日常操作必须的，而在Kraken版本时可选的。所以我的Jewel集群里没有这个管理区，在这里我们必须要安装它：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">\[root@idcv-ceph0 cluster\]# ceph-deploy mgr create idcv-ceph0 idcv-ceph1 idcv-ceph2 idcv-ceph3 \[ceph\_deploy.conf\]\[DEBUG \] found configuration file at: /root/.cephdeploy.conf \[ceph\_deploy.cli\]\[INFO \] Invoked (2.0.0): /usr/bin/ceph-deploy mgr create idcv-ceph0 idcv-ceph1 idcv-ceph2 idcv-ceph3 \[ceph\_deploy.cli\]\[INFO \] ceph-deploy options: \[ceph\_deploy.cli\]\[INFO \] username : None \[ceph\_deploy.cli\]\[INFO \] verbose : False \[ceph\_deploy.cli\]\[INFO \] mgr : \[(&#x27;idcv-ceph0&#x27;, &#x27;idcv-ceph0&#x27;), (&#x27;idcv-ceph1&#x27;, &#x27;idcv-ceph1&#x27;), (&#x27;idcv-ceph2&#x27;, &#x27;idcv-ceph2&#x27;), (&#x27;idcv-ceph3&#x27;, &#x27;idcv-ceph3&#x27;)\] \[ceph\_deploy.cli\]\[INFO \] overwrite\_conf : False \[ceph\_deploy.cli\]\[INFO \] subcommand : create \[ceph\_deploy.cli\]\[INFO \] quiet : False \[ceph\_deploy.cli\]\[INFO \] cd\_conf : &lt;ceph\_deploy.conf.cephdeploy.Conf instance at 0x7f229723e320&gt; \[ceph\_deploy.cli\]\[INFO \] cluster : ceph \[ceph\_deploy.cli\]\[INFO \] func : \[ceph\_deploy.cli\]\[INFO \] ceph\_conf : None \[ceph\_deploy.cli\]\[INFO \] default\_release : False \[ceph\_deploy.mgr\]\[DEBUG \] Deploying mgr, cluster ceph hosts idcv-ceph0:idcv-ceph0 idcv-ceph1:idcv-ceph1 idcv-ceph2:idcv-ceph2 idcv-ceph3:idcv-ceph3 \[idcv-ceph0\]\[DEBUG \] connected to host: idcv-ceph0 \[idcv-ceph0\]\[DEBUG \] detect platform information from remote host \[idcv-ceph0\]\[DEBUG \] detect machine type \[ceph\_deploy.mgr\]\[INFO \] Distro info: CentOS Linux 7.5.1804 Core \[ceph\_deploy.mgr\]\[DEBUG \] remote host will use systemd \[ceph\_deploy.mgr\]\[DEBUG \] deploying mgr bootstrap to idcv-ceph0 \[idcv-ceph0\]\[DEBUG \] write cluster configuration to /etc/ceph/&#123;cluster&#125;.conf \[idcv-ceph0\]\[WARNIN\] mgr keyring does not exist yet, creating one \[idcv-ceph0\]\[DEBUG \] create a keyring file \[idcv-ceph0\]\[DEBUG \] create path recursively if it doesn&#x27;t exist \[idcv-ceph0\]\[INFO \] Running command: ceph --cluster ceph --name client.bootstrap-mgr --keyring /var/lib/ceph/bootstrap-mgr/ceph.keyring auth get-or-create mgr.idcv-ceph0 mon allow profile mgr osd allow \* mds allow \* -o /var/lib/ceph/mgr/ceph-idcv-ceph0/keyring \[idcv-ceph0\]\[INFO \] Running command: systemctl enable ceph-mgr@idcv-ceph0 \[idcv-ceph0\]\[WARNIN\] Created symlink from /etc/systemd/system/ceph-mgr.target.wants/ceph-mgr@idcv-ceph0.service to /usr/lib/systemd/system/ceph-mgr@.service. \[idcv-ceph0\]\[INFO \] Running command: systemctl start ceph-mgr@idcv-ceph0 \[idcv-ceph0\]\[INFO \] Running command: systemctl enable ceph.target \[idcv-ceph1\]\[DEBUG \] connection detected need for sudo \[idcv-ceph1\]\[DEBUG \] connected to host: idcv-ceph1 \[idcv-ceph1\]\[DEBUG \] detect platform information from remote host \[idcv-ceph1\]\[DEBUG \] detect machine type \[ceph\_deploy.mgr\]\[INFO \] Distro info: CentOS Linux 7.5.1804 Core \[ceph\_deploy.mgr\]\[DEBUG \] remote host will use systemd \[ceph\_deploy.mgr\]\[DEBUG \] deploying mgr bootstrap to idcv-ceph1 \[idcv-ceph1\]\[DEBUG \] write cluster configuration to /etc/ceph/&#123;cluster&#125;.conf \[idcv-ceph1\]\[WARNIN\] mgr keyring does not exist yet, creating one \[idcv-ceph1\]\[DEBUG \] create a keyring file \[idcv-ceph1\]\[DEBUG \] create path recursively if it doesn&#x27;t exist \[idcv-ceph1\]\[INFO \] Running command: sudo ceph --cluster ceph --name client.bootstrap-mgr --keyring /var/lib/ceph/bootstrap-mgr/ceph.keyring auth get-or-create mgr.idcv-ceph1 mon allow profile mgr osd allow \* mds allow \* -o /var/lib/ceph/mgr/ceph-idcv-ceph1/keyring \[idcv-ceph1\]\[INFO \] Running command: sudo systemctl enable ceph-mgr@idcv-ceph1 \[idcv-ceph1\]\[WARNIN\] Created symlink from /etc/systemd/system/ceph-mgr.target.wants/ceph-mgr@idcv-ceph1.service to /usr/lib/systemd/system/ceph-mgr@.service. \[idcv-ceph1\]\[INFO \] Running command: sudo systemctl start ceph-mgr@idcv-ceph1 \[idcv-ceph1\]\[INFO \] Running command: sudo systemctl enable ceph.target \[idcv-ceph2\]\[DEBUG \] connection detected need for sudo \[idcv-ceph2\]\[DEBUG \] connected to host: idcv-ceph2 \[idcv-ceph2\]\[DEBUG \] detect platform information from remote host \[idcv-ceph2\]\[DEBUG \] detect machine type \[ceph\_deploy.mgr\]\[INFO \] Distro info: CentOS Linux 7.5.1804 Core \[ceph\_deploy.mgr\]\[DEBUG \] remote host will use systemd \[ceph\_deploy.mgr\]\[DEBUG \] deploying mgr bootstrap to idcv-ceph2 \[idcv-ceph2\]\[DEBUG \] write cluster configuration to /etc/ceph/&#123;cluster&#125;.conf \[idcv-ceph2\]\[WARNIN\] mgr keyring does not exist yet, creating one \[idcv-ceph2\]\[DEBUG \] create a keyring file \[idcv-ceph2\]\[DEBUG \] create path recursively if it doesn&#x27;t exist \[idcv-ceph2\]\[INFO \] Running command: sudo ceph --cluster ceph --name client.bootstrap-mgr --keyring /var/lib/ceph/bootstrap-mgr/ceph.keyring auth get-or-create mgr.idcv-ceph2 mon allow profile mgr osd allow \* mds allow \* -o /var/lib/ceph/mgr/ceph-idcv-ceph2/keyring \[idcv-ceph2\]\[INFO \] Running command: sudo systemctl enable ceph-mgr@idcv-ceph2 \[idcv-ceph2\]\[WARNIN\] Created symlink from /etc/systemd/system/ceph-mgr.target.wants/ceph-mgr@idcv-ceph2.service to /usr/lib/systemd/system/ceph-mgr@.service. \[idcv-ceph2\]\[INFO \] Running command: sudo systemctl start ceph-mgr@idcv-ceph2 \[idcv-ceph2\]\[INFO \] Running command: sudo systemctl enable ceph.target \[idcv-ceph3\]\[DEBUG \] connection detected need for sudo \[idcv-ceph3\]\[DEBUG \] connected to host: idcv-ceph3 \[idcv-ceph3\]\[DEBUG \] detect platform information from remote host \[idcv-ceph3\]\[DEBUG \] detect machine type \[ceph\_deploy.mgr\]\[INFO \] Distro info: CentOS Linux 7.5.1804 Core \[ceph\_deploy.mgr\]\[DEBUG \] remote host will use systemd \[ceph\_deploy.mgr\]\[DEBUG \] deploying mgr bootstrap to idcv-ceph3 \[idcv-ceph3\]\[DEBUG \] write cluster configuration to /etc/ceph/&#123;cluster&#125;.conf \[idcv-ceph3\]\[WARNIN\] mgr keyring does not exist yet, creating one \[idcv-ceph3\]\[DEBUG \] create a keyring file \[idcv-ceph3\]\[DEBUG \] create path recursively if it doesn&#x27;t exist \[idcv-ceph3\]\[INFO \] Running command: sudo ceph --cluster ceph --name client.bootstrap-mgr --keyring /var/lib/ceph/bootstrap-mgr/ceph.keyring auth get-or-create mgr.idcv-ceph3 mon allow profile mgr osd allow \* mds allow \* -o /var/lib/ceph/mgr/ceph-idcv-ceph3/keyring \[idcv-ceph3\]\[INFO \] Running command: sudo systemctl enable ceph-mgr@idcv-ceph3 \[idcv-ceph3\]\[WARNIN\] Created symlink from /etc/systemd/system/ceph-mgr.target.wants/ceph-mgr@idcv-ceph3.service to /usr/lib/systemd/system/ceph-mgr@.service. \[idcv-ceph3\]\[INFO \] Running command: sudo systemctl start ceph-mgr@idcv-ceph3 \[idcv-ceph3\]\[INFO \] Running command: sudo systemctl enable ceph.target \[root@idcv-ceph0 cluster\]# ceph -s cluster: id: 812d3acb-eaa8-4355-9a74-64f2cd5209b3 health: HEALTH\_WARN too many PGs per OSD (204 &gt; max 200) noout flag(s) set</span><br><span class="line"></span><br><span class="line">services: mon: 3 daemons, quorum idcv-ceph0,idcv-ceph2,idcv-ceph3 mgr: idcv-ceph0(active), standbys: idcv-ceph1, idcv-ceph2, idcv-ceph3 osd: 4 osds: 4 up, 4 in flags noout</span><br><span class="line"></span><br><span class="line">data: pools: 12 pools, 272 pgs objects: 1785k objects, 97496 MB usage: 296 GB used, 84816 MB / 379 GB avail pgs: 272 active+clean</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>8、重启osd<br>前题是所有节点都修改了国内L版本yum源第6步骤又介绍，执行了yum -y install ceph ceph-radosgw，他会升级二进制文件</p>
<blockquote>
<p>[root@idcv-ceph0 ceph]#systemctl restart ceph-osd.target<br>[root@idcv-ceph0 ceph]# ceph versions<br>{<br>    “mon”: {<br>        “ceph version 12.2.5 (cad919881333ac92274171586c827e01f554a70a) luminous (stable)”: 3<br>    },<br>    “mgr”: {<br>        “ceph version 12.2.5 (cad919881333ac92274171586c827e01f554a70a) luminous (stable)”: 4<br>    },<br>    “osd”: {<br>        “ceph version 12.2.5 (cad919881333ac92274171586c827e01f554a70a) luminous (stable)”: 4<br>    },<br>    “mds”: {},<br>    “overall”: {<br>        “ceph version 12.2.5 (cad919881333ac92274171586c827e01f554a70a) luminous (stable)”: 11<br>    }<br>}</p>
</blockquote>
<p>9、现在所有的组件都是最新的12.5版本了，我们可以禁止Luminous版本之前的OSD，运行Luminous的独有功能：</p>
<blockquote>
<p>[root@idcv-ceph0 ceph]# ceph osd require-osd-release luminous<br>recovery_deletes is set</p>
</blockquote>
<p>这也意味着现在只有Luminous节点才能加入这个集群了。</p>
<p>10、rgw服务也需要重启在ceph1上</p>
<blockquote>
<p>[root@idcv-ceph1 system]# systemctl restart ceph-radosgw.target<br>[root@idcv-ceph1 system]# systemctl status ceph-radosgw.target<br>● ceph-radosgw.target - ceph target allowing to start&#x2F;stop all ceph-radosgw@.service instances at once<br>   Loaded: loaded (&#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;ceph-radosgw.target; enabled; vendor preset: enabled)<br>   Active: active since Tue 2018-07-10 18:02:25 CST; 6s ago<br>Jul 10 18:02:25 idcv-ceph1 systemd[1]: Reached target ceph target allowing to start&#x2F;stop all ceph-radosgw@.service instances at once.<br>Jul 10 18:02:25 idcv-ceph1 systemd[1]: Starting ceph target allowing to start&#x2F;stop all ceph-radosgw@.service instances at once.</p>
</blockquote>
<p>11、启动dashboard</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">\[root@idcv-ceph0 ceph\]# rpm -qa |grep mgr ceph-mgr-12.2.5-0.el7.x86\_64 \[root@idcv-ceph0 ceph\]# ceph mgr module enable dashboard  </span><br><span class="line">\[root@idcv-ceph0 ceph\]# ceph mgr dump &#123; &quot;epoch&quot;: 53, &quot;active\_gid&quot;: 34146, &quot;active\_name&quot;: &quot;idcv-ceph0&quot;, &quot;active\_addr&quot;: &quot;172.20.1.138:6804/95951&quot;, &quot;available&quot;: true, &quot;standbys&quot;: \[ &#123; &quot;gid&quot;: 44129, &quot;name&quot;: &quot;idcv-ceph2&quot;, &quot;available\_modules&quot;: \[ &quot;balancer&quot;, &quot;dashboard&quot;, &quot;influx&quot;, &quot;localpool&quot;, &quot;prometheus&quot;, &quot;restful&quot;, &quot;selftest&quot;, &quot;status&quot;, &quot;zabbix&quot; \] &#125;, &#123; &quot;gid&quot;: 44134, &quot;name&quot;: &quot;idcv-ceph1&quot;, &quot;available\_modules&quot;: \[ &quot;balancer&quot;, &quot;dashboard&quot;, &quot;influx&quot;, &quot;localpool&quot;, &quot;prometheus&quot;, &quot;restful&quot;, &quot;selftest&quot;, &quot;status&quot;, &quot;zabbix&quot; \] &#125;, &#123; &quot;gid&quot;: 44135, &quot;name&quot;: &quot;idcv-ceph3&quot;, &quot;available\_modules&quot;: \[ &quot;balancer&quot;, &quot;dashboard&quot;, &quot;influx&quot;, &quot;localpool&quot;, &quot;prometheus&quot;, &quot;restful&quot;, &quot;selftest&quot;, &quot;status&quot;, &quot;zabbix&quot; \] &#125; \], &quot;modules&quot;: \[ &quot;balancer&quot;, &quot;dashboard&quot;, &quot;restful&quot;, &quot;status&quot; \], &quot;available\_modules&quot;: \[ &quot;balancer&quot;, &quot;dashboard&quot;, &quot;influx&quot;, &quot;localpool&quot;, &quot;prometheus&quot;, &quot;restful&quot;, &quot;selftest&quot;, &quot;status&quot;, &quot;zabbix&quot; \], &quot;services&quot;: &#123; &quot;dashboard&quot;: &quot;http://idcv-ceph0:7000/&quot; &#125; &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>浏览器访问<a target="_blank" rel="noopener" href="http://172.20.1.138:7000/">http://172.20.1.138:7000</a></p>
<blockquote>
<p><img src="https://upload-images.jianshu.io/upload_images/7535971-b0a2d62a6fa79789.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></p>
</blockquote>
<p>12、最后一步，禁止noot，以后集群就可以在需要的时候自己做负载均衡了：</p>
<pre><code>
\[root@idcv-ceph0 ceph\]# ceph osd unset noout noout is unset \[root@idcv-ceph0 ceph\]# ceph -s cluster: id: 812d3acb-eaa8-4355-9a74-64f2cd5209b3 health: HEALTH\_WARN application not enabled on 1 pool(s)

services: mon: 3 daemons, quorum idcv-ceph0,idcv-ceph2,idcv-ceph3 mgr: idcv-ceph0(active), standbys: idcv-ceph2, idcv-ceph1, idcv-ceph3 osd: 4 osds: 4 up, 4 in rgw: 1 daemon active

data: pools: 12 pools, 272 pgs objects: 1785k objects, 97496 MB usage: 296 GB used, 84830 MB / 379 GB avail pgs: 272 active+clean

io: client: 0 B/s rd, 0 op/s rd, 0 op/s wr

报错 health: HEALTH\_WARN application not enabled on 1 pool(s)

解决方案 \[root@idcv-ceph0 ceph\]# ceph health detail HEALTH\_WARN application not enabled on 1 pool(s) POOL\_APP\_NOT\_ENABLED application not enabled on 1 pool(s) application not enabled on pool &#39;test\_pool&#39; use &#39;ceph osd pool application enable &#39;, where is &#39;cephfs&#39;, &#39;rbd&#39;, &#39;rgw&#39;, or freeform for custom applications. \[root@idcv-ceph0 ceph\]# ceph osd pool application enable test\_pool Invalid command: missing required parameter app(&lt;string(goodchars \[A-Za-z0-9-\_.\])&gt;) osd pool application enable &#123;--yes-i-really-mean-it&#125; : enable use of an application \[cephfs,rbd,rgw\] on pool Error EINVAL: invalid command \[root@idcv-ceph0 ceph\]# ceph osd pool application enable test\_pool rbd enabled application &#39;rbd&#39; on pool &#39;test\_pool&#39; \[root@idcv-ceph0 ceph\]# ceph -s cluster: id: 812d3acb-eaa8-4355-9a74-64f2cd5209b3 health: HEALTH\_OK

services: mon: 3 daemons, quorum idcv-ceph0,idcv-ceph2,idcv-ceph3 mgr: idcv-ceph0(active), standbys: idcv-ceph2, idcv-ceph1, idcv-ceph3 osd: 4 osds: 4 up, 4 in rgw: 1 daemon active

data: pools: 12 pools, 272 pgs objects: 1785k objects, 97496 MB usage: 296 GB used, 84829 MB / 379 GB avail pgs: 272 active+clean

io: client: 0 B/s rd, 0 op/s rd, 0 op/s wr \`\`\`

### **总结**

整个升级过程大概1个多小时，主要是结合国内情况否则升级还是比较简单的，另外L版本新增了mgr和dashboard功能，升级完成后测试了对象存储功能和块存储功能都正常。
</code></pre>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://minminmsn.github.io/2018/12/06/2018/12/2018-12-06-jewel%E7%89%88%E6%9C%ACceph%E9%9B%86%E7%BE%A4%E5%8A%9F%E8%83%BD%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/index/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jerry Min">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="MinMinMsn">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/12/06/2018/12/2018-12-06-jewel%E7%89%88%E6%9C%ACceph%E9%9B%86%E7%BE%A4%E5%8A%9F%E8%83%BD%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/index/" class="post-title-link" itemprop="url">Jewel版本Ceph集群功能性能测试</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-12-06 08:00:00" itemprop="dateCreated datePublished" datetime="2018-12-06T08:00:00+08:00">2018-12-06</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2023-05-26 15:06:30" itemprop="dateModified" datetime="2023-05-26T15:06:30+08:00">2023-05-26</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a><strong>参考文档</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">http://docs.ceph.com/docs/master/start/quick-start-preflight/<span class="comment">#rhel-centos</span></span><br><span class="line">https://www.linuxidc.com/Linux/2017-09/146760.htm</span><br><span class="line">http://s3browser.com</span><br><span class="line">http://docs.ceph.org.cn/man/8/rbd/</span><br><span class="line">https://github.com/s3fs-fuse/s3fs-fuse</span><br><span class="line">https://blog.csdn.net/miaodichiyou/article/details/76050361</span><br><span class="line">http://elf8848.iteye.com/blog/2089055</span><br></pre></td></tr></table></figure>

<h3 id="测试目标"><a href="#测试目标" class="headerlink" title="测试目标"></a><strong>测试目标</strong></h3><p>使用rbd映射挂载块存储并测试性能 使用rbd-nbd映射挂载条带块存储并测试性能 使用s3brower测试对象存储读写 使用s3fs挂载挂载对象存储 使用对象存储写使用块存储读</p>
<h3 id="一，使用rbd映射挂载块存储并测试性能"><a href="#一，使用rbd映射挂载块存储并测试性能" class="headerlink" title="一，使用rbd映射挂载块存储并测试性能"></a><strong>一，使用rbd映射挂载块存储并测试性能</strong></h3><p>1、创建image</p>
<blockquote>
<p>[root@idcv-ceph0 cluster]# ceph osd pool create test_pool 100 pool ‘test_pool’ created [root@idcv-ceph0 cluster]# rados lspools rbd .rgw.root default.rgw.control default.rgw.data.root default.rgw.gc default.rgw.log default.rgw.users.uid default.rgw.users.keys default.rgw.buckets.index default.rgw.buckets.data test_pool [root@idcv-ceph0 cluster]# rbd list [root@idcv-ceph0 cluster]# rbd create test_pool&#x2F;testimage1 –size 40960 [root@idcv-ceph0 cluster]# rbd create test_pool&#x2F;testimage2 –size 40960 [root@idcv-ceph0 cluster]# rbd create test_pool&#x2F;testimage3 –size 40960 [root@idcv-ceph0 cluster]# rbd create test_pool&#x2F;testimage4 –size 40960 [root@idcv-ceph0 cluster]# rbd list [root@idcv-ceph0 cluster]# rbd list test_pool testimage1 testimage2 testimage3 testimage4</p>
</blockquote>
<p>2、映射image</p>
<blockquote>
<p>[root@idcv-ceph0 cluster]# rbd map test_pool&#x2F;testimage1 rbd: sysfs write failed RBD image feature set mismatch. You can disable features unsupported by the kernel with “rbd feature disable”. In some cases useful info is found in syslog - try “dmesg | tail” or so. rbd: map failed: (6) No such device or address [root@idcv-ceph0 cluster]# dmesg |tail [113320.926463] rbd: loaded (major 252) [113320.931044] libceph: mon2 172.20.1.141:6789 session established [113320.931364] libceph: client4193 fsid 812d3acb-eaa8-4355-9a74-64f2cd5209b3 [113320.936922] rbd: image testimage1: image uses unsupported features: 0x38 [113339.870548] libceph: mon1 172.20.1.140:6789 session established [113339.870906] libceph: client4168 fsid 812d3acb-eaa8-4355-9a74-64f2cd5209b3 [113339.877109] rbd: image testimage1: image uses unsupported features: 0x38 [113381.405453] libceph: mon2 172.20.1.141:6789 session established [113381.405784] libceph: client4202 fsid 812d3acb-eaa8-4355-9a74-64f2cd5209b3 [113381.411625] rbd: image testimage1: image uses unsupported features: 0x38</p>
</blockquote>
<p>报错处理方法：disable新特性</p>
<blockquote>
<p>[root@idcv-ceph0 cluster]# rbd info test_pool&#x2F;testimage1 rbd image ‘testimage1’: size 40960 MB in 10240 objects order 22 (4096 kB objects) block_name_prefix: rbd_data.10802ae8944a format: 2 features: layering, exclusive-lock, object-map, fast-diff, deep-flatten flags: [root@idcv-ceph0 cluster]# rbd feature disable test_pool&#x2F;testimage1 rbd: at least one feature name must be specified [root@idcv-ceph0 cluster]# rbd feature disable test_pool&#x2F;testimage1 fast-diff [root@idcv-ceph0 cluster]# rbd feature disable test_pool&#x2F;testimage1 object-map [root@idcv-ceph0 cluster]# rbd feature disable test_pool&#x2F;testimage1 exclusive-lock [root@idcv-ceph0 cluster]# rbd feature disable test_pool&#x2F;testimage1 deep-flatten [root@idcv-ceph0 cluster]# rbd info test_pool&#x2F;testimage1 rbd image ‘testimage1’: size 40960 MB in 10240 objects order 22 (4096 kB objects) block_name_prefix: rbd_data.10802ae8944a format: 2 features: layering flags: [root@idcv-ceph0 cluster]# rbd map test_pool&#x2F;testimage1 &#x2F;dev&#x2F;rbd0</p>
</blockquote>
<p>同理操作testimage2\3\4，最终如下</p>
<blockquote>
<p>[root@idcv-ceph0 cluster]# rbd showmapped id pool image snap device<br>0 test_pool testimage1 - &#x2F;dev&#x2F;rbd0 1 test_pool testimage2 - &#x2F;dev&#x2F;rbd1 2 test_pool testimage3 - &#x2F;dev&#x2F;rbd2 3 test_pool testimage4 - &#x2F;dev&#x2F;rbd3</p>
</blockquote>
<p>备注收缩image大小</p>
<blockquote>
<p>[root@idcv-ceph0 ceph-disk0]# rbd resize -p test_pool –image testimage1 -s 10240 –allow-shrink Resizing image: 100% complete…done. [root@idcv-ceph0 ceph-disk0]# rbd resize -p test_pool –image testimage2 -s 10240 –allow-shrink Resizing image: 100% complete…done. [root@idcv-ceph0 ceph-disk0]# rbd resize -p test_pool –image testimage3 -s 10240 –allow-shrink Resizing image: 100% complete…done. [root@idcv-ceph0 ceph-disk0]# rbd resize -p test_pool –image testimage4 -s 10240 –allow-shrink Resizing image: 100% complete…done.</p>
</blockquote>
<p>3、格式化挂载</p>
<blockquote>
<p>[root@idcv-ceph0 ceph-disk0]# mkfs.xfs &#x2F;dev&#x2F;rbd0 [root@idcv-ceph0 ceph-disk0]# mount &#x2F;mnt&#x2F;ceph-disk0 &#x2F;dev&#x2F;rbd0</p>
</blockquote>
<p>4、DD测试</p>
<blockquote>
<p>[root@idcv-ceph0 ceph-disk0]# dd if&#x3D;&#x2F;dev&#x2F;zero of&#x3D;&#x2F;mnt&#x2F;ceph-disk0&#x2F;file0 count&#x3D;1000 bs&#x3D;4M conv&#x3D;fsync 1000+0 records in 1000+0 records out 4194304000 bytes (4.2 GB) copied, 39.1407 s, 107 MB&#x2F;s</p>
</blockquote>
<h3 id="二、使用rbd-nbd映射挂载条带块存储并测试性能"><a href="#二、使用rbd-nbd映射挂载条带块存储并测试性能" class="headerlink" title="二、使用rbd-nbd映射挂载条带块存储并测试性能"></a><strong>二、使用rbd-nbd映射挂载条带块存储并测试性能</strong></h3><p>1、创建image 根据官网文档条带化测试需要带参数–stripe-unit及–stripe-count 计划测试object-size为4M、4K且count为1时，object-szie为32M且count为8、16时块存储性能</p>
<blockquote>
<p>[root@idcv-ceph0 ceph-disk0]# rbd create test_pool&#x2F;testimage5 –size 10240 –stripe-unit 2097152 –stripe-count 16 [root@idcv-ceph0 ceph-disk0]# rbd info test_pool&#x2F;testimage5 rbd image ‘testimage5’: size 10240 MB in 2560 objects order 22 (4096 kB objects) block_name_prefix: rbd_data.10c52ae8944a format: 2 features: layering, striping, exclusive-lock, object-map, fast-diff, deep-flatten flags: stripe unit: 2048 kB stripe count: 16 [root@idcv-ceph0 ceph-disk0]# rbd create test_pool&#x2F;testimage6 –size 10240 –stripe-unit 4096 –stripe-count 4 [root@idcv-ceph0 ceph-disk0]# rbd info test_pool&#x2F;testimage6 rbd image ‘testimage6’: size 10240 MB in 2560 objects order 22 (4096 kB objects) block_name_prefix: rbd_data.10c82ae8944a format: 2 features: layering, striping, exclusive-lock, object-map, fast-diff, deep-flatten flags: stripe unit: 4096 bytes stripe count: 4 [root@idcv-ceph0 ceph-disk0]# rbd create test_pool&#x2F;testimage7 –size 10240 –object-size 32M –stripe-unit 4194304 –stripe-count 4 [root@idcv-ceph0 ceph-disk0]# rbd info test_pool&#x2F;testimage7 rbd image ‘testimage7’: size 10240 MB in 320 objects order 25 (32768 kB objects) block_name_prefix: rbd_data.107e238e1f29 format: 2 features: layering, striping, exclusive-lock, object-map, fast-diff, deep-flatten flags: stripe unit: 4096 kB stripe count: 4 [root@idcv-ceph0 ceph-disk0]# rbd create test_pool&#x2F;testimage8 –size 10240 –object-size 32M –stripe-unit 2097152 –stripe-count 16 [root@idcv-ceph0 ceph-disk0]# rbd info test_pool&#x2F;testimage8 rbd image ‘testimage8’: size 10240 MB in 320 objects order 25 (32768 kB objects) block_name_prefix: rbd_data.109d2ae8944a format: 2 features: layering, striping, exclusive-lock, object-map, fast-diff, deep-flatten flags: stripe unit: 2048 kB stripe count: 16 [root@idcv-ceph0 ceph-disk0]# rbd create test_pool&#x2F;testimage11 –size 10240 –object-size 4M [root@idcv-ceph0 ceph-disk0]# rbd create test_pool&#x2F;testimage12 –size 10240 –object-size 4K [root@idcv-ceph0 ceph-disk0]# rbd info test_pool&#x2F;testimage11 rbd image ‘testimage11’: size 10240 MB in 2560 objects order 22 (4096 kB objects) block_name_prefix: rbd_data.10ac238e1f29 format: 2 features: layering, exclusive-lock, object-map, fast-diff, deep-flatten flags: [root@idcv-ceph0 ceph-disk0]# rbd info test_pool&#x2F;testimage12 rbd image ‘testimage12’: size 10240 MB in 2621440 objects order 12 (4096 bytes objects) block_name_prefix: rbd_data.10962ae8944a format: 2 features: layering, exclusive-lock, object-map, fast-diff, deep-flatten flags:</p>
</blockquote>
<p>2、映射image</p>
<blockquote>
<p>[root@idcv-ceph2 mnt]# rbd map test_pool&#x2F;testimage8 rbd: sysfs write failed In some cases useful info is found in syslog - try “dmesg | tail” or so. rbd: map failed: (22) Invalid argument [root@idcv-ceph2 mnt]# dmesg |tail [118760.024660] XFS (rbd0): Log I&#x2F;O Error Detected. Shutting down filesystem [118760.024710] XFS (rbd0): Please umount the filesystem and rectify the problem(s) [118760.024766] XFS (rbd0): Unable to update superblock counters. Freespace may not be correct on next mount. [118858.837102] XFS (rbd0): Mounting V5 Filesystem [118858.872345] XFS (rbd0): Ending clean mount [173522.968410] rbd: rbd0: encountered watch error: -107 [176701.031429] rbd: image testimage8: unsupported stripe unit (got 2097152 want 33554432) [176827.317008] rbd: image testimage8: unsupported stripe unit (got 2097152 want 33554432) [177423.107103] rbd: image testimage8: unsupported stripe unit (got 2097152 want 33554432) [177452.820032] rbd: image testimage8: unsupported stripe unit (got 2097152 want 33554432)</p>
</blockquote>
<p>3、排错发现rbd不支持条带特性需要需要使用rbd-nbd rbd-nbd支持所有的新特性，后续map时也不需要disable新特性，但是linux内核默认没有nbd模块，需要编译内核安装，可以参考下面链接<a target="_blank" rel="noopener" href="https://blog.csdn.net/miaodichiyou/article/details/76050361">https://blog.csdn.net/miaodichiyou/article/details/76050361</a></p>
<blockquote>
<p>[root@idcv-ceph2 ~]# wget <a target="_blank" rel="noopener" href="http://vault.centos.org/7.5.1804/updates/Source/SPackages/kernel-3.10.0-862.2.3.el7.src.rpm">http://vault.centos.org/7.5.1804/updates/Source/SPackages/kernel-3.10.0-862.2.3.el7.src.rpm</a> [root@idcv-ceph2 ~]# rpm -ivh kernel-3.10.0-862.2.3.el7.src.rpm [root@idcv-ceph2 ~]# cd &#x2F;root&#x2F;rpmbuild&#x2F; [root@idcv-ceph0 rpmbuild]# cd SOURCES&#x2F; [root@idcv-ceph0 SOURCES]# tar Jxvf linux-3.10.0-862.2.3.el7.tar.xz -C &#x2F;usr&#x2F;src&#x2F;kernels&#x2F; [root@idcv-ceph0 SOURCES]# cd &#x2F;usr&#x2F;src&#x2F;kernels&#x2F; [root@idcv-ceph0 kernels]# mv 3.10.0-862.6.3.el7.x86_64 3.10.0-862.6.3.el7.x86_64-old [root@idcv-ceph0 kernels]# mv linux-3.10.0-862.2.3.el7 3.10.0-862.6.3.el7.x86_64 [root@idcv-ceph0 3.10.0-862.6.3.el7.x86_64]# cd 3.10.0-862.6.3.el7.x86_64 [root@idcv-ceph0 3.10.0-862.6.3.el7.x86_64]# mkdir mrproper [root@idcv-ceph0 3.10.0-862.6.3.el7.x86_64]# cp ..&#x2F;3.10.0-862.6.3.el7.x86_64-old&#x2F;Module.symvers .&#x2F; [root@idcv-ceph0 3.10.0-862.6.3.el7.x86_64]# cp &#x2F;boot&#x2F;config-3.10.0-862.2.3.el7.x86_64 .&#x2F;.config [root@idcv-ceph0 3.10.0-862.6.3.el7.x86_64]# yum install elfutils-libelf-devel [root@idcv-ceph0 3.10.0-862.6.3.el7.x86_64]# make prepare [root@idcv-ceph0 3.10.0-862.6.3.el7.x86_64]# make scripts [root@idcv-ceph0 3.10.0-862.6.3.el7.x86_64]# make CONFIG_BLK_DEV_NBD&#x3D;m M&#x3D;drivers&#x2F;block [root@idcv-ceph0 3.10.0-862.6.3.el7.x86_64]# modinfo nbd [root@idcv-ceph0 3.10.0-862.6.3.el7.x86_64]# cp drivers&#x2F;block&#x2F;nbd.ko &#x2F;lib&#x2F;modules&#x2F;3.10.0-862.2.3.el7.x86_64&#x2F;kernel&#x2F;drivers&#x2F;block&#x2F; [root@idcv-ceph0 3.10.0-862.6.3.el7.x86_64]# depmod -a [root@idcv-ceph0 3.10.0-862.6.3.el7.x86_64]# modprobe nbd [root@idcv-ceph0 3.10.0-862.6.3.el7.x86_64]# lsmod |grep nbd nbd 17554 5</p>
</blockquote>
<p>4、使用rbd-nbd映射image</p>
<blockquote>
<p>[root@idcv-ceph0 ~]# rbd-nbd map test_pool&#x2F;testimage17 &#x2F;dev&#x2F;nbd0 [root@idcv-ceph0 ~]# rbd info test_pool&#x2F;testimage17 rbd image ‘testimage17’: size 10240 MB in 1280 objects order 23 (8192 kB objects) block_name_prefix: rbd_data.112d74b0dc51 format: 2 features: layering, striping flags: stripe unit: 1024 kB stripe count: 8 [root@idcv-ceph0 ~]# mkfs.xfs &#x2F;dev&#x2F;nbd0 meta-data&#x3D;&#x2F;dev&#x2F;nbd0 isize&#x3D;512 agcount&#x3D;4, agsize&#x3D;655360 blks &#x3D; sectsz&#x3D;512 attr&#x3D;2, projid32bit&#x3D;1 &#x3D; crc&#x3D;1 finobt&#x3D;0, sparse&#x3D;0 data &#x3D; bsize&#x3D;4096 blocks&#x3D;2621440, imaxpct&#x3D;25 &#x3D; sunit&#x3D;0 swidth&#x3D;0 blks naming &#x3D;version 2 bsize&#x3D;4096 ascii-ci&#x3D;0 ftype&#x3D;1 log &#x3D;internal log bsize&#x3D;4096 blocks&#x3D;2560, version&#x3D;2 &#x3D; sectsz&#x3D;512 sunit&#x3D;0 blks, lazy-count&#x3D;1 realtime &#x3D;none extsz&#x3D;4096 blocks&#x3D;0, rtextents&#x3D;0 [root@idcv-ceph0 ~]# mount &#x2F;dev&#x2F;nbd0 &#x2F;mnt&#x2F;ceph-8M&#x2F; [root@idcv-ceph0 ~]# df -h Filesystem Size Used Avail Use% Mounted on &#x2F;dev&#x2F;mapper&#x2F;centos-root 100G 3.5G 96G 4% &#x2F; devtmpfs 7.8G 0 7.8G 0% &#x2F;dev tmpfs 7.8G 0 7.8G 0% &#x2F;dev&#x2F;shm tmpfs 7.8G 12M 7.8G 1% &#x2F;run tmpfs 7.8G 0 7.8G 0% &#x2F;sys&#x2F;fs&#x2F;cgroup &#x2F;dev&#x2F;sda1 497M 150M 348M 31% &#x2F;boot tmpfs 1.6G 0 1.6G 0% &#x2F;run&#x2F;user&#x2F;0 &#x2F;dev&#x2F;sdb1 95G 40G 56G 42% &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;osd&#x2F;ceph-0 &#x2F;dev&#x2F;rbd0 10G 7.9G 2.2G 79% &#x2F;mnt&#x2F;ceph-disk0 &#x2F;dev&#x2F;rbd1 10G 7.9G 2.2G 79% &#x2F;mnt&#x2F;ceph-4M &#x2F;dev&#x2F;nbd0 10G 33M 10G 1% &#x2F;mnt&#x2F;ceph-8M</p>
</blockquote>
<p>5、dd测试性能 object-size为8M</p>
<blockquote>
<p>[root@idcv-ceph0 ~]# dd if&#x3D;&#x2F;dev&#x2F;zero of&#x3D;&#x2F;mnt&#x2F;ceph-8M&#x2F;file0-1 count&#x3D;800 bs&#x3D;10M conv&#x3D;fsync 800+0 records in 800+0 records out 8388608000 bytes (8.4 GB) copied, 50.964 s, 165 MB&#x2F;s [root@idcv-ceph0 ~]# dd if&#x3D;&#x2F;dev&#x2F;zero of&#x3D;&#x2F;mnt&#x2F;ceph-8M&#x2F;file0-1 count&#x3D;80 bs&#x3D;100M conv&#x3D;fsync 80+0 records in 80+0 records out 8388608000 bytes (8.4 GB) copied, 26.3178 s, 319 MB&#x2F;s</p>
</blockquote>
<p>object-size为32M</p>
<blockquote>
<p>[root@idcv-ceph0 ceph-32M]# rbd info test_pool&#x2F;testimage18 rbd image ‘testimage18’: size 40960 MB in 1280 objects order 25 (32768 kB objects) block_name_prefix: rbd_data.11052ae8944a format: 2 features: layering, striping, exclusive-lock, object-map, fast-diff, deep-flatten flags: stripe unit: 2048 kB stripe count: 8 [root@idcv-ceph0 ceph-32M]# dd if&#x3D;&#x2F;dev&#x2F;zero of&#x3D;&#x2F;mnt&#x2F;ceph-32M&#x2F;file0-1 count&#x3D;2000 bs&#x3D;10M conv&#x3D;fsync 2000+0 records in 2000+0 records out 20971520000 bytes (21 GB) copied, 67.4266 s, 311 MB&#x2F;s [root@idcv-ceph0 ceph-32M]# dd if&#x3D;&#x2F;dev&#x2F;zero of&#x3D;&#x2F;mnt&#x2F;ceph-32M&#x2F;file0-1 count&#x3D;20000 bs&#x3D;1M conv&#x3D;fsync 20000+0 records in 20000+0 records out 20971520000 bytes (21 GB) copied, 61.7757 s, 339 MB&#x2F;s</p>
</blockquote>
<p>6、测试方法汇总 4m cnt&#x3D;1 4k cnt&#x3D;1 32M cnt&#x3D;8，16 dd测试 1M 100M</p>
<p>32M &#x2F;mnt&#x2F;ceph-32M-8 &#x2F;mnt&#x2F;ceph-32M-16</p>
<blockquote>
<p>rbd create test_pool&#x2F;testimage8 –size 10240 –object-size 32M –stripe-unit 2097152 –stripe-count 16 dd if&#x3D;&#x2F;dev&#x2F;zero of&#x3D;&#x2F;mnt&#x2F;ceph-32M-16&#x2F;file32M count&#x3D;80 bs&#x3D;100M conv&#x3D;fsync dd if&#x3D;&#x2F;dev&#x2F;zero of&#x3D;&#x2F;mnt&#x2F;ceph-32M-16&#x2F;file32M count&#x3D;8000 bs&#x3D;1M conv&#x3D;fsync rbd create test_pool&#x2F;testimage19 –size 10240 –object-size 32M –stripe-unit 4194304 –stripe-count 8 dd if&#x3D;&#x2F;dev&#x2F;zero of&#x3D;&#x2F;mnt&#x2F;ceph-32M-8&#x2F;file32M count&#x3D;80 bs&#x3D;100M conv&#x3D;fsync dd if&#x3D;&#x2F;dev&#x2F;zero of&#x3D;&#x2F;mnt&#x2F;ceph-32M-8&#x2F;file32M count&#x3D;8000 bs&#x3D;1M conv&#x3D;fsync</p>
</blockquote>
<p>4M &#x2F;mnt&#x2F;ceph-4M</p>
<blockquote>
<p>rbd create test_pool&#x2F;testimage11 –size 10240 –object-size 4M dd if&#x3D;&#x2F;dev&#x2F;zero of&#x3D;&#x2F;mnt&#x2F;ceph-4M&#x2F;file4M count&#x3D;80 bs&#x3D;100M conv&#x3D;fsync dd if&#x3D;&#x2F;dev&#x2F;zero of&#x3D;&#x2F;mnt&#x2F;ceph-4M&#x2F;file4M count&#x3D;8000 bs&#x3D;1M conv&#x3D;fsync</p>
</blockquote>
<p>4K &#x2F;mnt&#x2F;ceph-4K</p>
<blockquote>
<p>rbd create test_pool&#x2F;testimage12 –size 10240 –object-size 4K dd if&#x3D;&#x2F;dev&#x2F;zero of&#x3D;&#x2F;mnt&#x2F;ceph-4K&#x2F;file4K count&#x3D;80 bs&#x3D;100M conv&#x3D;fsync dd if&#x3D;&#x2F;dev&#x2F;zero of&#x3D;&#x2F;mnt&#x2F;ceph-4K&#x2F;file4K count&#x3D;8000 bs&#x3D;1M conv&#x3D;fsync</p>
</blockquote>
<p>7、dd测试结果汇总</p>
<blockquote>
<p><img src="https://upload-images.jianshu.io/upload_images/7535971-7fbaf700b23eeb64.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></p>
</blockquote>
<p>8、使用fio随机写测试 先安装fio</p>
<blockquote>
<p>yum install libaio-devel wget <a target="_blank" rel="noopener" href="http://brick.kernel.dk/snaps/fio-2.1.10.tar.gz">http://brick.kernel.dk/snaps/fio-2.1.10.tar.gz</a> tar zxf fio-2.1.10.tar.gz cd fio-2.1.10&#x2F; make make install</p>
</blockquote>
<p>32M-8</p>
<blockquote>
<p>fio -ioengine&#x3D;libaio -bs&#x3D;1m -direct&#x3D;1 -thread -rw&#x3D;randwrite -size&#x3D;4G -filename&#x3D;&#x2F;dev&#x2F;nbd4 -name&#x3D;”EBS 1m randwrite test” -iodepth&#x3D;1 -runtime&#x3D;60 Run status group 0 (all jobs): WRITE: io&#x3D;4096.0MB, aggrb&#x3D;272729KB&#x2F;s, minb&#x3D;272729KB&#x2F;s, maxb&#x3D;272729KB&#x2F;s, mint&#x3D;15379msec, maxt&#x3D;15379msec Disk stats (read&#x2F;write): nbd4: ios&#x3D;0&#x2F;32280, merge&#x3D;0&#x2F;0, ticks&#x3D;0&#x2F;36624, in_queue&#x3D;36571, util&#x3D;97.61% fio -ioengine&#x3D;libaio -bs&#x3D;100m -direct&#x3D;1 -thread -rw&#x3D;randwrite -size&#x3D;4G -filename&#x3D;&#x2F;dev&#x2F;nbd4 -name&#x3D;”EBS 100m randwrite test” -iodepth&#x3D;1 -runtime&#x3D;60 Run status group 0 (all jobs): WRITE: io&#x3D;4000.0MB, aggrb&#x3D;326504KB&#x2F;s, minb&#x3D;326504KB&#x2F;s, maxb&#x3D;326504KB&#x2F;s, mint&#x3D;12545msec, maxt&#x3D;12545msec Disk stats (read&#x2F;write): nbd4: ios&#x3D;0&#x2F;31391, merge&#x3D;0&#x2F;0, ticks&#x3D;0&#x2F;1592756, in_queue&#x3D;1597878, util&#x3D;97.04%</p>
</blockquote>
<p>32M-16</p>
<blockquote>
<p>fio -ioengine&#x3D;libaio -bs&#x3D;1m -direct&#x3D;1 -thread -rw&#x3D;randwrite -size&#x3D;4G -filename&#x3D;&#x2F;dev&#x2F;nbd3 -name&#x3D;”EBS 1m randwrite test” -iodepth&#x3D;1 -runtime&#x3D;60 fio -ioengine&#x3D;libaio -bs&#x3D;100m -direct&#x3D;1 -thread -rw&#x3D;randwrite -size&#x3D;4G -filename&#x3D;&#x2F;dev&#x2F;nbd3 -name&#x3D;”EBS 100m randwrite test” -iodepth&#x3D;1 -runtime&#x3D;60</p>
</blockquote>
<p>4M</p>
<blockquote>
<p>fio -ioengine&#x3D;libaio -bs&#x3D;1m -direct&#x3D;1 -thread -rw&#x3D;randwrite -size&#x3D;4G -filename&#x3D;&#x2F;dev&#x2F;rbd1 -name&#x3D;”EBS 1m randwrite test” -iodepth&#x3D;1 -runtime&#x3D;60 fio -ioengine&#x3D;libaio -bs&#x3D;100m -direct&#x3D;1 -thread -rw&#x3D;randwrite -size&#x3D;4G -filename&#x3D;&#x2F;dev&#x2F;rbd1 -name&#x3D;”EBS 100m randwrite test” -iodepth&#x3D;1 -runtime&#x3D;60</p>
</blockquote>
<p>4K</p>
<blockquote>
<p>fio -ioengine&#x3D;libaio -bs&#x3D;1m -direct&#x3D;1 -thread -rw&#x3D;randwrite -size&#x3D;400M -filename&#x3D;&#x2F;dev&#x2F;rbd2 -name&#x3D;”EBS 1m randwrite test” -iodepth&#x3D;1 -runtime&#x3D;60 fio -ioengine&#x3D;libaio -bs&#x3D;100m -direct&#x3D;1 -thread -rw&#x3D;randwrite -size&#x3D;400M -filename&#x3D;&#x2F;dev&#x2F;rbd2 -name&#x3D;”EBS 100m randwrite test” -iodepth&#x3D;1 -runtime&#x3D;60</p>
</blockquote>
<p>9、fio测试结果汇总</p>
<blockquote>
<p><img src="https://upload-images.jianshu.io/upload_images/7535971-87ccc756c665fc05.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></p>
</blockquote>
<h3 id="三、使用s3brower测试对象存储读写"><a href="#三、使用s3brower测试对象存储读写" class="headerlink" title="三、使用s3brower测试对象存储读写"></a><strong>三、使用s3brower测试对象存储读写</strong></h3><p>1、创建对象存储账号密码</p>
<blockquote>
<p>[root@idcv-ceph0 cluster]# radosgw-admin user create –uid&#x3D;test –display-name&#x3D;”test” –access-key&#x3D;123456 –secret&#x3D;123456 [root@idcv-ceph0 cluster]# radosgw-admin user info –uid&#x3D;test { “user_id”: “test”, “display_name”: “test”, “email”: “”, “suspended”: 0, “max_buckets”: 1000, “auid”: 0, “subusers”: [], “keys”: [ { “user”: “test”, “access_key”: “123456”, “secret_key”: “123456” } ], “swift_keys”: [], “caps”: [], “op_mask”: “read, write, delete”, “default_placement”: “”, “placement_tags”: [], “bucket_quota”: { “enabled”: false, “max_size_kb”: -1, “max_objects”: -1 }, “user_quota”: { “enabled”: false, “max_size_kb”: -1, “max_objects”: -1 }, “temp_url_keys”: [] }</p>
</blockquote>
<p>2、安装配置s3brower</p>
<blockquote>
<p><img src="https://upload-images.jianshu.io/upload_images/7535971-0a1dd1f92552a217.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></p>
</blockquote>
<p>3、创建bucket上传下载测试</p>
<blockquote>
<p><img src="https://upload-images.jianshu.io/upload_images/7535971-17441354dc2dba4e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></p>
</blockquote>
<h3 id="四、使用s3fs挂载挂载对象存储读写"><a href="#四、使用s3fs挂载挂载对象存储读写" class="headerlink" title="四、使用s3fs挂载挂载对象存储读写"></a><strong>四、使用s3fs挂载挂载对象存储读写</strong></h3><p>测试对象存储方式写入文件，从rbd方式读目录 1、安装部署 <a target="_blank" rel="noopener" href="https://github.com/s3fs-fuse/s3fs-fuse/releases">https://github.com/s3fs-fuse/s3fs-fuse/releases</a></p>
<p>安装 查看README On CentOS 7:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo yum install automake fuse fuse-devel gcc-c++ git libcurl-devel libxml2-devel make openssl-devel</span><br></pre></td></tr></table></figure>

<p>Then compile from master via the following commands:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/s3fs-fuse/s3fs-fuse.git</span><br><span class="line">cd s3fs-fuse</span><br><span class="line">./autogen.sh</span><br><span class="line">./configure</span><br><span class="line">make</span><br><span class="line">sudo make install</span><br></pre></td></tr></table></figure>

<blockquote>
<p>[root@idcv-ceph0 ~]# wget <a target="_blank" rel="noopener" href="https://github.com/s3fs-fuse/s3fs-fuse/archive/v1.83.tar.gz">https://github.com/s3fs-fuse/s3fs-fuse/archive/v1.83.tar.gz</a> [root@idcv-ceph0 ~]# ls [root@idcv-ceph0 ~]# tar zxvf v1.83.tar.gz [root@idcv-ceph0 s3fs-fuse-1.83]# cd s3fs-fuse-1.83&#x2F; [root@idcv-ceph0 s3fs-fuse-1.83]# ls [root@idcv-ceph0 s3fs-fuse-1.83]# vi README.md [root@idcv-ceph0 s3fs-fuse-1.83]# yum install automake fuse fuse-devel gcc-c++ git libcurl-devel libxml2-devel make openssl-devel [root@idcv-ceph0 s3fs-fuse-1.83]# .&#x2F;autogen.sh [root@idcv-ceph0 s3fs-fuse-1.83]# ls [root@idcv-ceph0 s3fs-fuse-1.83]# .&#x2F;configure [root@idcv-ceph0 s3fs-fuse-1.83]# make [root@idcv-ceph0 s3fs-fuse-1.83]# make install [root@idcv-ceph0 s3fs-fuse-1.83]# mkdir &#x2F;mnt&#x2F;s3 [root@idcv-ceph0 s3fs-fuse-1.83]# vi &#x2F;root&#x2F;.passwd-s3fs [root@idcv-ceph0 s3fs-fuse-1.83]# chmod 600 &#x2F;root&#x2F;.passwd-s3fs</p>
</blockquote>
<p>2、挂载</p>
<blockquote>
<p>[root@idcv-ceph0 ~]# s3fs testbucket &#x2F;mnt&#x2F;s3 -o url&#x3D;<a target="_blank" rel="noopener" href="http://172.20.1.139:7480/">http://172.20.1.139:7480</a> -o umask&#x3D;0022 -o use_path_request_style [root@idcv-ceph0 ~]# df -h Filesystem Size Used Avail Use% Mounted on &#x2F;dev&#x2F;sdb1 95G 75G 21G 79% &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;osd&#x2F;ceph-0 &#x2F;dev&#x2F;rbd1 10G 7.9G 2.2G 79% &#x2F;mnt&#x2F;ceph-4M &#x2F;dev&#x2F;rbd2 10G 814M 9.2G 8% &#x2F;mnt&#x2F;ceph-4K &#x2F;dev&#x2F;nbd3 10G 7.9G 2.2G 79% &#x2F;mnt&#x2F;ceph-32M-16 &#x2F;dev&#x2F;nbd4 10G 33M 10G 1% &#x2F;mnt&#x2F;ceph-32M-8 s3fs 256T 0 256T 0% &#x2F;mnt&#x2F;s3</p>
</blockquote>
<p>3、验证读写</p>
<blockquote>
<p>[root@idcv-ceph0 ~]# ls &#x2F;mnt&#x2F;s3&#x2F;images&#x2F; kernel-3.10.0-862.2.3.el7.src.rpm nbd.ko test.jpg [root@idcv-ceph0 ~]# cp &#x2F;etc&#x2F;hosts hosts hosts.allow hosts.deny<br>[root@idcv-ceph0 ~]# cp &#x2F;etc&#x2F;hosts &#x2F;mnt&#x2F;s3&#x2F;images&#x2F; [root@idcv-ceph0 ~]# ls &#x2F;mnt&#x2F;s3&#x2F;images&#x2F; hosts kernel-3.10.0-862.2.3.el7.src.rpm nbd.ko test.jpg</p>
</blockquote>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://minminmsn.github.io/2018/12/03/2018/12/2018-12-03-%E7%A6%85%E7%9A%84%E8%A1%8C%E5%9B%8A%E8%B7%AF%E7%BA%BF%E5%9B%BE/index/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jerry Min">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="MinMinMsn">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/12/03/2018/12/2018-12-03-%E7%A6%85%E7%9A%84%E8%A1%8C%E5%9B%8A%E8%B7%AF%E7%BA%BF%E5%9B%BE/index/" class="post-title-link" itemprop="url">禅的行囊路线图</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-12-03 08:00:00" itemprop="dateCreated datePublished" datetime="2018-12-03T08:00:00+08:00">2018-12-03</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2023-05-26 15:06:30" itemprop="dateModified" datetime="2023-05-26T15:06:30+08:00">2023-05-26</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/zen/" itemprop="url" rel="index"><span itemprop="name">zen</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="比尔·波特"><a href="#比尔·波特" class="headerlink" title="比尔·波特"></a><strong>比尔·波特</strong></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">比尔·波特，美国当代作家、翻译家和著名汉学家，这次朝圣之旅始于2006年2月底的北京，终于2006年4月初的香港，耗时一个半月，从南到北游览了大半个中国。</span><br><span class="line">比尔的目的地是禅在中国的发源地，其中最重要者，包括了禅宗六位祖师 初祖达摩、二祖慧可、三祖僧璨、四祖道信、五祖弘忍和六祖惠能 开创的道场。</span><br></pre></td></tr></table></figure>

<h3 id="路线图"><a href="#路线图" class="headerlink" title="路线图"></a><strong>路线图</strong></h3><blockquote>
<p><a target="_blank" rel="noopener" href="https://i.loli.net/2018/12/03/5c04885469bb8.jpg"><img src="/images/5c04885469bb8.jpg"></a></p>
</blockquote>
<h3 id="行程表"><a href="#行程表" class="headerlink" title="行程表"></a><strong>行程表</strong></h3><h4 id="一、北京（2006年2月底）"><a href="#一、北京（2006年2月底）" class="headerlink" title="一、北京（2006年2月底）"></a><strong>一、北京（2006年2月底）</strong></h4><p>1、古钟博物馆（大钟寺） 2、北京人遗址博物馆 3、北京房山云居寺，石经山藏经洞（雷音洞）</p>
<h4 id="二、大同（2006年3月初）"><a href="#二、大同（2006年3月初）" class="headerlink" title="二、大同（2006年3月初）"></a><strong>二、大同（2006年3月初）</strong></h4><p>1、大同云冈石窟 为中国规模最大的古代石窟群之一，与敦煌莫高窟、洛阳龙门石窟和天水麦积山石窟并称为中国四大石窟艺术宝库。 2、大同华严寺 华严寺依据佛教经典《华严经》而命名。 3、大同九龙壁 建于明代洪武末年，是明太祖朱元璋第十三子代王朱桂府前的照壁。 4、鹿野苑禅窟</p>
<h4 id="三、五台山（2006年3月上旬）"><a href="#三、五台山（2006年3月上旬）" class="headerlink" title="三、五台山（2006年3月上旬）"></a><strong>三、五台山（2006年3月上旬）</strong></h4><p>山西五台山是中国四大四大名山之一，其他三个是浙江普陀山、四川峨眉山、安徽九华山，它们分别是文殊菩萨、观世音菩萨、普贤菩萨、地藏菩萨的道场。 1、途径悬空寺 2、北台（因积雪未达顶峰） 3、显通寺 显通寺是五台山第一大寺，始建于汉明帝永平年间，初名大孚灵鹫寺，清康熙二十六年（公元1687年），改名为大显通寺，它是中国最早的佛寺之一。 4、碧山寺 5、竹林寺</p>
<h4 id="四、石家庄（2006年3月中旬）"><a href="#四、石家庄（2006年3月中旬）" class="headerlink" title="四、石家庄（2006年3月中旬）"></a><strong>四、石家庄（2006年3月中旬）</strong></h4><p>1、赵县柏林寺（2016年3月12日） 公元858年，从谂和尚结束了他二十多年的云游生涯，驻锡赵州城弘法，成为禅门一代宗师，世称赵州禅师。 2、成安县匡教寺 始建于南北朝北齐天宝六年（公元555年），隋开皇初年佛教禅宗二祖慧可大师在此讲经说法达三十四余年，寺内为之筑台名“说法台”。</p>
<h4 id="五、洛阳（2006年3月中旬）"><a href="#五、洛阳（2006年3月中旬）" class="headerlink" title="五、洛阳（2006年3月中旬）"></a><strong>五、洛阳（2006年3月中旬）</strong></h4><p>1、洛阳白马寺 创建于东汉永平十一年（公元68年），中国第一古刹，世界著名伽蓝，是佛教传入中国后兴建的第一座官办寺院，有中国佛教的“祖庭”和“释源”之称。 2、永宁寺遗址 北魏後期都城洛阳的重要佛寺故址。 3、空相寺 空相寺是与白马寺同一时期建造的佛门圣地，是中国最早的寺院之一，公元536年，禅宗初祖达摩圆寂后葬在这里。 4、嵩山少林寺 少林寺是世界著名的佛教寺院，是汉传佛教的禅宗祖庭，在中国佛教史上占有重要地位，被誉为“天下第一名刹”。 5、达摩洞 禅宗初祖”达摩曾在此洞中面壁九年，因称“达摩洞”或“达摩面壁洞”。 6、诗魔白居易墓</p>
<h4 id="六、司空山（2006年3月中旬）"><a href="#六、司空山（2006年3月中旬）" class="headerlink" title="六、司空山（2006年3月中旬）"></a><strong>六、司空山（2006年3月中旬）</strong></h4><p>1、无相寺 无相寺，即二祖寺，唐天宝年间(742—755年)玄宗敕建。 2、法云寺</p>
<h4 id="七、潜山（2006年3月18日）"><a href="#七、潜山（2006年3月18日）" class="headerlink" title="七、潜山（2006年3月18日）"></a><strong>七、潜山（2006年3月18日）</strong></h4><p>三祖寺 三祖禅寺在南朝梁武帝时开山，原名山谷寺，后因禅宗三祖僧璨在此驻锡，故名三祖寺。</p>
<h4 id="八、黄梅（2006年3月中旬）"><a href="#八、黄梅（2006年3月中旬）" class="headerlink" title="八、黄梅（2006年3月中旬）"></a><strong>八、黄梅（2006年3月中旬）</strong></h4><p>1、黄梅双峰山四祖寺 黄梅四祖寺，古称幽居寺，原名正觉寺，又名双峰寺，是中国佛教禅宗第四代祖师道信大师的道场，寺庙创建于唐武德七年（公元624年）距今已有1370年的历史，是中国禅宗第一所寺院。 2、黄梅双峰山老祖寺 古时又称紫云山寺。是由印度来华高僧千岁宝掌禅师开山创建。老祖当年落户在此地修身养性，恰逢于四祖道信、五祖弘忍住破额（双峰山）、冯茂（东山）的二山，同为达摩禅宗门下的弟子，皆因老祖年长寿高，尊称为老祖，老祖创立的伽蓝为老祖寺。</p>
<h4 id="九、黄梅（2006年3月下旬）"><a href="#九、黄梅（2006年3月下旬）" class="headerlink" title="九、黄梅（2006年3月下旬）"></a><strong>九、黄梅（2006年3月下旬）</strong></h4><p>冯茂山五祖寺 五祖寺，原名东山寺，或东禅寺，后世改称五祖寺，建于唐永徽五年（654年），是中国禅宗第五代祖师弘忍大师的道场，也是六祖慧能大师得法受衣钵之圣地，被御赐为“天下祖庭”。</p>
<h4 id="十、九江（2006年3月23日）"><a href="#十、九江（2006年3月23日）" class="headerlink" title="十、九江（2006年3月23日）"></a><strong>十、九江（2006年3月23日）</strong></h4><p>庐山南麓的寺院</p>
<h4 id="十一、大金山寺（2006年3月下旬）"><a href="#十一、大金山寺（2006年3月下旬）" class="headerlink" title="十一、大金山寺（2006年3月下旬）"></a><strong>十一、大金山寺（2006年3月下旬）</strong></h4><p>禅宗尼众寺院</p>
<h4 id="十二、武汉（2006年3月下旬）"><a href="#十二、武汉（2006年3月下旬）" class="headerlink" title="十二、武汉（2006年3月下旬）"></a><strong>十二、武汉（2006年3月下旬）</strong></h4><p>1、汉阳龟山古琴台 2、钟子期墓</p>
<h4 id="十三、当阳（2006年3月底）"><a href="#十三、当阳（2006年3月底）" class="headerlink" title="十三、当阳（2006年3月底）"></a><strong>十三、当阳（2006年3月底）</strong></h4><p>1、玉泉寺 玉泉寺，是中国佛教天台宗祖庭之一。 2、度门寺 旧为唐代高僧神秀大师说法栖息之地，亦称“北宗初地”。</p>
<h4 id="十四、韶关（2006年4月初）"><a href="#十四、韶关（2006年4月初）" class="headerlink" title="十四、韶关（2006年4月初）"></a><strong>十四、韶关（2006年4月初）</strong></h4><p>1、大鉴寺 2、云门大觉禅寺 3、南华寺 是禅宗六祖惠能宏扬“南宗禅法”的发源地。 4、月华寺 建造寺庙的僧人是来自印度的智药和尚。 5、六祖避难石（2006年4月4日）</p>
<h4 id="十五、广州（2006年4月上旬）"><a href="#十五、广州（2006年4月上旬）" class="headerlink" title="十五、广州（2006年4月上旬）"></a><strong>十五、广州（2006年4月上旬）</strong></h4><p>1、华林寺（2006年4月5日） 华林寺前身是“西来庵”，达摩遵从师父自训谕，西来弘化禅宗妙旨，并于梁武帝普通年间从海上到达广州城外的珠江北岸（今下九路），“结草为庵”，潜心苦修。 2、光孝寺 原法性寺，慧能在此由印宗法师剃发受戒。 3、国恩寺 六祖圆寂处。 4、南海神庙 南海神庙又称波罗庙，是古代中国劳动人民祭海的场所。</p>
<h4 id="十六、香港（2006年4月中旬）"><a href="#十六、香港（2006年4月中旬）" class="headerlink" title="十六、香港（2006年4月中旬）"></a><strong>十六、香港（2006年4月中旬）</strong></h4><p>竹林禅院 竹林禅院是香港市中心区的最大佛寺。</p>
<h3 id="僧宝之言"><a href="#僧宝之言" class="headerlink" title="僧宝之言"></a><strong>僧宝之言</strong></h3><h4 id="五台山碧山寺方丈妙江"><a href="#五台山碧山寺方丈妙江" class="headerlink" title="五台山碧山寺方丈妙江"></a><strong>五台山碧山寺方丈妙江</strong></h4><blockquote>
<p>他说的是“研究禅”。他喜欢用“研究”这个词。修禅如同做研究，研究此时此地，研究五蕴十二处十八界，研究心。“每一念都是修行”，他说，“并不是非要看书才能学佛。一念即佛。思考空和有。研究这些就是修禅。喝茶也是修禅，吃饭也是修禅。禅是一切修行的基础。”</p>
</blockquote>
<h4 id="达摩二入四行论"><a href="#达摩二入四行论" class="headerlink" title="达摩二入四行论"></a><strong>达摩二入四行论</strong></h4><blockquote>
<p>若夫入道多途，要而言之，不出二种。一是理入，二是行入。理入者，谓藉教悟宗，深信含生同一真性，但为客尘妄想所覆，不能显了。若也舍妄改真，凝心壁观，无自无他，凡圣等一，坚住不移，更不随于文教，此即与理冥符，无有分别，寂然无为，名之理入。 行入者，所谓四行。其余诸行，悉入此行中。何等为四？一者报怨行，二者随缘行，三者无所求行，四者称法行。</p>
</blockquote>
<h4 id="少林寺僧值延颖"><a href="#少林寺僧值延颖" class="headerlink" title="少林寺僧值延颖"></a><strong>少林寺僧值延颖</strong></h4><blockquote>
<p>他没等我提问，就主动说道：“寺里的年轻和尚如果请我说法，我就说去喝杯茶吧。如果他们还不明白，我就让他们去体会茶的味道。大道藏在我们做的每件事情当中。喝茶、吃饭、大便，都无所谓，都是道。如果你不能在平常生活里见道，读多少书都是浪费时间。学武也是一样。每一拳、每一脚都是道。你是谁和你做什么了无分别。如果你有分别心，就不能见道。在少林寺，我们要做到内外无别。你今天能坐在我房间里跟我喝茶，说明我们一定有缘。来，再喝一杯。”</p>
</blockquote>
<h4 id="大金山寺监院顿成"><a href="#大金山寺监院顿成" class="headerlink" title="大金山寺监院顿成"></a><strong>大金山寺监院顿成</strong></h4><blockquote>
<p>“不管选择哪条道路，一旦开始修行，早晚都要学习经典以及历代祖师留下的言教。我们鼓励比丘尼学习这些经典和言教，但是不要忘记，学习它们是为了回到自己的内心。要点是修心，而不是修文字。有人读了佛经之后就觉得自己开悟了，这是盲目。我个人最喜欢的经典是六祖《坛经》，读过之后领悟很多，但它代替不了修行。这就好比你在书上看到一个很美的地方，你很想去。想去就得迈开两腿走路，而不是继续读书——不管读多少遍，你也到不了那个地方。修行就是这个意思。要行，而不是坐在那儿看、想。”</p>
</blockquote>
<h4 id="大金山寺主持印空法师"><a href="#大金山寺主持印空法师" class="headerlink" title="大金山寺主持印空法师"></a><strong>大金山寺主持印空法师</strong></h4><blockquote>
<p>“过上自食其力的生活对修行人非常重要。修禅不光是修心，还要修身。禅就是生活。这种修行是要花时间的，不是一两个月，而是好几年，甚至一辈子。没有耐心的人不能修禅，她们只能去修净土，念阿弥陀佛。修禅需要很大的毅力，这不是每个人都能做到的。” “净土宗就像基督教，强调的是虔诚，而禅宗强调的是自立。接着，她指了指我的茶杯，说茶要凉 了，如果我不尝一口，是不会知道茶的味道的。”</p>
</blockquote>
<h3 id="竹林禅院方丈意昭"><a href="#竹林禅院方丈意昭" class="headerlink" title="竹林禅院方丈意昭"></a><strong>竹林禅院方丈意昭</strong></h3><blockquote>
<p>“我们坐在这房子里，它有门，还有窗。如果关上门窗，我们就出不去了。为什么？是因为我们痴迷于房子和门窗的观念。这些观念就是我们的无明。如果能够领悟到这些观念的虚幻本质，我们就随时都可以出去。而根本上来说，其实‘出去’也是虚幻的——没有房子，也就没有出。我们看到的这一切都是镜花水月。对于虚老这样的禅师来说，禅定功夫有时候有用，有时候又可能毫无用处。” 意昭把照片递给我，继续说道：“自我并不存在。由于无明的遮蔽，我们总是因眼前所见而生起分别之心。无明使我们陷入无尽的生死轮回，而解脱之道就在于领悟世界的本质——自我并不存在。你我从未出生，也永远不会死亡。无明导致的分别之心使我们贪恋物欲，进而贪生怕死。释迦牟尼的证悟之道便是从领悟‘无我’，进而解脱生死开始的。这是一切众生都能做到的事。”</p>
</blockquote>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://minminmsn.github.io/2018/11/30/2018/11/2018-11-30-tcpcopy-1-0-%E9%83%A8%E7%BD%B2%E6%B5%8B%E8%AF%95%E4%BD%BF%E7%94%A8/index/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jerry Min">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="MinMinMsn">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/11/30/2018/11/2018-11-30-tcpcopy-1-0-%E9%83%A8%E7%BD%B2%E6%B5%8B%E8%AF%95%E4%BD%BF%E7%94%A8/index/" class="post-title-link" itemprop="url">TCPCOPY 1.0 部署测试使用</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-11-30 08:00:00" itemprop="dateCreated datePublished" datetime="2018-11-30T08:00:00+08:00">2018-11-30</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2023-05-26 15:06:30" itemprop="dateModified" datetime="2023-05-26T15:06:30+08:00">2023-05-26</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/middleware/" itemprop="url" rel="index"><span itemprop="name">middleware</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <blockquote>
<p>简介 TCPCOPY 是一个 tcp 流量的实时复制工具，其1.0版本由网易工程师 @tcpcopy 开发和维护。一般用来将生产环境的线上流量实时复制到测试环境进行测试。例如新系统上线前，如果我们希望进行一些基本的压力测试，那么我们可以直接利用 tcpcopy 来复制线上的流量过来对系统进行测试，这样的好处是测试数据接近真实水平，且实施起来相对简单。</p>
</blockquote>
<h3 id="一、架构"><a href="#一、架构" class="headerlink" title="一、架构"></a>一、架构</h3><blockquote>
<p><a target="_blank" rel="noopener" href="https://i.loli.net/2018/11/30/5c00a84e88627.gif"><img src="/images/5c00a84e88627.gif"></a></p>
</blockquote>
<h3 id="二、安装"><a href="#二、安装" class="headerlink" title="二、安装"></a>二、安装</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line">测试中用到的3台服务器信息如下：</span><br><span class="line">线上服务器 online server         内网IP地址 172.16.0.8/24    外网IP 223.202.0.8/28</span><br><span class="line">测试服务器 test server        内网IP地址 172.16.0.230/24   外网IP 223.202.0.13/28</span><br><span class="line">辅助服务器 assistant server      内网IP地址 172.16.0.219/24   外网IP 223.202.1.76/24</span><br><span class="line">172.16.0.8是线上服务器，172.16.0.230和172.17.0.219是测试环境。我们在172.17.0.8上运行tcpcopy把线上流量拷贝到172.17.0.230，在172.17.0.230上我们通过路由将应答转向</span><br><span class="line">172.16.0.219，在172.16.0.219上把应答丢弃。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">第一步，在 online server 172.16.0.8上安装并运行 tcpcopy daemon :</span><br><span class="line">我们从github上下载1.0版本的源码包；</span><br><span class="line">wget https://github.com/session-replay-tools/tcpcopy/archive/1.0.0.tar.gz -O tcpcopy-1.0.0.tar.gz</span><br><span class="line">安装依赖包；</span><br><span class="line">yum -y install libpcap-devel</span><br><span class="line">解压编译和安装；</span><br><span class="line">tar zxvf tcpcopy-1.0.0.tar.gz</span><br><span class="line">cd tcpcopy-1.0.0</span><br><span class="line">./configure （默认raw socket方式抓包）</span><br><span class="line">make </span><br><span class="line">make install</span><br><span class="line">最后运行 tcpcopy；</span><br><span class="line">/usr/local/tcpcopy/sbin/tcpcopy -x 80-172.16.0.230:80 -s 172.16.0.219 -c 192.168.100.x -d -C 4 -l tcpcopy.log  -P /var/run/tcpcopy.pid</span><br><span class="line"></span><br><span class="line">指令说明：</span><br><span class="line">-x 80-172.16.0.230:80将本机上80端口的流量复制到172.16.0.230（测试服务器）的80端口</span><br><span class="line">-s指定intercept进程所在的服务器172.16.0.219。（丢包服务器）</span><br><span class="line">-c修改请求的host地址为192.168.100.x，以便在230测试服务器上设置路由（设置路由是为了将应答转向丢219包服务器）</span><br><span class="line">-C 开启4个进程</span><br><span class="line">-d 以daemon形式运行</span><br><span class="line">-l 记录日志</span><br><span class="line">-P 记录pid</span><br><span class="line">其他参数可以通过/usr/local/tcpcopy/sbin/tcpcopy -h查看</span><br><span class="line"></span><br><span class="line">成功运行后可以观察到的网路连接状态：</span><br><span class="line">#ss -an|grep 172.16.0.219</span><br><span class="line">ESTAB      0      0                172.16.0.8:49034         172.16.0.219:36524 </span><br><span class="line">ESTAB      0      0                172.16.0.8:49035         172.16.0.219:36524 </span><br><span class="line">ESTAB      0      0                172.16.0.8:49032         172.16.0.219:36524 </span><br><span class="line">ESTAB      0      0                172.16.0.8:49033         172.16.0.219:36524 </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">第二步，在 auxiliary server 172.16.0.219上安装并运行 intercept daemon :</span><br><span class="line">从github上下载1.0版本的源码包；</span><br><span class="line">wget https://github.com/session-replay-tools/intercept/archive/1.0.0.tar.gz -O intercept-1.0.0.tar.gz</span><br><span class="line">安装依赖包；</span><br><span class="line">yum -y install libpcap-devel（CentOS6系统直接yum安装即可1.4版本）</span><br><span class="line">注意CentOS5系统libpcap-devel版本是libpcap-devel-0.9.4-15.el5，intercept-1.0需要libcap-devel1.4以上版本此时需要源码包安装</span><br><span class="line">yum remove libpcap</span><br><span class="line">wget http://www.tcpdump.org/release/libpcap-1.4.0.tar.gz</span><br><span class="line">tar zxvf libpcap-1.4.0.tar.gz</span><br><span class="line">cd libpcap-1.4.0</span><br><span class="line">./configure</span><br><span class="line">make</span><br><span class="line">make install</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">解压编译和安装；</span><br><span class="line">tar zxvf intercept-1.0.0.tar.gz</span><br><span class="line">cd intercept-1.0.0</span><br><span class="line">./configure （默认raw socket方式抓包）</span><br><span class="line">make &amp;&amp; make install</span><br><span class="line">最后运行 intercept；</span><br><span class="line">/usr/local/intercept/sbin/intercept -i eth1 -l intercept.log -P /var/run/intercept.pid -F &#x27;tcp and src port 80&#x27; -d</span><br><span class="line"></span><br><span class="line">指令说明：</span><br><span class="line">-i 监控网卡接口</span><br><span class="line">-l 记录日志</span><br><span class="line">-F 监控的协议和端口</span><br><span class="line">-P 记录pid</span><br><span class="line">-d 以daemon形式运行</span><br><span class="line">其他参数可以通过/usr/local/intercept/sbin/intercept -h查看</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">成功运行后可以观察到的网路连接状态：</span><br><span class="line"># ss -an |grep 36524</span><br><span class="line">LISTEN     0      5                         *:36524                    *:*     </span><br><span class="line">ESTAB      0      66             172.16.0.219:36524           172.16.0.8:49034 </span><br><span class="line">ESTAB      0      0              172.16.0.219:36524           172.16.0.8:49035 </span><br><span class="line">ESTAB      0      66             172.16.0.219:36524           172.16.0.8:49032 </span><br><span class="line">ESTAB      0      0              172.16.0.219:36524           172.16.0.8:49033 </span><br><span class="line"></span><br><span class="line">第三步，在 test server 172.16.0.230上设置一条路由 :</span><br><span class="line">[root@bogon ~]# route add -net 192.168.100.0 netmask 255.255.255.0 gw 172.16.0.219</span><br><span class="line">成功运行测试时可以观察到的网络连接状态：</span><br><span class="line"># ss -an |head</span><br><span class="line">State      Recv-Q Send-Q        Local Address:Port          Peer Address:Port</span><br><span class="line">LISTEN     0      0                 127.0.0.1:199                      *:*    </span><br><span class="line">LISTEN     512    0                         *:80                       *:*    </span><br><span class="line">ESTAB   0      0              172.16.0.230:80           192.168.100.1:62602</span><br><span class="line">ESTAB   0      0              172.16.0.230:80           192.168.100.4:54595</span><br><span class="line">ESTAB   0      0              172.16.0.230:80           192.168.100.3:53566</span><br><span class="line">ESTAB   0      0              172.16.0.230:80           192.168.100.6:49260</span><br><span class="line">ESTAB   0      0              172.16.0.230:80           192.168.100.8:57598</span><br><span class="line">ESTAB   0      0              172.16.0.230:80           192.168.100.7:64454</span><br><span class="line">ESTAB   0      0              172.16.0.230:80           192.168.100.1:63081</span><br></pre></td></tr></table></figure>

<h3 id="参考链接："><a href="#参考链接：" class="headerlink" title="参考链接："></a>参考链接：</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">http://blog.csdn.net/wangbin579/article/details/8949315</span><br><span class="line">http://blog.csdn.net/wangbin579/article/details/8950282</span><br><span class="line"></span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://minminmsn.github.io/2018/11/27/2018/11/2018-11-27-promethousalertmanagergrafana%E7%9B%91%E6%8E%A7%E4%BD%93%E7%B3%BB/index/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jerry Min">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="MinMinMsn">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/11/27/2018/11/2018-11-27-promethousalertmanagergrafana%E7%9B%91%E6%8E%A7%E4%BD%93%E7%B3%BB/index/" class="post-title-link" itemprop="url">Promethous+Alertmanager+Grafana监控体系</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-11-27 08:00:00" itemprop="dateCreated datePublished" datetime="2018-11-27T08:00:00+08:00">2018-11-27</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2023-05-26 15:06:30" itemprop="dateModified" datetime="2023-05-26T15:06:30+08:00">2023-05-26</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/op/" itemprop="url" rel="index"><span itemprop="name">op</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <blockquote>
<p><strong>Promethous+Alertmanager+Grafana</strong> 监控技术栈如下： Prometheus(最新版)：基于TSDB的微服务指标采集&amp;报警； Alertmanager：报警服务； Grafana（&gt;&#x3D;5.x）：监控报表展示。</p>
</blockquote>
<h3 id="一、安装"><a href="#一、安装" class="headerlink" title="一、安装"></a><strong>一、安装</strong></h3><h4 id="1-Prometheus安装"><a href="#1-Prometheus安装" class="headerlink" title="1.Prometheus安装"></a>1.Prometheus安装</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># wget https://github.com/prometheus/prometheus/releases/download/v2.5.0/prometheus-2.5.0.linux-amd64.tar.gz</span><br><span class="line"># tar zxvf prometheus-2.5.0.linux-amd64.tar.gz -C /usr/local/</span><br><span class="line"># cd /usr/local/</span><br><span class="line"># ln -s prometheus-2.5.0.linux-amd64 prometheus</span><br><span class="line"># chown work:work prometheus* -R</span><br><span class="line"># cd prometheus</span><br><span class="line"># ls</span><br><span class="line"># console_libraries consoles LICENSE NOTICE prometheus prometheus.yml promtool</span><br></pre></td></tr></table></figure>

<h4 id="2-Alertmanager安装"><a href="#2-Alertmanager安装" class="headerlink" title="2.Alertmanager安装"></a>2.Alertmanager安装</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># wget https://github.com/prometheus/alertmanager/releases/download/v0.15.3/alertmanager-0.15.3.linux-amd64.tar.gz</span><br><span class="line"># tar zxvf alertmanager-0.15.3.linux-amd64.tar.gz </span><br><span class="line"># ln -s alertmanager-0.15.3.linux-amd64 alertmanager</span><br><span class="line"># chown work:work alertmanager* -R</span><br><span class="line"># cd alertmanager</span><br><span class="line"># ls</span><br><span class="line"># alertmanager alertmanager.yml amtool LICENSE NOTICE</span><br></pre></td></tr></table></figure>

<h4 id="3-Grafana安装"><a href="#3-Grafana安装" class="headerlink" title="3.Grafana安装"></a>3.Grafana安装</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># wget wget https://dl.grafana.com/oss/release/grafana-5.3.4-1.x86_64.rpm </span><br><span class="line"># rpm -Uvh grafana-5.3.4-1.x86_64.rpm </span><br><span class="line"># systemctl restart grafana.service</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://minminmsn.github.io/2018/11/24/2018/11/2018-11-24-https%E9%85%8D%E7%BD%AE%E4%BC%98%E5%8C%96%E5%8F%8A%E6%B3%A8%E6%84%8F%E7%82%B9/index/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jerry Min">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="MinMinMsn">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/11/24/2018/11/2018-11-24-https%E9%85%8D%E7%BD%AE%E4%BC%98%E5%8C%96%E5%8F%8A%E6%B3%A8%E6%84%8F%E7%82%B9/index/" class="post-title-link" itemprop="url">HTTPS配置优化及注意点</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-11-24 08:00:00" itemprop="dateCreated datePublished" datetime="2018-11-24T08:00:00+08:00">2018-11-24</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2023-05-26 15:06:30" itemprop="dateModified" datetime="2023-05-26T15:06:30+08:00">2023-05-26</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/middleware/" itemprop="url" rel="index"><span itemprop="name">middleware</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <blockquote>
<p>Nginx官网反向代理时配置SSL证书，需要enable ngx_http_ssl_module模块，且需要支持的openssl开发版，默认配置参数比较少，但是可以根据实际情况对性能及安全性做成优化，具体如下！</p>
</blockquote>
<h3 id="一、SSL参数具体优化（这里只填主要的）"><a href="#一、SSL参数具体优化（这里只填主要的）" class="headerlink" title="一、SSL参数具体优化（这里只填主要的）"></a>一、SSL参数具体优化（这里只填主要的）</h3><p><strong>1. ssl_protocols TLSv1.3 TLSv1.2 TLSv1.1 TLSv1;</strong></p>
<blockquote>
<p>苹果APP只支持TLSv1.2，会优先使用TLSv1.2，考虑到客户端兼容性，其他2各也加上</p>
</blockquote>
<p><strong>2. ssl_certificate_key ssl&#x2F;minminmsn.comsha256.key;</strong></p>
<blockquote>
<p>私钥，服务器加密使用</p>
</blockquote>
<p><strong>3. ssl_certificate ssl&#x2F;minminmsn.comsha256.crt;</strong></p>
<blockquote>
<p>证书，客户端解密使用，服务器证书和中间证书合并到一个文件，不需要根证书；另外1.7.3版本增加了新指令ssl_password_file可以支持带密码的私钥</p>
</blockquote>
<p><strong>4. ssl_session_cache shared:SSL:10m;</strong></p>
<blockquote>
<p>会耗费一部分内存，1m可以同时保存4000个会话，10m理论支持4万个会话，注意这个改动后需要重启 nginx才会生效，nginx启动时会申请资源，一般分配后比较难修改，内存空间不足时老的会话自动清理用于新的会话</p>
</blockquote>
<p><strong>5. ssl_session_timeout 60m;</strong></p>
<blockquote>
<p>考虑到APP操作习惯及安全性暂定60分钟，这个默认5分钟，一般为30分钟到4小时，如果是网页形式可以时间更长一般不超过24小时，多了有安全隐患</p>
</blockquote>
<p><strong>6. ssl_prefer_server_ciphers on;</strong></p>
<blockquote>
<p>让服务器选择要使用的算法套件，这样避免客户端选择低安全的算法造成攻击</p>
</blockquote>
<p><strong>7. ssl_ciphers (共18个，ECDHE、DHE、AES开头个6个)</strong></p>
<blockquote>
<p>“ECDHE-RSA-AES128-GCM-SHA256:ECDHE-RSA-AES256-GCM-SHA384:DHE-RSA-AES256-GCM-SHA384:DHE-RSA-AES128-GCM-SHA256:ECDHE-RSA-AES256-SHA384:ECDHE-RSA-AES128-SHA256:ECDHE-RSA-AES256-SHA:ECDHE-RSA-AES128-SHA:DHE-RSA-AES256-SHA256:DHE-RSA-AES128-SHA256:DHE-RSA-AES256-SHA:DHE-RSA-AES128-SHA:AES256-GCM-SHA384:AES128-GCM-SHA256:AES256-SHA256:AES128-SHA256:AES256-SHA:AES128-SHA:HIGH:!aNULL:!eNULL:!EXPORT:!DES:!MD5:!PSK:!RC4 “; 这18个算法是通过TLS版本和考虑到安全和性能及各种客户端兼容性默认选择ECDHE-RSA-AES128-GCM-SHA256，另外向哪些已经确认不安全的算法（如MD5、RC4、DES）会直接拒绝防止黑客根据客户端兼容性来降级安全算法，这里是安全和性能的核心，需要长期关注定期更新。另外特别注意的是HTTPS里面耗时的有两个地方一个是网络方面的RTT就是延时，一个是密钥交换优化需要在这两个地方下功夫</p>
</blockquote>
<h3 id="二、重点注意事项"><a href="#二、重点注意事项" class="headerlink" title="二、重点注意事项"></a>二、重点注意事项</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">1. SHA256签名算法支持最少XPSP3和Android2.3版本</span><br><span class="line">2. 服务器密码套件配置优先，这样更安全</span><br><span class="line">3. AES可以和GCM已验证套件一起使用，建议TLS协议中只使用GCM套件，不使用CBC套件</span><br><span class="line">4. 前向保密 ECDHE套件</span><br><span class="line">5. 性能GCM套件是最快的</span><br><span class="line">6. SNI服务器名称指示，2006年后才加入TLS中，支持一个IP绑定多个域名，但是域名过多，证书也会变大，通配域名理论上不能超过上百域名；另外SNI有的客户端不支持例如IE7.0以下、Windows XP、Mac OS版要求最低X 10.5.6，早期Android版本，Nginx 0.5.32及后续版本,Openssl0.98f（0.98j开始默认支持SNI）</span><br><span class="line">7. 会话缓存，例如一个小时，Twitter为例，12小时会更新一次密钥36小时候删除</span><br><span class="line">8. 分布式会话缓存，https使用ip_hash，保证同一个用户始终分配到统一服务器上</span><br><span class="line">9. Cookie安全问题</span><br><span class="line">10. HSTS可以解决不安全到Cookie，HTTPS stripping攻击，相同网站内的混合内容问题。HSTS可以禁止浏览器使用无效证书。最好效果是包括子域名</span><br><span class="line">11. CSP，允许网站控制在HTML页面中嵌入的资源用什么协议来对抗XSS攻击</span><br><span class="line">12. Openssl 1.0.1版本后开始支持，协议降级保护,使用Openssl最新库，性能明显优化，但是也不能盲目升级1.0.1版本后才出现心脏出血漏洞，1.0.2版本后会输出密钥强度，系统自带Openssl-1.0.1e版本，官网Openssl三大版本最新版本1.1.0c、1.0.2j、1.0.1u</span><br><span class="line">13. 2010 Google数据TLS计算只占CPU负载的不到1%，每个连接只占不到10KB的内存，以及不到2%的网络开销</span><br><span class="line">14. initcwnd初始拥塞窗口调优ip route change  59.151.116.115 route change  initcwnd 10 </span><br><span class="line">15. net.ipv4.tcp_slow_start_after_idle = 0  改成0防止空闲时慢启动，HTTP长连接</span><br><span class="line">16. 保持TCP连接时间越长，传输越快，有了长连接，可以最小化TLS开销，同时也提高了TCP性能。HTTP/1.1默认开启保持活动状态（keep-alive）</span><br><span class="line">17. SNI 机制，解决server 单ip支持多host https</span><br><span class="line">18. 尽早完成握手，cdn与客户端建立tls</span><br><span class="line">19. 让服务器支持HTTP/2，Nginx 的版本需要大于1.9.5，同时OpenSSL的版本需要大于1.0.2j</span><br><span class="line">20. Nginx不会对反向代理的后端做证书验证，当后端服务器是公网服务器就会有安全缺陷，Nginx 1.8.x版本后支持后端证书验证</span><br><span class="line">21. 线上Tengine2.1.0版本（Nginx1.6.2),线上Tengine2.2.0版本（Nginx1.8.1)支持HTTP2.0，新版本出来20来天等稳定一段时间后再升级</span><br><span class="line">22. HTTPS总共需要三个往返（TCP一个，TLS二个），RTT大约30毫秒的用户，HTTPS大约需要90毫秒完成连接建立，RTT要是比较大，这个建立连接的时间将会大得多</span><br><span class="line">23. TLS建立连接的耗时对比：直接TLS连接设置3*90ms=270ms，通过CDN进行的TLS连接设置（使用连接池）3*5ms=15ms  用户到CDN节点RTT 5ms，CDN节点到服务器RTT 85ms，RTT为联合往返时间</span><br><span class="line">24. TLS最大的成本除了延迟以外，就是用于安全参数协商的CPU密集型加密操作，即密钥交换，而密钥交换的CPU消耗很大程度上取决于服务器选择的私钥算法、密钥长度和密钥交换算法建议使用这个TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256（百度，京东，阿里都是这个），由于按照服务器端优先级为准，这个算法应放在ssl_ciphers：ECDHE-RSA-AES128-GCM-SHA256第一位</span><br><span class="line">25. 证书链里证书越少越好，这样速度更快、</span><br><span class="line">26. TLS回使DsS攻击更加容易成本更低，安全风险少量的可以限制连接，大量的需要资源超配或第三方援助才行</span><br><span class="line">27. HSTS考虑到客户端兼容性和目前没有全部域名HTTPS，现在没有开启，后续再开启</span><br><span class="line">28. 默认站点可以对不正确域名的请求返回错误消息listen 443 ssl default_server;不需要配置server_name，所有未匹配的请求都会进入默认站点server_name “”;</span><br><span class="line">29. 服务器集群且不希望部署共享票证密钥时，可以ssl_session_tickets off;这个从1.5.9版本开始支持，默认不配置就行集群总体上会话票证弊大于利</span><br><span class="line">30. Http转Https最节省资源的配置方法  return https://$host$request_uri;</span><br><span class="line">31. TLS缓冲区调优ssl_buffer_size默认16KB，减少TLS缓冲区大小，可以显著减少首字节时间例如配置1400字节，注意会降低吞吐量，访问量大且数据为图片等大数据时的域名不需要降低</span><br><span class="line">32. TLS使用情况监控日志可以加变量$ssl_session_reused（1.5.10后支持），根据会话恢复率可以了解TLS会话缓存的工作性能，并设置TLS日志格式 </span><br><span class="line">33. log_format ssl “$time_local $server_name $remote_addr $connection $connection_requests $ssl_protocol $ssl_cipher $ssl_session_id $ssl_session_reused”;</span><br><span class="line">34. Ssl日志位置也分开 access_log /data/ssllog/dom</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/21/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/21/">21</a><span class="page-number current">22</span><a class="page-number" href="/page/23/">23</a><span class="space">&hellip;</span><a class="page-number" href="/page/26/">26</a><a class="extend next" rel="next" href="/page/23/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Jerry Min</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">260</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
        <span class="site-state-item-count">34</span>
        <span class="site-state-item-name">categories</span>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">61</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Jerry Min</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
